{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2bPw-5W4hJe"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb3YQA5HFeNz",
    "outputId": "0d1d7a8b-a54e-4118-89a9-da0faa3566ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Obtaining dependency information for torchinfo from https://files.pythonhosted.org/packages/72/25/973bd6128381951b23cdcd8a9870c6dcfc5606cb864df8eabd82e529f9c1/torchinfo-1.8.0-py3-none-any.whl.metadata\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uyr3kLf61IIP"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KAAYJlZW4p9a"
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"seed\": 4242,\n",
    "    \"img_res\": (3, 256, 256),\n",
    "    \"depth_img_res\": (1, 64, 64),\n",
    "    \"n_workers\": 2,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"batch_size_eval\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_patience\": 15,\n",
    "    \"e_stop_epochs\": 30,\n",
    "    \"epochs\": 120,\n",
    "}\n",
    "\n",
    "augmentation_parameters = {\n",
    "    'flip': 0.5,\n",
    "    'mirror': 0.5,\n",
    "    'color&bright': 0.5,\n",
    "    'c_swap': 0.5,\n",
    "    'random_crop': 0.5,\n",
    "    'random_d_shift': 0.5  # range(+-10)cm\n",
    "}\n",
    "\n",
    "dataset_root = './data/NYUv2/'\n",
    "save_model_root = './results/mob_eff/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm2TAq6B5UBI",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s77jAM-X5VsY"
   },
   "outputs": [],
   "source": [
    "def hardware_check():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Actual device: \", device)\n",
    "    if 'cuda' in device:\n",
    "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def plot_depth_map(dm):\n",
    "\n",
    "    MIN_DEPTH = 0.0\n",
    "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
    "\n",
    "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
    "    cmap = plt.cm.plasma_r\n",
    "\n",
    "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
    "\n",
    "\n",
    "def resize_keeping_aspect_ratio(img, base):\n",
    "    \"\"\"\n",
    "    Resize the image to a defined length manteining its proportions\n",
    "    Scaling the shortest side of the image to a fixed 'base' length'\n",
    "    \"\"\"\n",
    "\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        basewidth = int(base)\n",
    "        wpercent = (basewidth / float(img.shape[0]))\n",
    "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
    "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
    "    else:\n",
    "        baseheight = int(base)\n",
    "        wpercent = (baseheight / float(img.shape[1]))\n",
    "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
    "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, depths):\n",
    "    valid_mask = depths > 0.0\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
    "    valid_mask = y_true > 0.0\n",
    "    valid_pred = y_pred[valid_mask]\n",
    "    valid_true = y_true[valid_mask]\n",
    "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
    "    return 100 * torch.mean(correct.float())\n",
    "\n",
    "\n",
    "def print_model(model, input_shape):\n",
    "    info = summary(model, input_size=input_shape)\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name, path_save_model):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    if not os.path.isdir(path_save_model):\n",
    "        os.makedirs(path_save_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path_save_model + name)\n",
    "\n",
    "\n",
    "def save_history(history, filepath):\n",
    "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "\n",
    "def save_csv_history(model_name, path):\n",
    "    objects = []\n",
    "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    df = pd.DataFrame(objects)\n",
    "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if do_pretrained:\n",
    "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
    "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"checkpoint loaded\\n\")\n",
    "\n",
    "    if imagenet_w_init:\n",
    "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
    "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model_dict = model.state_dict()\n",
    "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
    "\n",
    "        # pretrained_param = len(pretrained_dict.items())\n",
    "        counter_param = 0\n",
    "        for i, j in pretrained_dict.items():\n",
    "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
    "                counter_param += 1\n",
    "\n",
    "        print(f'Pertained parameters: {counter_param}\\n')\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        # alternativa to 2 e 3\n",
    "        # model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Partial initialization computed\\n\")\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title, path):\n",
    "    epochs = range(0, len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_history(history, path):\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
    "\n",
    "\n",
    "def plot_loss_parts(history, path, title):\n",
    "    l_mae_list = history['l_mae']\n",
    "    l_norm_list = history['l_norm']\n",
    "    l_grad_list = history['l_grad']\n",
    "    l_ssim_list = history['l_ssim']\n",
    "    epochs = range(0, len(l_mae_list))\n",
    "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
    "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
    "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
    "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
    "    for i in range(quantity):\n",
    "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
    "\n",
    "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
    "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
    "              f' variance =  {torch.var(img)}\\n')\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input image')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Grayscale DepthMap')\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Colored DepthMap')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        print(\"************************** \",save_model_root)\n",
    "        save_path = save_model_root + 'example&augment_img/'\n",
    "        print(\"************************** \",save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
    "    for i, index in zip(range(len(indices)), indices):\n",
    "        img, depth = dataset.__getitem__(index)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.from_numpy(img).to(device))\n",
    "            # Build plot\n",
    "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "            plt.subplot(1, len(indices), i+1)\n",
    "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
    "            if False:\n",
    "                plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
    "    save_path = save_model_root + type + '_predictions/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(list_type)):\n",
    "        index_image = list_type[i][0]\n",
    "        rmse_value = list_type[i][1]\n",
    "\n",
    "        img, depth = dataset.__getitem__(index=index_image)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f'Original image {index_image}')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title('Predicted DepthMap')\n",
    "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
    "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
    "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
    "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def compute_MeanVar(dataset):\n",
    "    r_mean, g_mean, b_mean = [], [], []\n",
    "    r_var, g_var, b_var = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r = np.array(img[0, :, :])\n",
    "        g = np.array(img[1, :, :])\n",
    "        b = np.array(img[2, :, :])\n",
    "\n",
    "        r_mean.append(np.mean(r))\n",
    "        g_mean.append(np.mean(g))\n",
    "        b_mean.append(np.mean(b))\n",
    "\n",
    "        r_var.append(np.var(r))\n",
    "        g_var.append(np.var(g))\n",
    "        b_var.append(np.var(b))\n",
    "\n",
    "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
    "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
    "\n",
    "\n",
    "def compute_MeanImg(dataset, save_model_root):\n",
    "    r, g, b = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r.append(np.array(img[0, :, :]))\n",
    "        g.append(np.array(img[1, :, :]))\n",
    "        b.append(np.array(img[2, :, :]))\n",
    "\n",
    "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
    "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
    "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
    "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
    "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
    "\n",
    "    print(\"Process Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zUH-3na7eAH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0CFEYqVd7i1H"
   },
   "outputs": [],
   "source": [
    "def pixel_shift(depth_img, shift):\n",
    "    depth_img = depth_img + shift\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "def random_crop(x, y, crop_size=(192, 256)):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    h, w, _ = x.shape\n",
    "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
    "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
    "    if cropped_y.shape[-1] == 0:\n",
    "        return x, y\n",
    "    else:\n",
    "        return cropped_x, cropped_y\n",
    "\n",
    "\n",
    "def augmentation2D(img, depth, print_info_aug):\n",
    "    # Random flipping\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
    "        img = (img[..., ::1, :, :]).copy()\n",
    "        depth = (depth[..., ::1, :, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random flipped')\n",
    "    # Random mirroring\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
    "        img = (img[..., ::-1, :]).copy()\n",
    "        depth = (depth[..., ::-1, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random mirrored')\n",
    "    # Augment image\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
    "        # gamma augmentation\n",
    "        gamma = random.uniform(0.9, 1.1)\n",
    "        img = img ** gamma\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        img = img * brightness\n",
    "        # color augmentation\n",
    "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
    "        white = np.ones((img.shape[0], img.shape[1]))\n",
    "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        img *= color_image\n",
    "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
    "        if print_info_aug:\n",
    "            print('--> Image randomly augmented')\n",
    "    # Channel swap\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
    "        indices = list(product([0, 1, 2], repeat=3))\n",
    "        policy_idx = random.randint(0, len(indices) - 1)\n",
    "        img = img[..., list(indices[policy_idx])]\n",
    "        if print_info_aug:\n",
    "            print('--> Channel swapped')\n",
    "    # Random crop\n",
    "    if random.random() <= augmentation_parameters['random_crop']:\n",
    "        img, depth = random_crop(img, depth)\n",
    "        if print_info_aug:\n",
    "            print('--> Random cropped')\n",
    "    # Depth Shift\n",
    "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
    "        random_shift = random.randint(-10, 10)\n",
    "        depth = pixel_shift(depth, shift=random_shift)\n",
    "        if print_info_aug:\n",
    "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
    "\n",
    "    return img, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbCnAwv453IN"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zhuQQhwf597g"
   },
   "outputs": [],
   "source": [
    "class NYU2_Dataset:\n",
    "    \"\"\"\n",
    "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
    "      * 654 Test and 50688 Train images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
    "        self.dataset = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.info = 0\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.rgb_h_res = rgb_h_res\n",
    "        self.d_h_res = d_h_res\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "        # Handle dataset\n",
    "        if self.dts_type == 'test':\n",
    "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
    "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
    "\n",
    "            rgb = np.load(img_path)\n",
    "            depth = np.load(depth_path)\n",
    "\n",
    "            self.x = rgb\n",
    "            self.y = depth\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = rgb[:dts_size]\n",
    "                self.y = depth[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        elif self.dts_type == 'train':\n",
    "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
    "            for scene in scenarios:\n",
    "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
    "                for el in elem:\n",
    "                    if 'jpg' in el:\n",
    "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    elif 'png' in el:\n",
    "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    else:\n",
    "                        raise SystemError('Type image error (train)')\n",
    "\n",
    "            if len(self.x) != len(self.y):\n",
    "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
    "\n",
    "            self.x.sort()\n",
    "            self.y.sort()\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = self.x[:dts_size]\n",
    "                self.y = self.y[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        else:\n",
    "            raise SystemError('Problem in the path')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def __getitem__(self, index=None, print_info_aug=False):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "\n",
    "        # Load Image\n",
    "        if self.dts_type == 'test':\n",
    "            img = self.x[index]\n",
    "        else:\n",
    "            img_name = self.dataset + self.x[index]\n",
    "            try:\n",
    "                raw_img = Image.open(img_name)\n",
    "                img = np.array(raw_img.convert('RGB'))\n",
    "                raw_img.close()\n",
    "            except:\n",
    "                exit(f\"Failed opening {img_name}\")\n",
    "\n",
    "        # Load Depth Image\n",
    "        if self.dts_type == 'test':\n",
    "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
    "        else:\n",
    "            depth = Image.open(self.dataset + self.y[index])\n",
    "            depth = np.array(depth) / 255\n",
    "            depth = np.clip(depth * 1000, 50, 1000)\n",
    "            depth = np.expand_dims(depth, axis=-1)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
    "\n",
    "        img_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['img_res'][1], param['img_res'][2]), antialias=True),\n",
    "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
    "        ])\n",
    "        depth_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['depth_img_res'][1], param['depth_img_res'][2]), antialias=True),\n",
    "        ])\n",
    "\n",
    "        img = img_post_processing(img/255)\n",
    "        depth = depth_post_processing(depth)\n",
    "\n",
    "        return img.float(), depth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KKUmyNBL5m1o"
   },
   "outputs": [],
   "source": [
    "def init_train_test_loader(dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
    "    # Load Datasets\n",
    "    test_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
    "    )\n",
    "    training_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
    "    )\n",
    "    # Create Dataloaders\n",
    "    training_DataLoader = DataLoader(\n",
    "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    test_DataLoader = DataLoader(\n",
    "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAWQQQzf8ctn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DTaUymYS8mDJ"
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class balanced_loss_function(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / 2\n",
    "\n",
    "        return loss_depth, loss_ssim, loss_normal, loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhLCflR28fxo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim, input_resolution):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0, resolution=input_resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim, resolution=input_resolution)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0, resolution=input_resolution // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h, resolution):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h, resolution=resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0, resolution=resolution)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CascadedGroupAttention(torch.nn.Module):\n",
    "    r\"\"\" Cascaded Group Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution, correspond to the window size.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.attn_ratio = attn_ratio\n",
    "\n",
    "        qkvs = []\n",
    "        dws = []\n",
    "        for i in range(num_heads):\n",
    "            qkvs.append(Conv2d_BN(dim // (num_heads), self.key_dim * 2 + self.d, resolution=resolution))\n",
    "            dws.append(Conv2d_BN(self.key_dim, self.key_dim, kernels[i], 1, kernels[i]//2, groups=self.key_dim, resolution=resolution))\n",
    "        self.qkvs = torch.nn.ModuleList(qkvs)\n",
    "        self.dws = torch.nn.ModuleList(dws)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            self.d * num_heads, dim, bn_weight_init=0, resolution=resolution))\n",
    "\n",
    "        points = list(itertools.product(range(resolution), range(resolution)))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "        trainingab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "        feats_in = x.chunk(len(self.qkvs), dim=1)\n",
    "        feats_out = []\n",
    "        feat = feats_in[0]\n",
    "        for i, qkv in enumerate(self.qkvs):\n",
    "            if i > 0: # add the previous output to the input\n",
    "                feat = feat + feats_in[i]\n",
    "            feat = qkv(feat)\n",
    "            q, k, v = feat.view(B, -1, H, W).split([self.key_dim, self.key_dim, self.d], dim=1) # B, C/h, H, W\n",
    "            q = self.dws[i](q)\n",
    "            q, k, v = q.flatten(2), k.flatten(2), v.flatten(2) # B, C/h, N\n",
    "            attn = (\n",
    "                (q.transpose(-2, -1) @ k) * self.scale\n",
    "                +\n",
    "                (trainingab[i] if self.training else self.ab[i])\n",
    "            )\n",
    "            attn = attn.softmax(dim=-1) # BNN\n",
    "            feat = (v @ attn.transpose(-2, -1)).view(B, self.d, H, W) # BCHW\n",
    "            feats_out.append(feat)\n",
    "        x = self.proj(torch.cat(feats_out, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocalWindowAttention(torch.nn.Module):\n",
    "    r\"\"\" Local Window Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.resolution = resolution\n",
    "        assert window_resolution > 0, 'window_size must be greater than 0'\n",
    "        self.window_resolution = window_resolution\n",
    "        \n",
    "        window_resolution = min(window_resolution, resolution)\n",
    "        self.attn = CascadedGroupAttention(dim, key_dim, num_heads,\n",
    "                                attn_ratio=attn_ratio, \n",
    "                                resolution=window_resolution,\n",
    "                                kernels=kernels,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = W = self.resolution\n",
    "        B, C, H_, W_ = x.shape\n",
    "        # Only check this for classifcation models\n",
    "        assert H == H_ and W == W_, 'input feature has wrong size, expect {}, got {}'.format((H, W), (H_, W_))\n",
    "               \n",
    "        if H <= self.window_resolution and W <= self.window_resolution:\n",
    "            x = self.attn(x)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            pad_b = (self.window_resolution - H %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            pad_r = (self.window_resolution - W %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            padding = pad_b > 0 or pad_r > 0\n",
    "\n",
    "            if padding:\n",
    "                x = torch.nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "\n",
    "            pH, pW = H + pad_b, W + pad_r\n",
    "            nH = pH // self.window_resolution\n",
    "            nW = pW // self.window_resolution\n",
    "            # window partition, BHWC -> B(nHh)(nWw)C -> BnHnWhwC -> (BnHnW)hwC -> (BnHnW)Chw\n",
    "            x = x.view(B, nH, self.window_resolution, nW, self.window_resolution, C).transpose(2, 3).reshape(\n",
    "                B * nH * nW, self.window_resolution, self.window_resolution, C\n",
    "            ).permute(0, 3, 1, 2)\n",
    "            x = self.attn(x)\n",
    "            # window reverse, (BnHnW)Chw -> (BnHnW)hwC -> BnHnWhwC -> B(nHh)(nWw)C -> BHWC\n",
    "            x = x.permute(0, 2, 3, 1).view(B, nH, nW, self.window_resolution, self.window_resolution,\n",
    "                       C).transpose(2, 3).reshape(B, pH, pW, C)\n",
    "            if padding:\n",
    "                x = x[:, :H, :W].contiguous()\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientViTBlock(torch.nn.Module):    \n",
    "    \"\"\" A basic EfficientViT building block.\n",
    "\n",
    "    Args:\n",
    "        type (str): Type for token mixer. Default: 's' for self-attention.\n",
    "        ed (int): Number of input channels.\n",
    "        kd (int): Dimension for query and key in the token mixer.\n",
    "        nh (int): Number of attention heads.\n",
    "        ar (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ed, kd, nh=8,\n",
    "                 ar=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dw0 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn0 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "        \n",
    "        self.mixer = Residual(LocalWindowAttention(ed, kd, nh, attn_ratio=ar, \\\n",
    "                resolution=resolution, window_resolution=window_resolution, kernels=kernels))\n",
    "        \n",
    "        self.dw1 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn1 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn1(self.dw1(self.mixer(self.ffn0(self.dw0(x)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   groups=depth,\n",
    "                                   padding=1,\n",
    "                                   stride=stride,\n",
    "                                   bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "                        bias=False, device='cpu'),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        self.transformer_times = transformer_times ############################## Time measurament\n",
    "        self.sample_cnt = sample_cnt ############################## Time measurament\n",
    "\n",
    "        L = [6, 8, 10]  # L = [2, 4, 3] # --> +5 FPS\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(EfficientViTBlock(channels[5], L[0], nh=8, resolution=32, kernels=[5, 5, 5, 5, 5, 5, 5, 5]))\n",
    "        self.mvit.append(EfficientViTBlock(channels[7], L[1], nh=7, resolution=16, kernels=[5, 5, 5, 5, 5, 5, 5]))\n",
    "        self.mvit.append(EfficientViTBlock(channels[9], L[2], nh=8, resolution=8, kernels=[5, 5, 5, 5, 5, 5, 5, 5]))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.conv1(x)\n",
    "        x = self.mv2[0](y0)\n",
    "\n",
    "        y1 = self.mv2[1](x)\n",
    "        x = self.mv2[2](y1)\n",
    "        x = self.mv2[3](x)  # Repeat\n",
    "\n",
    "        y2 = self.mv2[4](x)\n",
    "        x  = self.mvit[0](y2)\n",
    "        # self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
    "\n",
    "        y3 = self.mv2[5](x)\n",
    "        x = self.mvit[1](y3)\n",
    "        # # self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
    "\n",
    "        x = self.mv2[6](x)\n",
    "        x = self.mvit[2](x)\n",
    "        # # self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # self.sample_cnt += 1 ############################## Time measurament\n",
    "        # if(self.sample_cnt == 655):\n",
    "        #   self.sample_cnt = 0\n",
    "\n",
    "        return x, [y0, y1, y2, y3]\n",
    "\n",
    "\n",
    "def mobilevit_s(transformer_times, sample_cnt):\n",
    "    enc_type = 's'\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [32, 64, 64, 64, 192, 192, 224, 224, 320, 320, 320]\n",
    "    return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        x = self.end_up_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, device, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
    "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          flag=True,\n",
    "                                          sep_conv_filters=288 if typ == 's' else 144 if typ == 'xs' else 96,\n",
    "                                          name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=224 if typ == 's' else 96 if typ == 'xs' else 64,\n",
    "                                          name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                    1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[3])\n",
    "        x = self.ups_block_2(x, enc_layer_list[2])\n",
    "        x = self.ups_block_3(x, enc_layer_list[1])\n",
    "        x = self.conv2d_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_model(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(build_model, self).__init__()\n",
    "        self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
    "        self.sample_cnt = 0 ############################## Time measurament\n",
    "\n",
    "        self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
    "        self.decoder = SPEED_decoder(device=device, typ='s')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAvmPzvA8Pu-"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "es_aqA008Sof"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    return torch.log(x) / math.log(10)\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.irmse, self.imae = 0, 0\n",
    "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
    "        self.absrel, self.lg10 = 0, 0\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def set_to_worst(self):\n",
    "        self.irmse, self.imae = np.inf, np.inf\n",
    "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
    "        self.absrel, self.lg10 = np.inf, np.inf\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
    "        self.irmse, self.imae = irmse, imae\n",
    "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
    "        self.absrel, self.lg10 = absrel, lg10\n",
    "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
    "\n",
    "    def evaluate(self, output, target):\n",
    "        valid_mask = target > 0\n",
    "\n",
    "        output = output[valid_mask]\n",
    "        target = target[valid_mask]\n",
    "        \n",
    "\n",
    "        abs_diff = (output - target).abs()\n",
    "\n",
    "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
    "        self.rmse = math.sqrt(self.mse)\n",
    "        self.mae = float(abs_diff.mean())\n",
    "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
    "        self.absrel = float((abs_diff / target).mean())\n",
    "\n",
    "        maxRatio = torch.max(output / target, target / output)\n",
    "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
    "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
    "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
    "\n",
    "        inv_output = 1 / output\n",
    "        inv_target = 1 / target\n",
    "        abs_inv_diff = (inv_output - inv_target).abs()\n",
    "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
    "        self.imae = float(abs_inv_diff.mean())\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0.0\n",
    "        self.sum_irmse, self.sum_imae = 0, 0\n",
    "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
    "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
    "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, result, n=1):\n",
    "        self.count += n\n",
    "\n",
    "        self.sum_irmse += n * result.irmse\n",
    "        self.sum_imae += n * result.imae\n",
    "        self.sum_mse += n * result.mse\n",
    "        self.sum_rmse += n * result.rmse\n",
    "        self.sum_mae += n * result.mae\n",
    "        self.sum_absrel += n * result.absrel\n",
    "        self.sum_lg10 += n * result.lg10\n",
    "        self.sum_delta1 += n * result.delta1\n",
    "        self.sum_delta2 += n * result.delta2\n",
    "        self.sum_delta3 += n * result.delta3\n",
    "\n",
    "    def average(self):\n",
    "        avg = Result()\n",
    "        avg.update(\n",
    "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
    "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
    "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
    "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
    "        return avg\n",
    "\n",
    "\n",
    "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
    "    best_worst_dict = {}\n",
    "    result = Result()\n",
    "    result.set_to_worst()\n",
    "    average_meter = AverageMeter()\n",
    "    model.eval()  # switch to evaluate mode\n",
    "\n",
    "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
    "        inputs, depths = inputs.cuda(), depths.cuda()\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "        result.evaluate(predictions, depths)\n",
    "        average_meter.update(result)  # (result, inputs.size(0))\n",
    "        best_worst_dict[i] = result.rmse\n",
    "\n",
    "    avg = average_meter.average()\n",
    "\n",
    "    print('MAE={average.mae:.3f}\\n'\n",
    "          'RMSE={average.rmse:.3f}\\n'\n",
    "          'Delta1={average.delta1:.3f}\\n'\n",
    "          'Delta2={average.delta2:.3f}\\n'\n",
    "          'Delta3={average.delta3:.3f}\\n'\n",
    "          'Absrel={average.absrel:.3f}\\n'\n",
    "          'REL={average.absrel:.3f}\\n'\n",
    "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
    "\n",
    "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
    "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
    "\n",
    "    return best_worst_dict, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjiqGK4q42zP"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('./mob_eff_best')\n",
    "# device = hardware_check()\n",
    "# model = build_model(device=device).to(device=device)\n",
    "# model.load_state_dict(checkpoint)\n",
    "# training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "#     dts_root_path=dataset_root,\n",
    "#     rgb_h_res=param['img_res'][1],\n",
    "#     d_h_res=param['depth_img_res'][1],\n",
    "#     bs_train=param['batch_size'],\n",
    "#     bs_eval=param['batch_size_eval'],\n",
    "#     num_workers=param['n_workers'],\n",
    "# )\n",
    "# best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                                            Output Shape              Param #\n",
      "===================================================================================================================\n",
      "build_model                                                       [1, 1, 64, 64]            --\n",
      "MobileViT: 1-1                                                  [1, 320, 8, 8]            --\n",
      "    Sequential: 2-1                                            [1, 32, 128, 128]         --\n",
      "        SeparableConv2d: 3-1                                  [1, 32, 128, 128]         1,888\n",
      "        BatchNorm2d: 3-2                                      [1, 32, 128, 128]         64\n",
      "        ReLU: 3-3                                             [1, 32, 128, 128]         --\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-4                                         [1, 64, 128, 128]         14,080\n",
      "        MV2Block: 3-5                                         [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-6                                         [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-7                                         [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-8                                         [1, 192, 32, 32]          69,248\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        EfficientViTBlock: 3-9                                [1, 192, 32, 32]          347,864\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-10                                        [1, 224, 16, 16]          329,920\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        EfficientViTBlock: 3-11                               [1, 224, 16, 16]          472,927\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-12                                        [1, 320, 8, 8]            499,712\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        EfficientViTBlock: 3-13                               [1, 320, 8, 8]            955,832\n",
      "    Sequential: 2-8                                            [1, 320, 8, 8]            --\n",
      "        Conv2d: 3-14                                          [1, 320, 8, 8]            102,400\n",
      "        BatchNorm2d: 3-15                                     [1, 320, 8, 8]            640\n",
      "        ReLU: 3-16                                            [1, 320, 8, 8]            --\n",
      "SPEED_decoder: 1-2                                              [1, 1, 64, 64]            --\n",
      "    Conv2d: 2-9                                                [1, 128, 8, 8]            40,960\n",
      "    UpSample_layer: 2-10                                       [1, 64, 16, 16]           --\n",
      "        ConvTranspose2d: 3-17                                 [1, 64, 16, 16]           73,728\n",
      "        Sequential: 3-18                                      [1, 64, 16, 16]           169,984\n",
      "    UpSample_layer: 2-11                                       [1, 32, 32, 32]           --\n",
      "        ConvTranspose2d: 3-19                                 [1, 32, 32, 32]           18,432\n",
      "        Sequential: 3-20                                      [1, 32, 32, 32]           65,536\n",
      "    UpSample_layer: 2-12                                       [1, 16, 64, 64]           --\n",
      "        ConvTranspose2d: 3-21                                 [1, 16, 64, 64]           4,608\n",
      "        Sequential: 3-22                                      [1, 16, 64, 64]           11,776\n",
      "    Conv2d: 2-13                                               [1, 1, 64, 64]            144\n",
      "===================================================================================================================\n",
      "Total params: 3,288,415\n",
      "Trainable params: 3,288,415\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.06\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 361.49\n",
      "Params size (MB): 13.15\n",
      "Estimated Total Size (MB): 375.42\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "print_model(model=model, input_shape=(1, *param['img_res']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "id": "590sGQdh45FS",
    "outputId": "5e5cdbab-651b-4a2e-b682-090e0dc53868",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: There are 50688 training and 654 testing samples\n",
      " --- Test samples --- \n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 509.86090087890625, min = 107.78614044189453\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.29162493348121643, variance =  1.499471664428711\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 992.6409301757812, min = 187.010009765625\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.114142417907715, mean = -0.197127565741539, variance =  1.3339691162109375\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n",
      " --- Training augmented samples --- \n",
      "--> Random flipped\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Depth Shifted of 8 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 397.2088928222656, min = 90.37699127197266\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.5614824295043945, variance =  1.774697184562683\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random flipped\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "--> Depth Shifted of -5 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 558.4122924804688, min = 163.62745666503906\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 0.32197824120521545, variance =  1.3423311710357666\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n",
      "--> Random cropped\n",
      "--> Depth Shifted of 6 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 405.9183044433594, min = 291.0140075683594\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.0365612506866455, mean = 0.2912631332874298, variance =  0.6459691524505615\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random flipped\n",
      "--> Depth Shifted of 10 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 599.3338623046875, min = 148.83006286621094\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1136350631713867, mean = 0.27235546708106995, variance =  2.357508420944214\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random mirrored\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 996.0784301757812, min = 370.0027160644531\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.067014217376709, min = -2.1179039478302, mean = 0.7378785610198975, variance =  0.671877920627594\n",
      "\n",
      "**************************  ./results/mob_eff/v1\n",
      "**************************  ./results/mob_eff/v1example&augment_img/\n",
      "The build_model model has: 3288415 trainable parameters\n",
      "Start training: build_model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 - Training: 100%|| 792/792 [03:50<00:00,  3.44step/s, Loss=265, Acc\n",
      "Epoch 1/120 - Validation: 100%|| 654/654 [00:06<00:00, 97.12step/s, Loss=244, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 122.204 at epoch 1\n",
      "New best ACCURACY: 9.346 at epoch 1\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 - Training: 100%|| 792/792 [03:40<00:00,  3.60step/s, Loss=219, Acc\n",
      "Epoch 2/120 - Validation: 100%|| 654/654 [00:06<00:00, 97.17step/s, Loss=178, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 78.545 at epoch 2\n",
      "New best ACCURACY: 13.370 at epoch 2\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=200, Acc\n",
      "Epoch 3/120 - Validation: 100%|| 654/654 [00:06<00:00, 95.65step/s, Loss=175, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 77.521 at epoch 3\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 - Training: 100%|| 792/792 [03:39<00:00,  3.60step/s, Loss=189, Acc\n",
      "Epoch 4/120 - Validation: 100%|| 654/654 [00:06<00:00, 96.77step/s, Loss=161, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 72.075 at epoch 4\n",
      "New best ACCURACY: 16.100 at epoch 4\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=180, Acc\n",
      "Epoch 5/120 - Validation: 100%|| 654/654 [00:06<00:00, 94.97step/s, Loss=158, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.792 at epoch 5\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 - Training: 100%|| 792/792 [03:47<00:00,  3.47step/s, Loss=172, Acc\n",
      "Epoch 6/120 - Validation: 100%|| 654/654 [00:06<00:00, 95.82step/s, Loss=165, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 - Training: 100%|| 792/792 [03:40<00:00,  3.60step/s, Loss=166, Acc\n",
      "Epoch 7/120 - Validation: 100%|| 654/654 [00:06<00:00, 96.29step/s, Loss=149, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.878 at epoch 7\n",
      "New best ACCURACY: 18.533 at epoch 7\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=161, Acc\n",
      "Epoch 8/120 - Validation: 100%|| 654/654 [00:06<00:00, 95.52step/s, Loss=144, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 63.019 at epoch 8\n",
      "New best ACCURACY: 19.639 at epoch 8\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=156, Acc\n",
      "Epoch 9/120 - Validation: 100%|| 654/654 [00:06<00:00, 97.87step/s, Loss=139, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 60.918 at epoch 9\n",
      "New best ACCURACY: 19.799 at epoch 9\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=152, Ac\n",
      "Epoch 10/120 - Validation: 100%|| 654/654 [00:06<00:00, 97.82step/s, Loss=145, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=148, Ac\n",
      "Epoch 11/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.65step/s, Loss=137, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.107 at epoch 11\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=145, Ac\n",
      "Epoch 12/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.04step/s, Loss=133, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 58.060 at epoch 12\n",
      "New best ACCURACY: 22.497 at epoch 12\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 - Training: 100%|| 792/792 [03:50<00:00,  3.44step/s, Loss=141, Ac\n",
      "Epoch 13/120 - Validation: 100%|| 654/654 [00:06<00:00, 97.72step/s, Loss=135, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=138, Ac\n",
      "Epoch 14/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.92step/s, Loss=134, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=135, Ac\n",
      "Epoch 15/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.88step/s, Loss=132, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 - Training: 100%|| 792/792 [03:47<00:00,  3.48step/s, Loss=132, Ac\n",
      "Epoch 16/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.18step/s, Loss=129, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 56.541 at epoch 16\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=129, Ac\n",
      "Epoch 17/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.62step/s, Loss=128, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 22.927 at epoch 17\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=127, Ac\n",
      "Epoch 18/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.36step/s, Loss=129, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=125, Ac\n",
      "Epoch 19/120 - Validation: 100%|| 654/654 [00:06<00:00, 96.97step/s, Loss=126, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 55.273 at epoch 19\n",
      "New best ACCURACY: 23.093 at epoch 19\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=123, Ac\n",
      "Epoch 20/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.45step/s, Loss=125, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 54.585 at epoch 20\n",
      "New best ACCURACY: 24.019 at epoch 20\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=121, Ac\n",
      "Epoch 21/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.20step/s, Loss=134, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=120, Ac\n",
      "Epoch 22/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.71step/s, Loss=125, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=118, Ac\n",
      "Epoch 23/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.44step/s, Loss=123, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "New best ACCURACY: 24.348 at epoch 23\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=117, Ac\n",
      "Epoch 24/120 - Validation: 100%|| 654/654 [00:06<00:00, 102.28step/s, Loss=123,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 53.829 at epoch 24\n",
      "New best ACCURACY: 24.442 at epoch 24\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=114, Ac\n",
      "Epoch 25/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.13step/s, Loss=122, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 - Training: 100%|| 792/792 [03:40<00:00,  3.58step/s, Loss=113, Ac\n",
      "Epoch 26/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.57step/s, Loss=126,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 - Training: 100%|| 792/792 [03:48<00:00,  3.47step/s, Loss=112, Ac\n",
      "Epoch 27/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.46step/s, Loss=122,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 53.193 at epoch 27\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=110, Ac\n",
      "Epoch 28/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.13step/s, Loss=124,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 - Training: 100%|| 792/792 [03:50<00:00,  3.44step/s, Loss=109, Ac\n",
      "Epoch 29/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.08step/s, Loss=122,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=108, Ac\n",
      "Epoch 30/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.19step/s, Loss=121,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 - Training: 100%|| 792/792 [03:42<00:00,  3.57step/s, Loss=106, Ac\n",
      "Epoch 31/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.01step/s, Loss=122,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=105, Ac\n",
      "Epoch 32/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.58step/s, Loss=120,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 52.784 at epoch 32\n",
      "New best ACCURACY: 24.544 at epoch 32\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=105, Ac\n",
      "Epoch 33/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.48step/s, Loss=118,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 52.214 at epoch 33\n",
      "New best ACCURACY: 25.239 at epoch 33\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=104, Ac\n",
      "Epoch 34/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.86step/s, Loss=119,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=102, Ac\n",
      "Epoch 35/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.09step/s, Loss=121,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 - Training: 100%|| 792/792 [03:51<00:00,  3.43step/s, Loss=101, Ac\n",
      "Epoch 36/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.53step/s, Loss=118, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=101, Ac\n",
      "Epoch 37/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.99step/s, Loss=120,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=99.7, A\n",
      "Epoch 38/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.22step/s, Loss=116,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 51.267 at epoch 38\n",
      "New best ACCURACY: 26.270 at epoch 38\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 - Training: 100%|| 792/792 [03:42<00:00,  3.57step/s, Loss=98.8, A\n",
      "Epoch 39/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.65step/s, Loss=117,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 - Training: 100%|| 792/792 [03:50<00:00,  3.43step/s, Loss=97.8, A\n",
      "Epoch 40/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.36step/s, Loss=117,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=97.3, A\n",
      "Epoch 41/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.50step/s, Loss=117,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=95.8, A\n",
      "Epoch 42/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.64step/s, Loss=116, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=95, Acc\n",
      "Epoch 43/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.84step/s, Loss=116, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=94.8, A\n",
      "Epoch 44/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.05step/s, Loss=115,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 - Training: 100%|| 792/792 [03:51<00:00,  3.42step/s, Loss=94.2, A\n",
      "Epoch 45/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.98step/s, Loss=115, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "New best ACCURACY: 26.608 at epoch 45\n",
      "EarlyStopping increased due to Accuracy, stop in 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 - Training: 100%|| 792/792 [03:52<00:00,  3.40step/s, Loss=93.4, A\n",
      "Epoch 46/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.75step/s, Loss=116,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 - Training: 100%|| 792/792 [03:42<00:00,  3.57step/s, Loss=92.9, A\n",
      "Epoch 47/120 - Validation: 100%|| 654/654 [00:06<00:00, 102.05step/s, Loss=114,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 50.692 at epoch 47\n",
      "New best ACCURACY: 27.103 at epoch 47\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 - Training: 100%|| 792/792 [03:41<00:00,  3.57step/s, Loss=92.3, A\n",
      "Epoch 48/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.67step/s, Loss=114,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 50.255 at epoch 48\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=91.6, A\n",
      "Epoch 49/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.95step/s, Loss=116, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 - Training: 100%|| 792/792 [03:42<00:00,  3.57step/s, Loss=91.1, A\n",
      "Epoch 50/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.81step/s, Loss=115, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 - Training: 100%|| 792/792 [03:51<00:00,  3.41step/s, Loss=90.8, A\n",
      "Epoch 51/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.80step/s, Loss=114,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=90.5, A\n",
      "Epoch 52/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.89step/s, Loss=114,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 50.243 at epoch 52\n",
      "New best ACCURACY: 27.171 at epoch 52\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=89.7, A\n",
      "Epoch 53/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.09step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 50.077 at epoch 53\n",
      "New best ACCURACY: 27.942 at epoch 53\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=89.1, A\n",
      "Epoch 54/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.94step/s, Loss=113, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=88.9, A\n",
      "Epoch 55/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.61step/s, Loss=114, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 - Training: 100%|| 792/792 [03:42<00:00,  3.57step/s, Loss=88.4, A\n",
      "Epoch 56/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.89step/s, Loss=112, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 49.915 at epoch 56\n",
      "New best ACCURACY: 28.163 at epoch 56\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=87.8, A\n",
      "Epoch 57/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.61step/s, Loss=112, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 49.704 at epoch 57\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 - Training: 100%|| 792/792 [03:37<00:00,  3.64step/s, Loss=87.6, A\n",
      "Epoch 58/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.27step/s, Loss=113,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=86.7, A\n",
      "Epoch 59/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.71step/s, Loss=114, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=86.9, A\n",
      "Epoch 60/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.21step/s, Loss=114,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 - Training: 100%|| 792/792 [03:42<00:00,  3.55step/s, Loss=86.5, A\n",
      "Epoch 61/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.01step/s, Loss=113,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=85.9, A\n",
      "Epoch 62/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.83step/s, Loss=113, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=85.8, A\n",
      "Epoch 63/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.67step/s, Loss=113,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=85.7, A\n",
      "Epoch 64/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.69step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=85.2, A\n",
      "Epoch 65/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.60step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 49.300 at epoch 65\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=84.6, A\n",
      "Epoch 66/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.28step/s, Loss=113,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=84, Acc\n",
      "Epoch 67/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.29step/s, Loss=112, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 - Training: 100%|| 792/792 [03:37<00:00,  3.64step/s, Loss=83.8, A\n",
      "Epoch 68/120 - Validation: 100%|| 654/654 [00:06<00:00, 102.32step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/120 - Training: 100%|| 792/792 [03:51<00:00,  3.41step/s, Loss=83.7, A\n",
      "Epoch 69/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.26step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=83.7, A\n",
      "Epoch 70/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.60step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=83.4, A\n",
      "Epoch 71/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.50step/s, Loss=113, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=82.9, A\n",
      "Epoch 72/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.31step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/120 - Training: 100%|| 792/792 [03:44<00:00,  3.54step/s, Loss=82.5, A\n",
      "Epoch 73/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.54step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "New best ACCURACY: 28.198 at epoch 73\n",
      "EarlyStopping increased due to Accuracy, stop in 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=82.8, A\n",
      "Epoch 74/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.31step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=82.4, A\n",
      "Epoch 75/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.52step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/120 - Training: 100%|| 792/792 [03:51<00:00,  3.42step/s, Loss=81.8, A\n",
      "Epoch 76/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.46step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 49.141 at epoch 76\n",
      "New best ACCURACY: 28.291 at epoch 76\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/120 - Training: 100%|| 792/792 [03:42<00:00,  3.55step/s, Loss=81.4, A\n",
      "Epoch 77/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.50step/s, Loss=112,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=81.4, A\n",
      "Epoch 78/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.23step/s, Loss=111, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=81.3, A\n",
      "Epoch 79/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.84step/s, Loss=109, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 48.398 at epoch 79\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=80.9, A\n",
      "Epoch 80/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.91step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/120 - Training: 100%|| 792/792 [03:52<00:00,  3.41step/s, Loss=80.8, A\n",
      "Epoch 81/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.42step/s, Loss=111, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=80.5, A\n",
      "Epoch 82/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.14step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=80, Acc\n",
      "Epoch 83/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.73step/s, Loss=113, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=80.1, A\n",
      "Epoch 84/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.38step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/120 - Training: 100%|| 792/792 [03:44<00:00,  3.54step/s, Loss=79.9, A\n",
      "Epoch 85/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.59step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=79.7, A\n",
      "Epoch 86/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.02step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=79.3, A\n",
      "Epoch 87/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.31step/s, Loss=110, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=79.5, A\n",
      "Epoch 88/120 - Validation: 100%|| 654/654 [00:06<00:00, 102.26step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 21 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=79.2, A\n",
      "Epoch 89/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.66step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 20 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/120 - Training: 100%|| 792/792 [03:42<00:00,  3.55step/s, Loss=78.9, A\n",
      "Epoch 90/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.98step/s, Loss=111, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 19 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=79, Acc\n",
      "Epoch 91/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.09step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 18 epochs\n",
      "New best ACCURACY: 28.864 at epoch 91\n",
      "EarlyStopping increased due to Accuracy, stop in 20 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=78.3, A\n",
      "Epoch 92/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.36step/s, Loss=109,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 19 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=78.1, A\n",
      "Epoch 93/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.42step/s, Loss=111,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 18 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=78.5, A\n",
      "Epoch 94/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.08step/s, Loss=111, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 17 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=78.2, A\n",
      "Epoch 95/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.50step/s, Loss=110,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 16 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=72.3, A\n",
      "Epoch 96/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.17step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 48.035 at epoch 96\n",
      "New best ACCURACY: 29.510 at epoch 96\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=70.6, A\n",
      "Epoch 97/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.68step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.986 at epoch 97\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=70, Acc\n",
      "Epoch 98/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.98step/s, Loss=107, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.893 at epoch 98\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=69.4, A\n",
      "Epoch 99/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.01step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.845 at epoch 99\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=69.2, \n",
      "Epoch 100/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.34step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=68.9, \n",
      "Epoch 101/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.76step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.744 at epoch 101\n",
      "New best ACCURACY: 29.659 at epoch 101\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=68.7, \n",
      "Epoch 102/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.21step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=68.5, \n",
      "Epoch 103/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.41step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=68.2, \n",
      "Epoch 104/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.05step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=68.3, \n",
      "Epoch 105/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.17step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=68, Ac\n",
      "Epoch 106/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.23step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.740 at epoch 106\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/120 - Training: 100%|| 792/792 [03:42<00:00,  3.55step/s, Loss=67.8, \n",
      "Epoch 107/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.97step/s, Loss=106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.682 at epoch 107\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=67.7, \n",
      "Epoch 108/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.85step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=67.7, \n",
      "Epoch 109/120 - Validation: 100%|| 654/654 [00:06<00:00, 103.00step/s, Loss=106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=67.6, \n",
      "Epoch 110/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.55step/s, Loss=106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=67.5, \n",
      "Epoch 111/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.91step/s, Loss=106,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.597 at epoch 111\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/120 - Training: 100%|| 792/792 [03:44<00:00,  3.54step/s, Loss=67.4, \n",
      "Epoch 112/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.93step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=67.3, \n",
      "Epoch 113/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.03step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=67.1, \n",
      "Epoch 114/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.37step/s, Loss=106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=66.9, \n",
      "Epoch 115/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.71step/s, Loss=106,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.557 at epoch 115\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=67, Ac\n",
      "Epoch 116/120 - Validation: 100%|| 654/654 [00:06<00:00, 101.13step/s, Loss=106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 47.525 at epoch 116\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=67, Ac\n",
      "Epoch 117/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.17step/s, Loss=106,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=66.8, \n",
      "Epoch 118/120 - Validation: 100%|| 654/654 [00:06<00:00, 100.35step/s, Loss=107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=66.7, \n",
      "Epoch 119/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.31step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=66.6, \n",
      "Epoch 120/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.82step/s, Loss=107,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 3, Conv2d: 4, Conv2d: 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[10], line 214\u001b[0m, in \u001b[0;36mbuild_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 214\u001b[0m     x, enc_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x, enc_layer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[0;32mIn[10], line 108\u001b[0m, in \u001b[0;36mMobileViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 108\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmv2[\u001b[38;5;241m0\u001b[39m](y0)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 4D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 220\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# if not os.path.exists(save_model_root + 'info_code/'):\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m#     os.makedirs(save_model_root + 'info_code/')\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# files_directory = '/work/project/'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m#     shutil.copy(f, save_model_root + 'info_code/')\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Run process\u001b[39;00m\n\u001b[1;32m    219\u001b[0m start_time \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 220\u001b[0m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    222\u001b[0m end_time \u001b[38;5;241m=\u001b[39m perf_counter()\n",
      "Cell \u001b[0;32mIn[14], line 187\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    177\u001b[0m         shutil\u001b[38;5;241m.\u001b[39mrmtree(save_model_root \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample&augment_img/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# model, model_name = load_pretrained_model(model=model,\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#                                           path_weigths=save_model_root + 'build_model_best',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#                                           imagenet_w_init=global_var['imagenet_w_init'])\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# if global_var['do_print_model']:\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[43mprint_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_res\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m --- Begin evaluation --- \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mprint_model\u001b[0;34m(model, input_shape)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_model\u001b[39m(model, input_shape):\n\u001b[0;32m---> 58\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(info)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 3, Conv2d: 4, Conv2d: 4]"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "def process(device):\n",
    "    # Set-seed\n",
    "    seed = param['seed']\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Datasets loading\n",
    "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "        dts_root_path=dataset_root,\n",
    "        rgb_h_res=param['img_res'][1],\n",
    "        d_h_res=param['depth_img_res'][1],\n",
    "        bs_train=param['batch_size'],\n",
    "        bs_eval=param['batch_size_eval'],\n",
    "        num_workers=param['n_workers'],\n",
    "    )\n",
    "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
    "    # Prints samples\n",
    "    print(' --- Test samples --- ')\n",
    "    print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
    "              save_model_root=save_model_root)\n",
    "    print(' --- Training augmented samples --- ')\n",
    "    print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
    "                  save_model_root=save_model_root)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # Globals\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
    "               'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
    "    min_rmse = float('inf')\n",
    "    min_acc = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    # Loss\n",
    "    criterion = balanced_loss_function(device=device)\n",
    "    # Model\n",
    "    model = build_model(device=device).to(device=device)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # print_model(model=model, input_shape=param['img_res'])\n",
    "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=param['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
    "    )\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=param['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
    "        cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
    "    )\n",
    "    # Early stopping\n",
    "    trigger_times, early_stopping_epochs = 0, param['e_stop_epochs']\n",
    "    print(\"Start training: {}\\n\".format(model_name))\n",
    "    \n",
    "    epochs = param['epochs']\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        iter = 1\n",
    "        model.train()\n",
    "        running_loss, accuracy = 0, 0\n",
    "        running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Forward\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # Compute loss\n",
    "                loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Evaluation and Stats\n",
    "                running_loss += loss.item()\n",
    "                running_l_mae += loss_depth.item()\n",
    "                running_l_norm += loss_normal.item()\n",
    "                running_l_grad += loss_grad.item()\n",
    "                running_l_ssim += loss_ssim.item()\n",
    "\n",
    "                train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                train_loss_list.append(train_loss_support)\n",
    "\n",
    "                accuracy += compute_accuracy(outputs, depths)\n",
    "                tepoch.set_postfix({'Loss': running_loss / iter,\n",
    "                                    'Acc': accuracy.item() / iter,\n",
    "                                    'Lr': param['lr'] if not history['lrs'] else history['lrs'][-1],\n",
    "                                    'L_mae': running_l_mae / iter,\n",
    "                                    'L_norm': running_l_norm / iter,\n",
    "                                    'L_grad': running_l_grad / iter,\n",
    "                                    'L_ssim': running_l_ssim / iter\n",
    "                                    })\n",
    "                iter += 1\n",
    "\n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    # Evaluation metrics\n",
    "                    test_accuracy += compute_accuracy(outputs, depths)\n",
    "                    # Loss\n",
    "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                    test_loss_list.append(test_loss_support)\n",
    "\n",
    "                    # RMSE\n",
    "                    test_rmse += compute_rmse(outputs, depths)\n",
    "                    tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
    "                                        'RMSE': test_rmse.item() / iter})\n",
    "                    iter += 1\n",
    "\n",
    "        # Update history infos\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        history['train_loss'].append(running_loss / len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss / len(test_DataLoader))\n",
    "        history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
    "        history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
    "        # Update history losses infos\n",
    "        history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
    "        history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
    "        history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
    "        history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
    "        # Update scheduler LR\n",
    "        scheduler.step(history['test_rmse'][-1])\n",
    "        # Save model by best RMSE\n",
    "        if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
    "            trigger_times = 0\n",
    "            min_rmse = test_rmse / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best', save_model_root)\n",
    "            print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
    "        # Save model by best ACCURACY\n",
    "        if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
    "            min_acc = test_accuracy / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
    "            print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
    "            if trigger_times > 4:\n",
    "                trigger_times = trigger_times - 2\n",
    "                print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
    "\n",
    "        save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
    "                                 save_path=save_model_root + 'evolution_img/')\n",
    "        save_history(history, save_model_root + model_name + '_history')\n",
    "        # Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if trigger_times == early_stopping_epochs:\n",
    "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
    "            break\n",
    "\n",
    "        # Save loss for graphs\n",
    "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
    "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
    "\n",
    "        print('Finished Training')\n",
    "        save_csv_history(model_name=model_name, path=save_model_root)\n",
    "        plot_history(history, path=save_model_root)\n",
    "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
    "\n",
    "        if os.path.exists(save_model_root + 'example&augment_img/'):\n",
    "            shutil.rmtree(save_model_root + 'example&augment_img/')\n",
    "\n",
    "\n",
    "    # model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
    "    # model, model_name = load_pretrained_model(model=model,\n",
    "    #                                           path_weigths=save_model_root + 'build_model_best',\n",
    "    #                                           device=device,\n",
    "    #                                           do_pretrained=global_var['do_pretrained'],\n",
    "    #                                           imagenet_w_init=global_var['imagenet_w_init'])\n",
    "    # if global_var['do_print_model']:\n",
    "    # print_model(model=model, input_shape=param['img_res'])\n",
    "    # print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "\n",
    "    # Evaluate\n",
    "    print(' --- Begin evaluation --- ')\n",
    "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "    print(' --- End evaluation --- ')\n",
    "\n",
    "    sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "    save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "    save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware\n",
    "    device = hardware_check()\n",
    "\n",
    "    # -- TRAIN 1\n",
    "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
    "    # Directory test\n",
    "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
    "    #print(save_model_root)\n",
    "    # Create folders\n",
    "    if not os.path.exists(save_model_root):\n",
    "        os.makedirs(save_model_root)\n",
    "    # if not os.path.exists(save_model_root + 'info_code/'):\n",
    "    #     os.makedirs(save_model_root + 'info_code/')\n",
    "    # files_directory = '/work/project/'\n",
    "    # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
    "    # for f in files:\n",
    "    #     shutil.copy(f, save_model_root + 'info_code/')\n",
    "    # Run process\n",
    "    start_time = perf_counter()\n",
    "    process(device=device)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = perf_counter()\n",
    "    print(\"Total time elapsed: \",end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      " --- Begin evaluation --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE=35.344\n",
      "RMSE=47.525\n",
      "Delta1=0.825\n",
      "Delta2=0.963\n",
      "Delta3=0.990\n",
      "Absrel=0.138\n",
      "REL=0.138\n",
      "Lg10=0.058\n",
      " --- End evaluation --- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./results/mob_eff/v1build_model_best')\n",
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "    dts_root_path=dataset_root,\n",
    "    rgb_h_res=param['img_res'][1],\n",
    "    d_h_res=param['depth_img_res'][1],\n",
    "    bs_train=param['batch_size'],\n",
    "    bs_eval=param['batch_size_eval'],\n",
    "    num_workers=param['n_workers'],\n",
    ")\n",
    "\n",
    "print(' --- Begin evaluation --- ')\n",
    "best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "print(' --- End evaluation --- ')\n",
    "\n",
    "sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Nm2TAq6B5UBI",
    "BoQYEe4V5j5E",
    "8zUH-3na7eAH",
    "bbCnAwv453IN",
    "mLUvGTJF55VD",
    "tAWQQQzf8ctn",
    "eAvmPzvA8Pu-"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
