{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g2bPw-5W4hJe"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.8.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb3YQA5HFeNz",
    "outputId": "133cab2d-cd5d-4a53-df6d-6cce2fc083c4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uyr3kLf61IIP"
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"seed\": 4242,\n",
    "    \"img_res\": (3, 256, 256),\n",
    "    \"depth_img_res\": (1, 64, 64),\n",
    "    \"n_workers\": 2,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"batch_size_eval\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_patience\": 15,\n",
    "    \"e_stop_epochs\": 30,\n",
    "    \"epochs\": 120,\n",
    "}\n",
    "\n",
    "augmentation_parameters = {\n",
    "    'flip': 0.5,\n",
    "    'mirror': 0.5,\n",
    "    'color&bright': 0.5,\n",
    "    'c_swap': 0.5,\n",
    "    'random_crop': 0.5,\n",
    "    'random_d_shift': 0.5  # range(+-10)cm\n",
    "}\n",
    "\n",
    "dataset_root = './data/NYUv2/'\n",
    "save_model_root = './results/meta_meter'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm2TAq6B5UBI",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s77jAM-X5VsY"
   },
   "outputs": [],
   "source": [
    "def hardware_check():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Actual device: \", device)\n",
    "    if 'cuda' in device:\n",
    "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def plot_depth_map(dm):\n",
    "\n",
    "    MIN_DEPTH = 0.0\n",
    "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
    "\n",
    "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
    "    cmap = plt.cm.plasma_r\n",
    "\n",
    "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
    "\n",
    "\n",
    "def resize_keeping_aspect_ratio(img, base):\n",
    "    \"\"\"\n",
    "    Resize the image to a defined length manteining its proportions\n",
    "    Scaling the shortest side of the image to a fixed 'base' length'\n",
    "    \"\"\"\n",
    "\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        basewidth = int(base)\n",
    "        wpercent = (basewidth / float(img.shape[0]))\n",
    "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
    "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
    "    else:\n",
    "        baseheight = int(base)\n",
    "        wpercent = (baseheight / float(img.shape[1]))\n",
    "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
    "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, depths):\n",
    "    valid_mask = depths > 0.0\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
    "    valid_mask = y_true > 0.0\n",
    "    valid_pred = y_pred[valid_mask]\n",
    "    valid_true = y_true[valid_mask]\n",
    "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
    "    return 100 * torch.mean(correct.float())\n",
    "\n",
    "\n",
    "def print_model(model, input_shape):\n",
    "    info = summary(model, input_size=input_shape)\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name, path_save_model):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    if not os.path.isdir(path_save_model):\n",
    "        os.makedirs(path_save_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path_save_model + name)\n",
    "\n",
    "\n",
    "def save_history(history, filepath):\n",
    "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "\n",
    "def save_csv_history(model_name, path):\n",
    "    objects = []\n",
    "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    df = pd.DataFrame(objects)\n",
    "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if do_pretrained:\n",
    "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
    "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"checkpoint loaded\\n\")\n",
    "\n",
    "    if imagenet_w_init:\n",
    "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
    "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model_dict = model.state_dict()\n",
    "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
    "\n",
    "        # pretrained_param = len(pretrained_dict.items())\n",
    "        counter_param = 0\n",
    "        for i, j in pretrained_dict.items():\n",
    "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
    "                counter_param += 1\n",
    "\n",
    "        print(f'Pertained parameters: {counter_param}\\n')\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        # alternativa to 2 e 3\n",
    "        # model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Partial initialization computed\\n\")\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title, path):\n",
    "    epochs = range(0, len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_history(history, path):\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
    "\n",
    "\n",
    "def plot_loss_parts(history, path, title):\n",
    "    l_mae_list = history['l_mae']\n",
    "    l_norm_list = history['l_norm']\n",
    "    l_grad_list = history['l_grad']\n",
    "    l_ssim_list = history['l_ssim']\n",
    "    epochs = range(0, len(l_mae_list))\n",
    "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
    "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
    "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
    "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
    "    for i in range(quantity):\n",
    "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
    "\n",
    "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
    "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
    "              f' variance =  {torch.var(img)}\\n')\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input image')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Grayscale DepthMap')\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Colored DepthMap')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        print(\"************************** \",save_model_root)\n",
    "        save_path = save_model_root + 'example&augment_img/'\n",
    "        print(\"************************** \",save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
    "    for i, index in zip(range(len(indices)), indices):\n",
    "        img, depth = dataset.__getitem__(index)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.from_numpy(img).to(device))\n",
    "            # Build plot\n",
    "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "            plt.subplot(1, len(indices), i+1)\n",
    "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
    "            if False:\n",
    "                plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
    "    save_path = save_model_root + type + '_predictions/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(list_type)):\n",
    "        index_image = list_type[i][0]\n",
    "        rmse_value = list_type[i][1]\n",
    "\n",
    "        img, depth = dataset.__getitem__(index=index_image)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f'Original image {index_image}')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title('Predicted DepthMap')\n",
    "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
    "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
    "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
    "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def compute_MeanVar(dataset):\n",
    "    r_mean, g_mean, b_mean = [], [], []\n",
    "    r_var, g_var, b_var = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r = np.array(img[0, :, :])\n",
    "        g = np.array(img[1, :, :])\n",
    "        b = np.array(img[2, :, :])\n",
    "\n",
    "        r_mean.append(np.mean(r))\n",
    "        g_mean.append(np.mean(g))\n",
    "        b_mean.append(np.mean(b))\n",
    "\n",
    "        r_var.append(np.var(r))\n",
    "        g_var.append(np.var(g))\n",
    "        b_var.append(np.var(b))\n",
    "\n",
    "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
    "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
    "\n",
    "\n",
    "def compute_MeanImg(dataset, save_model_root):\n",
    "    r, g, b = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r.append(np.array(img[0, :, :]))\n",
    "        g.append(np.array(img[1, :, :]))\n",
    "        b.append(np.array(img[2, :, :]))\n",
    "\n",
    "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
    "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
    "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
    "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
    "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
    "\n",
    "    print(\"Process Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8zUH-3na7eAH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0CFEYqVd7i1H"
   },
   "outputs": [],
   "source": [
    "def pixel_shift(depth_img, shift):\n",
    "    depth_img = depth_img + shift\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "def random_crop(x, y, crop_size=(192, 256)):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    h, w, _ = x.shape\n",
    "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
    "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
    "    if cropped_y.shape[-1] == 0:\n",
    "        return x, y\n",
    "    else:\n",
    "        return cropped_x, cropped_y\n",
    "\n",
    "\n",
    "def augmentation2D(img, depth, print_info_aug):\n",
    "    # Random flipping\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
    "        img = (img[..., ::1, :, :]).copy()\n",
    "        depth = (depth[..., ::1, :, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random flipped')\n",
    "    # Random mirroring\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
    "        img = (img[..., ::-1, :]).copy()\n",
    "        depth = (depth[..., ::-1, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random mirrored')\n",
    "    # Augment image\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
    "        # gamma augmentation\n",
    "        gamma = random.uniform(0.9, 1.1)\n",
    "        img = img ** gamma\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        img = img * brightness\n",
    "        # color augmentation\n",
    "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
    "        white = np.ones((img.shape[0], img.shape[1]))\n",
    "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        img *= color_image\n",
    "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
    "        if print_info_aug:\n",
    "            print('--> Image randomly augmented')\n",
    "    # Channel swap\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
    "        indices = list(product([0, 1, 2], repeat=3))\n",
    "        policy_idx = random.randint(0, len(indices) - 1)\n",
    "        img = img[..., list(indices[policy_idx])]\n",
    "        if print_info_aug:\n",
    "            print('--> Channel swapped')\n",
    "    # Random crop\n",
    "    if random.random() <= augmentation_parameters['random_crop']:\n",
    "        img, depth = random_crop(img, depth)\n",
    "        if print_info_aug:\n",
    "            print('--> Random cropped')\n",
    "    # Depth Shift\n",
    "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
    "        random_shift = random.randint(-10, 10)\n",
    "        depth = pixel_shift(depth, shift=random_shift)\n",
    "        if print_info_aug:\n",
    "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
    "\n",
    "    return img, depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bbCnAwv453IN",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zhuQQhwf597g"
   },
   "outputs": [],
   "source": [
    "class NYU2_Dataset:\n",
    "    \"\"\"\n",
    "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
    "      * 654 Test and 50688 Train images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
    "        self.dataset = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.info = 0\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.rgb_h_res = rgb_h_res\n",
    "        self.d_h_res = d_h_res\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "        # Handle dataset\n",
    "        if self.dts_type == 'test':\n",
    "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
    "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
    "\n",
    "            rgb = np.load(img_path)\n",
    "            depth = np.load(depth_path)\n",
    "\n",
    "            self.x = rgb\n",
    "            self.y = depth\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = rgb[:dts_size]\n",
    "                self.y = depth[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        elif self.dts_type == 'train':\n",
    "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
    "            for scene in scenarios:\n",
    "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
    "                for el in elem:\n",
    "                    if 'jpg' in el:\n",
    "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    elif 'png' in el:\n",
    "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    else:\n",
    "                        raise SystemError('Type image error (train)')\n",
    "\n",
    "            if len(self.x) != len(self.y):\n",
    "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
    "\n",
    "            self.x.sort()\n",
    "            self.y.sort()\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = self.x[:dts_size]\n",
    "                self.y = self.y[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        else:\n",
    "            raise SystemError('Problem in the path')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def __getitem__(self, index=None, print_info_aug=False):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "\n",
    "        # Load Image\n",
    "        if self.dts_type == 'test':\n",
    "            img = self.x[index]\n",
    "        else:\n",
    "            img_name = self.dataset + self.x[index]\n",
    "            try:\n",
    "                raw_img = Image.open(img_name)\n",
    "                img = np.array(raw_img.convert('RGB'))\n",
    "                raw_img.close()\n",
    "            except:\n",
    "                exit(f\"Failed opening {img_name}\")\n",
    "\n",
    "        # Load Depth Image\n",
    "        if self.dts_type == 'test':\n",
    "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
    "        else:\n",
    "            depth = Image.open(self.dataset + self.y[index])\n",
    "            depth = np.array(depth) / 255\n",
    "            depth = np.clip(depth * 1000, 50, 1000)\n",
    "            depth = np.expand_dims(depth, axis=-1)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
    "\n",
    "        img_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['img_res'][1], param['img_res'][2]), antialias=True),\n",
    "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
    "        ])\n",
    "        depth_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['depth_img_res'][1], param['depth_img_res'][2]), antialias=True),\n",
    "        ])\n",
    "\n",
    "        img = img_post_processing(img/255)\n",
    "        depth = depth_post_processing(depth)\n",
    "\n",
    "        return img.float(), depth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KKUmyNBL5m1o"
   },
   "outputs": [],
   "source": [
    "def init_train_test_loader(dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
    "    # Load Datasets\n",
    "    test_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
    "    )\n",
    "    training_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
    "    )\n",
    "    # Create Dataloaders\n",
    "    training_DataLoader = DataLoader(\n",
    "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    test_DataLoader = DataLoader(\n",
    "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tAWQQQzf8ctn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DTaUymYS8mDJ"
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class balanced_loss_function(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / 2\n",
    "\n",
    "        return loss_depth, loss_ssim, loss_normal, loss_grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vhLCflR28fxo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim, input_resolution):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0, resolution=input_resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim, resolution=input_resolution)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0, resolution=input_resolution // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h, resolution):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h, resolution=resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0, resolution=resolution)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, pool_size):\n",
    "      super().__init__()\n",
    "      self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "      #print(\"Pre pool: \", x.shape)\n",
    "      #print(\"Post pool: \", self.pool(x).shape)\n",
    "      return self.pool(x) - x\n",
    "\n",
    "\n",
    "class EfficientViTBlock(torch.nn.Module):    \n",
    "    \"\"\" A basic EfficientViT building block.\n",
    "\n",
    "    Args:\n",
    "        type (str): Type for token mixer. Default: 's' for self-attention.\n",
    "        ed (int): Number of input channels.\n",
    "        kd (int): Dimension for query and key in the token mixer.\n",
    "        nh (int): Number of attention heads.\n",
    "        ar (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ed, kd, nh=8,\n",
    "                 ar=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.dw0 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn0 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "        self.mixer = Residual(Attention(1))\n",
    "                \n",
    "        self.dw1 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn1 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn1(self.dw1(self.mixer(self.ffn0(self.dw0(x)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M9_rdzyp87dB"
   },
   "outputs": [],
   "source": [
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   groups=depth,\n",
    "                                   padding=1,\n",
    "                                   stride=stride,\n",
    "                                   bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "                        #bias=False, device='cuda:0'),\n",
    "                        bias=False, device='cpu'),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "\n",
    "        # Local representations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #B, C, H, W = Batch, Channels, Height, Width\n",
    "        # Global representations\n",
    "        _, _, h, w = x.shape\n",
    "        # b = batch\n",
    "        # d = depth = channels ?\n",
    "        # h = height\n",
    "        # ph = ?\n",
    "        # pw = ?\n",
    "\n",
    "        start_time = perf_counter() ############################## Time measurament\n",
    "        x = self.transformer(x)\n",
    "        end_time = perf_counter() ############################## Time measurament\n",
    "\n",
    "        # Fusion\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x, end_time-start_time ############################## Time measurament\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt,expansion=4, kernel_size=3, patch_size=(2, 2)): ############################## Time measurament\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        self.transformer_times = transformer_times ############################## Time measurament\n",
    "        self.sample_cnt = sample_cnt ############################## Time measurament\n",
    "\n",
    "        L = [6, 8, 10]  # L = [2, 4, 3] # --> +5 FPS\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(EfficientViTBlock(channels[5], L[0], nh=4, resolution=32))\n",
    "        self.mvit.append(EfficientViTBlock(channels[7], L[1], nh=4, resolution=16))\n",
    "        self.mvit.append(EfficientViTBlock(channels[9], L[2], nh=4, resolution=8))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "        # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.conv1(x)\n",
    "        x = self.mv2[0](y0)\n",
    "\n",
    "        y1 = self.mv2[1](x)\n",
    "        x = self.mv2[2](y1)\n",
    "        x = self.mv2[3](x)  # Repeat\n",
    "\n",
    "        y2 = self.mv2[4](x)\n",
    "        x = self.mvit[0](y2)\n",
    "        # self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
    "\n",
    "        y3 = self.mv2[5](x)\n",
    "        x = self.mvit[1](y3)\n",
    "        # self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
    "\n",
    "        x = self.mv2[6](x)\n",
    "        x = self.mvit[2](x)\n",
    "        # self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # self.sample_cnt += 1 ############################## Time measurament\n",
    "        # if(self.sample_cnt == 655):\n",
    "        #   self.sample_cnt = 0\n",
    "\n",
    "        return x, [y0, y1, y2, y3]\n",
    "\n",
    "\n",
    "def mobilevit_s(transformer_times, sample_cnt):\n",
    "    enc_type = 's'\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
    "    return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        x = self.end_up_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, device, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
    "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          flag=True,\n",
    "                                          sep_conv_filters=192 if typ == 's' else 144 if typ == 'xs' else 96,\n",
    "                                          name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=128 if typ == 's' else 96 if typ == 'xs' else 64,\n",
    "                                          name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                    1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[3])\n",
    "        x = self.ups_block_2(x, enc_layer_list[2])\n",
    "        x = self.ups_block_3(x, enc_layer_list[1])\n",
    "        x = self.conv2d_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_model(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(build_model, self).__init__()\n",
    "        self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
    "        self.sample_cnt = 0 ############################## Time measurament\n",
    "\n",
    "        self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
    "        self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eAvmPzvA8Pu-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "es_aqA008Sof"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    return torch.log(x) / math.log(10)\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.irmse, self.imae = 0, 0\n",
    "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
    "        self.absrel, self.lg10 = 0, 0\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def set_to_worst(self):\n",
    "        self.irmse, self.imae = np.inf, np.inf\n",
    "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
    "        self.absrel, self.lg10 = np.inf, np.inf\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
    "        self.irmse, self.imae = irmse, imae\n",
    "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
    "        self.absrel, self.lg10 = absrel, lg10\n",
    "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
    "\n",
    "    def evaluate(self, output, target):\n",
    "        valid_mask = target > 0\n",
    "\n",
    "        output = output[valid_mask]\n",
    "        target = target[valid_mask]\n",
    "        \n",
    "\n",
    "        abs_diff = (output - target).abs()\n",
    "\n",
    "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
    "        self.rmse = math.sqrt(self.mse)\n",
    "        self.mae = float(abs_diff.mean())\n",
    "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
    "        self.absrel = float((abs_diff / target).mean())\n",
    "\n",
    "        maxRatio = torch.max(output / target, target / output)\n",
    "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
    "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
    "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
    "\n",
    "        inv_output = 1 / output\n",
    "        inv_target = 1 / target\n",
    "        abs_inv_diff = (inv_output - inv_target).abs()\n",
    "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
    "        self.imae = float(abs_inv_diff.mean())\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0.0\n",
    "        self.sum_irmse, self.sum_imae = 0, 0\n",
    "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
    "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
    "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, result, n=1):\n",
    "        self.count += n\n",
    "\n",
    "        self.sum_irmse += n * result.irmse\n",
    "        self.sum_imae += n * result.imae\n",
    "        self.sum_mse += n * result.mse\n",
    "        self.sum_rmse += n * result.rmse\n",
    "        self.sum_mae += n * result.mae\n",
    "        self.sum_absrel += n * result.absrel\n",
    "        self.sum_lg10 += n * result.lg10\n",
    "        self.sum_delta1 += n * result.delta1\n",
    "        self.sum_delta2 += n * result.delta2\n",
    "        self.sum_delta3 += n * result.delta3\n",
    "\n",
    "    def average(self):\n",
    "        avg = Result()\n",
    "        avg.update(\n",
    "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
    "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
    "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
    "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
    "        return avg\n",
    "\n",
    "\n",
    "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
    "    best_worst_dict = {}\n",
    "    result = Result()\n",
    "    result.set_to_worst()\n",
    "    average_meter = AverageMeter()\n",
    "    model.eval()  # switch to evaluate mode\n",
    "\n",
    "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
    "        inputs, depths = inputs.cuda(), depths.cuda()\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "        result.evaluate(predictions, depths)\n",
    "        average_meter.update(result)  # (result, inputs.size(0))\n",
    "        best_worst_dict[i] = result.rmse\n",
    "\n",
    "    avg = average_meter.average()\n",
    "\n",
    "    print('MAE={average.mae:.3f}\\n'\n",
    "          'RMSE={average.rmse:.3f}\\n'\n",
    "          'Delta1={average.delta1:.3f}\\n'\n",
    "          'REL={average.absrel:.3f}\\n'\n",
    "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
    "\n",
    "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
    "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
    "\n",
    "    return best_worst_dict, avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EjiqGK4q42zP"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "build_model                                             [1, 1, 64, 64]            --\n",
      "MobileViT: 1-1                                        [1, 320, 8, 8]            --\n",
      "    Sequential: 2-1                                  [1, 16, 128, 128]         --\n",
      "        SeparableConv2d: 3-1                        [1, 16, 128, 128]         688\n",
      "        BatchNorm2d: 3-2                            [1, 16, 128, 128]         32\n",
      "        ReLU: 3-3                                   [1, 16, 128, 128]         --\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-4                               [1, 32, 128, 128]         3,968\n",
      "        MV2Block: 3-5                               [1, 64, 64, 64]           14,080\n",
      "        MV2Block: 3-6                               [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-7                               [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-8                               [1, 96, 32, 32]           44,480\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-9                      [1, 96, 32, 32]           76,992\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-10                              [1, 128, 16, 16]          91,264\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-11                     [1, 128, 16, 16]          135,424\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-12                              [1, 160, 8, 8]            154,432\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-13                     [1, 160, 8, 8]            210,240\n",
      "    Sequential: 2-8                                  [1, 320, 8, 8]            --\n",
      "        Conv2d: 3-14                                [1, 320, 8, 8]            51,200\n",
      "        BatchNorm2d: 3-15                           [1, 320, 8, 8]            640\n",
      "        ReLU: 3-16                                  [1, 320, 8, 8]            --\n",
      "SPEED_decoder: 1-2                                    [1, 1, 64, 64]            --\n",
      "    Conv2d: 2-9                                      [1, 128, 8, 8]            40,960\n",
      "    UpSample_layer: 2-10                             [1, 64, 16, 16]           --\n",
      "        ConvTranspose2d: 3-17                       [1, 64, 16, 16]           73,728\n",
      "        Sequential: 3-18                            [1, 64, 16, 16]           114,688\n",
      "    UpSample_layer: 2-11                             [1, 32, 32, 32]           --\n",
      "        ConvTranspose2d: 3-19                       [1, 32, 32, 32]           18,432\n",
      "        Sequential: 3-20                            [1, 32, 32, 32]           37,888\n",
      "    UpSample_layer: 2-12                             [1, 16, 64, 64]           --\n",
      "        ConvTranspose2d: 3-21                       [1, 16, 64, 64]           4,608\n",
      "        Sequential: 3-22                            [1, 16, 64, 64]           11,776\n",
      "    Conv2d: 2-13                                     [1, 1, 64, 64]            144\n",
      "=========================================================================================================\n",
      "Total params: 1,158,112\n",
      "Trainable params: 1,158,112\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 936.12\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 224.85\n",
      "Params size (MB): 4.63\n",
      "Estimated Total Size (MB): 230.27\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "print_model(model=model, input_shape=(1, *param['img_res']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "590sGQdh45FS",
    "outputId": "14b1c02e-ada3-4ba5-8e8c-4d5be04cc13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: There are 50688 training and 654 testing samples\n",
      " --- Test samples --- \n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 509.86090087890625, min = 107.78614044189453\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.29162493348121643, variance =  1.499471664428711\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 992.6409301757812, min = 187.010009765625\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.114142417907715, mean = -0.197127565741539, variance =  1.3339691162109375\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      " --- Training augmented samples --- \n",
      "--> Random flipped\n",
      "--> Image randomly augmented\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 389.2088928222656, min = 82.37699127197266\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.4649743139743805, variance =  1.884259581565857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      "--> Random mirrored\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Depth Shifted of 5 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 549.9065551757812, min = 134.4192657470703\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1170005798339844, mean = 0.7230955958366394, variance =  1.9172313213348389\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random mirrored\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Depth Shifted of 10 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 599.3338623046875, min = 148.83006286621094\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.141890048980713, min = -2.112826108932495, mean = 0.2593028247356415, variance =  1.9803962707519531\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Image randomly augmented\n",
      "--> Random cropped\n",
      "--> Depth Shifted of 3 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 824.6775512695312, min = 238.7765350341797\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 0.9317018985748291, variance =  1.7739200592041016\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      "--> Random flipped\n",
      "--> Random mirrored\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 373.42047119140625, min = 160.78431701660156\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.3691955506801605, variance =  1.623880386352539\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "build_model                                             [1, 1, 64, 64]            --\n",
      "MobileViT: 1-1                                        [1, 320, 8, 8]            --\n",
      "    Sequential: 2-1                                  [1, 16, 128, 128]         --\n",
      "        SeparableConv2d: 3-1                        [1, 16, 128, 128]         688\n",
      "        BatchNorm2d: 3-2                            [1, 16, 128, 128]         32\n",
      "        ReLU: 3-3                                   [1, 16, 128, 128]         --\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-4                               [1, 32, 128, 128]         3,968\n",
      "        MV2Block: 3-5                               [1, 64, 64, 64]           14,080\n",
      "        MV2Block: 3-6                               [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-7                               [1, 64, 64, 64]           36,224\n",
      "        MV2Block: 3-8                               [1, 96, 32, 32]           44,480\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-9                      [1, 96, 32, 32]           76,992\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-10                              [1, 128, 16, 16]          91,264\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-11                     [1, 128, 16, 16]          135,424\n",
      "    ModuleList: 2-6                                  --                        (recursive)\n",
      "        MV2Block: 3-12                              [1, 160, 8, 8]            154,432\n",
      "    ModuleList: 2-7                                  --                        (recursive)\n",
      "        EfficientViTBlock: 3-13                     [1, 160, 8, 8]            210,240\n",
      "    Sequential: 2-8                                  [1, 320, 8, 8]            --\n",
      "        Conv2d: 3-14                                [1, 320, 8, 8]            51,200\n",
      "        BatchNorm2d: 3-15                           [1, 320, 8, 8]            640\n",
      "        ReLU: 3-16                                  [1, 320, 8, 8]            --\n",
      "SPEED_decoder: 1-2                                    [1, 1, 64, 64]            --\n",
      "    Conv2d: 2-9                                      [1, 128, 8, 8]            40,960\n",
      "    UpSample_layer: 2-10                             [1, 64, 16, 16]           --\n",
      "        ConvTranspose2d: 3-17                       [1, 64, 16, 16]           73,728\n",
      "        Sequential: 3-18                            [1, 64, 16, 16]           114,688\n",
      "    UpSample_layer: 2-11                             [1, 32, 32, 32]           --\n",
      "        ConvTranspose2d: 3-19                       [1, 32, 32, 32]           18,432\n",
      "        Sequential: 3-20                            [1, 32, 32, 32]           37,888\n",
      "    UpSample_layer: 2-12                             [1, 16, 64, 64]           --\n",
      "        ConvTranspose2d: 3-21                       [1, 16, 64, 64]           4,608\n",
      "        Sequential: 3-22                            [1, 16, 64, 64]           11,776\n",
      "    Conv2d: 2-13                                     [1, 1, 64, 64]            144\n",
      "=========================================================================================================\n",
      "Total params: 1,158,112\n",
      "Trainable params: 1,158,112\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 936.12\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.79\n",
      "Forward/backward pass size (MB): 224.85\n",
      "Params size (MB): 4.63\n",
      "Estimated Total Size (MB): 230.27\n",
      "=========================================================================================================\n",
      "The build_model model has: 1158112 trainable parameters\n",
      "Start training: build_model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 - Training: 100%|| 792/792 [05:07<00:00,  2.58step/s, Loss=268, Acc=8.99, Lr=0.001, L_mae=115, L_norm=66.5, L\n",
      "Epoch 1/120 - Validation: 100%|| 654/654 [00:06<00:00, 94.84step/s, Loss=200, Acc=11.2, RMSE=92.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 92.585 at epoch 1\n",
      "New best ACCURACY: 11.247 at epoch 1\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 - Training: 100%|| 792/792 [05:08<00:00,  2.56step/s, Loss=223, Acc=11.3, Lr=0.001, L_mae=90.3, L_norm=59.2, \n",
      "Epoch 2/120 - Validation: 100%|| 654/654 [00:06<00:00, 95.77step/s, Loss=180, Acc=14.5, RMSE=80]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 79.967 at epoch 2\n",
      "New best ACCURACY: 14.529 at epoch 2\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 - Training: 100%|| 792/792 [05:08<00:00,  2.57step/s, Loss=207, Acc=12.9, Lr=0.001, L_mae=81.7, L_norm=55.9, \n",
      "Epoch 3/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.25step/s, Loss=174, Acc=12.9, RMSE=79.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 79.430 at epoch 3\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 - Training: 100%|| 792/792 [05:13<00:00,  2.53step/s, Loss=196, Acc=14.1, Lr=0.001, L_mae=75.9, L_norm=53.3, \n",
      "Epoch 4/120 - Validation: 100%|| 654/654 [00:04<00:00, 133.00step/s, Loss=162, Acc=16.6, RMSE=70.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.555 at epoch 4\n",
      "New best ACCURACY: 16.619 at epoch 4\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 - Training: 100%|| 792/792 [05:12<00:00,  2.54step/s, Loss=187, Acc=15.3, Lr=0.001, L_mae=70.9, L_norm=51.5, \n",
      "Epoch 5/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.23step/s, Loss=167, Acc=13.1, RMSE=78.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 - Training: 100%|| 792/792 [05:01<00:00,  2.62step/s, Loss=179, Acc=16.5, Lr=0.001, L_mae=66.6, L_norm=49.8, \n",
      "Epoch 6/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.41step/s, Loss=165, Acc=12.9, RMSE=77.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 - Training: 100%|| 792/792 [05:05<00:00,  2.60step/s, Loss=173, Acc=17.5, Lr=0.001, L_mae=63.7, L_norm=48.6, \n",
      "Epoch 7/120 - Validation: 100%|| 654/654 [00:06<00:00, 98.50step/s, Loss=148, Acc=18.2, RMSE=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.058 at epoch 7\n",
      "New best ACCURACY: 18.246 at epoch 7\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 - Training: 100%|| 792/792 [05:03<00:00,  2.61step/s, Loss=168, Acc=18.6, Lr=0.001, L_mae=60.8, L_norm=47.4, \n",
      "Epoch 8/120 - Validation: 100%|| 654/654 [00:06<00:00, 99.69step/s, Loss=147, Acc=18.4, RMSE=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 18.374 at epoch 8\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 - Training: 100%|| 792/792 [04:19<00:00,  3.05step/s, Loss=163, Acc=19.4, Lr=0.001, L_mae=58.2, L_norm=46.5, \n",
      "Epoch 9/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.07step/s, Loss=146, Acc=18.4, RMSE=64.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 18.383 at epoch 9\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=159, Acc=20.4, Lr=0.001, L_mae=56.1, L_norm=45.6,\n",
      "Epoch 10/120 - Validation: 100%|| 654/654 [00:03<00:00, 177.36step/s, Loss=148, Acc=18.6, RMSE=64.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "New best ACCURACY: 18.622 at epoch 10\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 - Training: 100%|| 792/792 [03:35<00:00,  3.67step/s, Loss=155, Acc=21.1, Lr=0.001, L_mae=54.4, L_norm=44.8,\n",
      "Epoch 11/120 - Validation: 100%|| 654/654 [00:03<00:00, 175.08step/s, Loss=143, Acc=18.8, RMSE=63.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 63.387 at epoch 11\n",
      "New best ACCURACY: 18.762 at epoch 11\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 - Training: 100%|| 792/792 [03:37<00:00,  3.64step/s, Loss=152, Acc=21.7, Lr=0.001, L_mae=52.9, L_norm=44.1,\n",
      "Epoch 12/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.83step/s, Loss=137, Acc=21.7, RMSE=59.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 59.505 at epoch 12\n",
      "New best ACCURACY: 21.692 at epoch 12\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 - Training: 100%|| 792/792 [03:36<00:00,  3.65step/s, Loss=147, Acc=23, Lr=0.001, L_mae=50.3, L_norm=43.4, L\n",
      "Epoch 13/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.62step/s, Loss=143, Acc=16.9, RMSE=66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=145, Acc=23.3, Lr=0.001, L_mae=49.5, L_norm=42.8,\n",
      "Epoch 14/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.44step/s, Loss=137, Acc=20.6, RMSE=60.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 - Training: 100%|| 792/792 [03:36<00:00,  3.67step/s, Loss=143, Acc=24, Lr=0.001, L_mae=48.4, L_norm=42.2, L\n",
      "Epoch 15/120 - Validation: 100%|| 654/654 [00:04<00:00, 162.61step/s, Loss=139, Acc=19.8, RMSE=61.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 - Training: 100%|| 792/792 [03:37<00:00,  3.65step/s, Loss=140, Acc=24.7, Lr=0.001, L_mae=47.1, L_norm=41.8,\n",
      "Epoch 16/120 - Validation: 100%|| 654/654 [00:03<00:00, 179.61step/s, Loss=139, Acc=19.9, RMSE=61.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 - Training: 100%|| 792/792 [03:34<00:00,  3.69step/s, Loss=138, Acc=25.3, Lr=0.001, L_mae=45.9, L_norm=41.2,\n",
      "Epoch 17/120 - Validation: 100%|| 654/654 [00:03<00:00, 177.09step/s, Loss=135, Acc=21.6, RMSE=60.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 - Training: 100%|| 792/792 [03:34<00:00,  3.68step/s, Loss=136, Acc=26, Lr=0.001, L_mae=44.9, L_norm=40.9, L\n",
      "Epoch 18/120 - Validation: 100%|| 654/654 [00:03<00:00, 175.26step/s, Loss=134, Acc=20.8, RMSE=59.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 59.364 at epoch 18\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 - Training: 100%|| 792/792 [03:35<00:00,  3.68step/s, Loss=133, Acc=26.8, Lr=0.001, L_mae=43.7, L_norm=40.4,\n",
      "Epoch 19/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.75step/s, Loss=134, Acc=21.5, RMSE=59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 59.046 at epoch 19\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=132, Acc=27.2, Lr=0.001, L_mae=43.2, L_norm=40, L\n",
      "Epoch 20/120 - Validation: 100%|| 654/654 [00:04<00:00, 146.68step/s, Loss=132, Acc=21.9, RMSE=59.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 21.941 at epoch 20\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 - Training: 100%|| 792/792 [03:36<00:00,  3.65step/s, Loss=130, Acc=27.8, Lr=0.001, L_mae=42.1, L_norm=39.6,\n",
      "Epoch 21/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.07step/s, Loss=131, Acc=21.8, RMSE=58.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 58.088 at epoch 21\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 - Training: 100%|| 792/792 [03:34<00:00,  3.69step/s, Loss=128, Acc=28.5, Lr=0.001, L_mae=41.3, L_norm=39.2,\n",
      "Epoch 22/120 - Validation: 100%|| 654/654 [00:03<00:00, 177.55step/s, Loss=131, Acc=21.5, RMSE=57.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 57.660 at epoch 22\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 - Training: 100%|| 792/792 [03:36<00:00,  3.66step/s, Loss=127, Acc=29.1, Lr=0.001, L_mae=40.6, L_norm=38.9,\n",
      "Epoch 23/120 - Validation: 100%|| 654/654 [00:03<00:00, 176.46step/s, Loss=130, Acc=22.4, RMSE=57.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 22.352 at epoch 23\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 - Training: 100%|| 792/792 [03:35<00:00,  3.67step/s, Loss=126, Acc=29.3, Lr=0.001, L_mae=40, L_norm=38.6, L\n",
      "Epoch 24/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.10step/s, Loss=134, Acc=21, RMSE=60.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 - Training: 100%|| 792/792 [03:37<00:00,  3.63step/s, Loss=124, Acc=30.1, Lr=0.001, L_mae=39.2, L_norm=38.2,\n",
      "Epoch 25/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.76step/s, Loss=132, Acc=22.4, RMSE=58.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "New best ACCURACY: 22.392 at epoch 25\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=123, Acc=30.3, Lr=0.001, L_mae=38.8, L_norm=37.9,\n",
      "Epoch 26/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.17step/s, Loss=129, Acc=23.5, RMSE=57.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 57.218 at epoch 26\n",
      "New best ACCURACY: 23.522 at epoch 26\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=121, Acc=31.4, Lr=0.001, L_mae=37.8, L_norm=37.7,\n",
      "Epoch 27/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.23step/s, Loss=131, Acc=20.2, RMSE=60.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 - Training: 100%|| 792/792 [03:36<00:00,  3.65step/s, Loss=120, Acc=31.5, Lr=0.001, L_mae=37.3, L_norm=37.3,\n",
      "Epoch 28/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.77step/s, Loss=134, Acc=20.1, RMSE=60.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 - Training: 100%|| 792/792 [03:36<00:00,  3.65step/s, Loss=120, Acc=31.6, Lr=0.001, L_mae=37.2, L_norm=37.3,\n",
      "Epoch 29/120 - Validation: 100%|| 654/654 [00:03<00:00, 168.77step/s, Loss=129, Acc=22.7, RMSE=57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 57.017 at epoch 29\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 - Training: 100%|| 792/792 [03:36<00:00,  3.65step/s, Loss=118, Acc=32.3, Lr=0.001, L_mae=36.5, L_norm=37, L\n",
      "Epoch 30/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.43step/s, Loss=128, Acc=23.3, RMSE=56.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 56.365 at epoch 30\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 - Training: 100%|| 792/792 [03:49<00:00,  3.46step/s, Loss=118, Acc=32.4, Lr=0.001, L_mae=36.1, L_norm=36.9,\n",
      "Epoch 31/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.98step/s, Loss=129, Acc=21.9, RMSE=57.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=116, Acc=33.4, Lr=0.001, L_mae=35.3, L_norm=36.6,\n",
      "Epoch 32/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.39step/s, Loss=126, Acc=23.4, RMSE=56.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 56.264 at epoch 32\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=115, Acc=33.4, Lr=0.001, L_mae=35.2, L_norm=36.3,\n",
      "Epoch 33/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.41step/s, Loss=127, Acc=23, RMSE=56.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 - Training: 100%|| 792/792 [03:37<00:00,  3.65step/s, Loss=114, Acc=33.9, Lr=0.001, L_mae=34.6, L_norm=36.2,\n",
      "Epoch 34/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.85step/s, Loss=128, Acc=21.9, RMSE=56.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 - Training: 100%|| 792/792 [03:48<00:00,  3.47step/s, Loss=113, Acc=34.4, Lr=0.001, L_mae=34.3, L_norm=35.9,\n",
      "Epoch 35/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.20step/s, Loss=128, Acc=22.9, RMSE=56.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 - Training: 100%|| 792/792 [03:37<00:00,  3.64step/s, Loss=113, Acc=34.9, Lr=0.001, L_mae=33.7, L_norm=35.7,\n",
      "Epoch 36/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.26step/s, Loss=126, Acc=23.2, RMSE=56.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 - Training: 100%|| 792/792 [03:37<00:00,  3.63step/s, Loss=112, Acc=35.3, Lr=0.001, L_mae=33.4, L_norm=35.6,\n",
      "Epoch 37/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.35step/s, Loss=124, Acc=23.4, RMSE=55.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 55.397 at epoch 37\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=111, Acc=35.6, Lr=0.001, L_mae=33.1, L_norm=35.4,\n",
      "Epoch 38/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.60step/s, Loss=126, Acc=23.6, RMSE=55.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 55.338 at epoch 38\n",
      "New best ACCURACY: 23.608 at epoch 38\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 - Training: 100%|| 792/792 [03:50<00:00,  3.44step/s, Loss=111, Acc=35.8, Lr=0.001, L_mae=32.9, L_norm=35.3,\n",
      "Epoch 39/120 - Validation: 100%|| 654/654 [00:03<00:00, 169.47step/s, Loss=127, Acc=23.9, RMSE=56.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 23.888 at epoch 39\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=110, Acc=36, Lr=0.001, L_mae=32.5, L_norm=35.2, L\n",
      "Epoch 40/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.61step/s, Loss=124, Acc=24.1, RMSE=55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 54.980 at epoch 40\n",
      "New best ACCURACY: 24.119 at epoch 40\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 - Training: 100%|| 792/792 [03:48<00:00,  3.46step/s, Loss=109, Acc=36.9, Lr=0.001, L_mae=31.9, L_norm=34.8,\n",
      "Epoch 41/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.47step/s, Loss=124, Acc=23.3, RMSE=55.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=109, Acc=36.4, Lr=0.001, L_mae=32.1, L_norm=34.9,\n",
      "Epoch 42/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.29step/s, Loss=125, Acc=23.8, RMSE=55.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 - Training: 100%|| 792/792 [03:37<00:00,  3.64step/s, Loss=108, Acc=37, Lr=0.001, L_mae=31.7, L_norm=34.6, L\n",
      "Epoch 43/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.81step/s, Loss=125, Acc=23.6, RMSE=56.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=107, Acc=37.6, Lr=0.001, L_mae=31.2, L_norm=34.5,\n",
      "Epoch 44/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.20step/s, Loss=127, Acc=23.3, RMSE=56.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=107, Acc=37.9, Lr=0.001, L_mae=30.9, L_norm=34.5,\n",
      "Epoch 45/120 - Validation: 100%|| 654/654 [00:03<00:00, 166.92step/s, Loss=126, Acc=23.6, RMSE=57.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 - Training: 100%|| 792/792 [03:38<00:00,  3.63step/s, Loss=106, Acc=37.7, Lr=0.001, L_mae=31, L_norm=34.3, L\n",
      "Epoch 46/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.45step/s, Loss=126, Acc=22.4, RMSE=57.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=106, Acc=38.2, Lr=0.001, L_mae=30.6, L_norm=34.2,\n",
      "Epoch 47/120 - Validation: 100%|| 654/654 [00:03<00:00, 169.46step/s, Loss=124, Acc=24, RMSE=55.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 - Training: 100%|| 792/792 [03:40<00:00,  3.60step/s, Loss=105, Acc=38.4, Lr=0.001, L_mae=30.5, L_norm=34.2,\n",
      "Epoch 48/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.41step/s, Loss=125, Acc=23.2, RMSE=56.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=104, Acc=38.9, Lr=0.001, L_mae=30, L_norm=33.8, L\n",
      "Epoch 49/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.98step/s, Loss=123, Acc=24, RMSE=56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 21 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 - Training: 100%|| 792/792 [03:55<00:00,  3.37step/s, Loss=105, Acc=38.7, Lr=0.001, L_mae=30.2, L_norm=33.9,\n",
      "Epoch 50/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.01step/s, Loss=123, Acc=23.6, RMSE=54.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 54.458 at epoch 50\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=104, Acc=39.4, Lr=0.001, L_mae=29.7, L_norm=33.8,\n",
      "Epoch 51/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.37step/s, Loss=123, Acc=23.9, RMSE=55.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=104, Acc=39.1, Lr=0.001, L_mae=29.8, L_norm=33.6,\n",
      "Epoch 52/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.13step/s, Loss=125, Acc=23.9, RMSE=55.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=103, Acc=39.4, Lr=0.001, L_mae=29.6, L_norm=33.6,\n",
      "Epoch 53/120 - Validation: 100%|| 654/654 [00:03<00:00, 178.15step/s, Loss=126, Acc=23.6, RMSE=55.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=102, Acc=40, Lr=0.001, L_mae=29.1, L_norm=33.5, L\n",
      "Epoch 54/120 - Validation: 100%|| 654/654 [00:03<00:00, 168.44step/s, Loss=123, Acc=24.1, RMSE=54.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 - Training: 100%|| 792/792 [03:53<00:00,  3.39step/s, Loss=102, Acc=40.2, Lr=0.001, L_mae=29, L_norm=33.4, L\n",
      "Epoch 55/120 - Validation: 100%|| 654/654 [00:03<00:00, 168.71step/s, Loss=122, Acc=24.2, RMSE=54.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 54.131 at epoch 55\n",
      "New best ACCURACY: 24.210 at epoch 55\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=102, Acc=40.2, Lr=0.001, L_mae=28.9, L_norm=33.2,\n",
      "Epoch 56/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.54step/s, Loss=122, Acc=23.9, RMSE=54.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=101, Acc=40.6, Lr=0.001, L_mae=28.6, L_norm=33.2,\n",
      "Epoch 57/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.80step/s, Loss=123, Acc=24.5, RMSE=54.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 24.460 at epoch 57\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 - Training: 100%|| 792/792 [03:38<00:00,  3.62step/s, Loss=101, Acc=40.5, Lr=0.001, L_mae=28.7, L_norm=33.1,\n",
      "Epoch 58/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.38step/s, Loss=123, Acc=24, RMSE=55.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=101, Acc=40.8, Lr=0.001, L_mae=28.4, L_norm=33, L\n",
      "Epoch 59/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.93step/s, Loss=125, Acc=24.2, RMSE=55.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 - Training: 100%|| 792/792 [03:39<00:00,  3.61step/s, Loss=101, Acc=40.9, Lr=0.001, L_mae=28.4, L_norm=33.1,\n",
      "Epoch 60/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.95step/s, Loss=122, Acc=24.3, RMSE=55.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=100, Acc=41.1, Lr=0.001, L_mae=28.1, L_norm=32.8,\n",
      "Epoch 61/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.34step/s, Loss=122, Acc=24.6, RMSE=54.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "New best ACCURACY: 24.618 at epoch 61\n",
      "EarlyStopping increased due to Accuracy, stop in 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 - Training: 100%|| 792/792 [03:40<00:00,  3.59step/s, Loss=100, Acc=41.1, Lr=0.001, L_mae=28.2, L_norm=32.9,\n",
      "Epoch 62/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.59step/s, Loss=123, Acc=24.8, RMSE=55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 24.811 at epoch 62\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 - Training: 100%|| 792/792 [03:54<00:00,  3.37step/s, Loss=99.1, Acc=41.9, Lr=0.001, L_mae=27.7, L_norm=32.6\n",
      "Epoch 63/120 - Validation: 100%|| 654/654 [00:03<00:00, 169.98step/s, Loss=123, Acc=23.5, RMSE=55.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 - Training: 100%|| 792/792 [03:45<00:00,  3.52step/s, Loss=99.2, Acc=41.6, Lr=0.001, L_mae=27.8, L_norm=32.7\n",
      "Epoch 64/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.64step/s, Loss=123, Acc=24.4, RMSE=55.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 - Training: 100%|| 792/792 [03:45<00:00,  3.52step/s, Loss=99.3, Acc=41.5, Lr=0.001, L_mae=27.9, L_norm=32.7\n",
      "Epoch 65/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.43step/s, Loss=123, Acc=24.1, RMSE=55.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 - Training: 100%|| 792/792 [03:45<00:00,  3.52step/s, Loss=99, Acc=41.5, Lr=0.001, L_mae=27.7, L_norm=32.6, \n",
      "Epoch 66/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.66step/s, Loss=123, Acc=24.5, RMSE=54.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=98.5, Acc=42, Lr=0.001, L_mae=27.5, L_norm=32.5, \n",
      "Epoch 67/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.00step/s, Loss=121, Acc=24.8, RMSE=53.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 53.918 at epoch 67\n",
      "New best ACCURACY: 24.816 at epoch 67\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=98.1, Acc=42.3, Lr=0.001, L_mae=27.3, L_norm=32.4\n",
      "Epoch 68/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.71step/s, Loss=122, Acc=24.8, RMSE=54.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 24.824 at epoch 68\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=98.1, Acc=42.1, Lr=0.001, L_mae=27.4, L_norm=32.4\n",
      "Epoch 69/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.34step/s, Loss=124, Acc=23.7, RMSE=56.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/120 - Training: 100%|| 792/792 [03:46<00:00,  3.49step/s, Loss=98, Acc=42.3, Lr=0.001, L_mae=27.3, L_norm=32.4, \n",
      "Epoch 70/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.51step/s, Loss=122, Acc=24.1, RMSE=54.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=96.9, Acc=43.2, Lr=0.001, L_mae=26.8, L_norm=32.1\n",
      "Epoch 71/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.04step/s, Loss=123, Acc=23.9, RMSE=55.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/120 - Training: 100%|| 792/792 [03:42<00:00,  3.56step/s, Loss=97.2, Acc=42.9, Lr=0.001, L_mae=26.9, L_norm=32.2\n",
      "Epoch 72/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.48step/s, Loss=123, Acc=23.7, RMSE=55.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=97, Acc=42.8, Lr=0.001, L_mae=26.9, L_norm=32.1, \n",
      "Epoch 73/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.16step/s, Loss=122, Acc=25.1, RMSE=55.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "New best ACCURACY: 25.126 at epoch 73\n",
      "EarlyStopping increased due to Accuracy, stop in 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=97, Acc=42.8, Lr=0.001, L_mae=26.9, L_norm=32.2, \n",
      "Epoch 74/120 - Validation: 100%|| 654/654 [00:03<00:00, 175.78step/s, Loss=123, Acc=23.3, RMSE=55.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=96.6, Acc=43, Lr=0.001, L_mae=26.7, L_norm=32, L_\n",
      "Epoch 75/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.39step/s, Loss=122, Acc=23.5, RMSE=55.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/120 - Training: 100%|| 792/792 [03:46<00:00,  3.50step/s, Loss=96, Acc=43.3, Lr=0.001, L_mae=26.4, L_norm=31.8, \n",
      "Epoch 76/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.77step/s, Loss=122, Acc=24.4, RMSE=54.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/120 - Training: 100%|| 792/792 [03:41<00:00,  3.58step/s, Loss=96.2, Acc=43.5, Lr=0.001, L_mae=26.5, L_norm=32, \n",
      "Epoch 77/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.56step/s, Loss=123, Acc=24.1, RMSE=55.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/120 - Training: 100%|| 792/792 [03:46<00:00,  3.50step/s, Loss=95.9, Acc=43.4, Lr=0.001, L_mae=26.5, L_norm=31.8\n",
      "Epoch 78/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.26step/s, Loss=127, Acc=22.3, RMSE=57.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 21 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=95.6, Acc=43.8, Lr=0.001, L_mae=26.2, L_norm=31.8\n",
      "Epoch 79/120 - Validation: 100%|| 654/654 [00:03<00:00, 178.04step/s, Loss=123, Acc=22.9, RMSE=56.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 20 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/120 - Training: 100%|| 792/792 [03:47<00:00,  3.48step/s, Loss=95.6, Acc=43.5, Lr=0.001, L_mae=26.4, L_norm=31.7\n",
      "Epoch 80/120 - Validation: 100%|| 654/654 [00:03<00:00, 175.24step/s, Loss=123, Acc=24.5, RMSE=54.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 19 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/120 - Training: 100%|| 792/792 [03:46<00:00,  3.50step/s, Loss=95.4, Acc=43.9, Lr=0.001, L_mae=26.2, L_norm=31.7\n",
      "Epoch 81/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.12step/s, Loss=124, Acc=23.5, RMSE=55.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 18 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=95.2, Acc=43.8, Lr=0.001, L_mae=26.2, L_norm=31.6\n",
      "Epoch 82/120 - Validation: 100%|| 654/654 [00:03<00:00, 171.40step/s, Loss=125, Acc=23.2, RMSE=56.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 17 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/120 - Training: 100%|| 792/792 [03:45<00:00,  3.52step/s, Loss=95, Acc=44.2, Lr=0.001, L_mae=26, L_norm=31.6, L_\n",
      "Epoch 83/120 - Validation: 100%|| 654/654 [00:03<00:00, 174.64step/s, Loss=121, Acc=24.9, RMSE=53.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 53.869 at epoch 83\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/120 - Training: 100%|| 792/792 [03:46<00:00,  3.49step/s, Loss=94.6, Acc=44.5, Lr=0.001, L_mae=25.8, L_norm=31.5\n",
      "Epoch 84/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.52step/s, Loss=121, Acc=24.9, RMSE=54.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=94.9, Acc=44.3, Lr=0.001, L_mae=26, L_norm=31.6, \n",
      "Epoch 85/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.33step/s, Loss=121, Acc=24.6, RMSE=54.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/120 - Training: 100%|| 792/792 [03:56<00:00,  3.35step/s, Loss=94.6, Acc=44.3, Lr=0.001, L_mae=25.9, L_norm=31.6\n",
      "Epoch 86/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.52step/s, Loss=122, Acc=23.9, RMSE=54.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=94.2, Acc=44.8, Lr=0.001, L_mae=25.6, L_norm=31.4\n",
      "Epoch 87/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.27step/s, Loss=123, Acc=23.8, RMSE=55.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/120 - Training: 100%|| 792/792 [03:48<00:00,  3.47step/s, Loss=94.2, Acc=44.3, Lr=0.001, L_mae=25.8, L_norm=31.4\n",
      "Epoch 88/120 - Validation: 100%|| 654/654 [00:03<00:00, 173.00step/s, Loss=122, Acc=23.6, RMSE=56.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=94.2, Acc=44.5, Lr=0.001, L_mae=25.7, L_norm=31.4\n",
      "Epoch 89/120 - Validation: 100%|| 654/654 [00:03<00:00, 170.45step/s, Loss=122, Acc=25.1, RMSE=54.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/120 - Training: 100%|| 792/792 [03:40<00:00,  3.60step/s, Loss=94, Acc=44.7, Lr=0.001, L_mae=25.6, L_norm=31.4, \n",
      "Epoch 90/120 - Validation: 100%|| 654/654 [00:03<00:00, 172.59step/s, Loss=122, Acc=23.9, RMSE=56.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/120 - Training:   3%| | 25/792 [00:07<03:42,  3.45step/s, Loss=93.1, Acc=45.2, Lr=0.001, L_mae=25.2, L_norm=31.1,"
     ]
    }
   ],
   "source": [
    "def process(device):\n",
    "    # Set-seed\n",
    "    seed = param['seed']\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Datasets loading\n",
    "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "        dts_root_path=dataset_root,\n",
    "        rgb_h_res=param['img_res'][1],\n",
    "        d_h_res=param['depth_img_res'][1],\n",
    "        bs_train=param['batch_size'],\n",
    "        bs_eval=param['batch_size_eval'],\n",
    "        num_workers=param['n_workers'],\n",
    "    )\n",
    "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
    "    # Prints samples\n",
    "    print(' --- Test samples --- ')\n",
    "    print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
    "              save_model_root=save_model_root)\n",
    "    print(' --- Training augmented samples --- ')\n",
    "    print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
    "                  save_model_root=save_model_root)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # Globals\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
    "               'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
    "    min_rmse = float('inf')\n",
    "    min_acc = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    # Loss\n",
    "    criterion = balanced_loss_function(device=device)\n",
    "    # Model\n",
    "    model = build_model(device=device).to(device=device)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    print_model(model=model, input_shape=(1, *param['img_res']))\n",
    "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=param['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
    "    )\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=param['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
    "        cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
    "    )\n",
    "    # Early stopping\n",
    "    trigger_times, early_stopping_epochs = 0, param['e_stop_epochs']\n",
    "    print(\"Start training: {}\\n\".format(model_name))\n",
    "    \n",
    "    epochs = param['epochs']\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        iter = 1\n",
    "        model.train()\n",
    "        running_loss, accuracy = 0, 0\n",
    "        running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Forward\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # Compute loss\n",
    "                loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Evaluation and Stats\n",
    "                running_loss += loss.item()\n",
    "                running_l_mae += loss_depth.item()\n",
    "                running_l_norm += loss_normal.item()\n",
    "                running_l_grad += loss_grad.item()\n",
    "                running_l_ssim += loss_ssim.item()\n",
    "\n",
    "                train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                train_loss_list.append(train_loss_support)\n",
    "\n",
    "                accuracy += compute_accuracy(outputs, depths)\n",
    "                tepoch.set_postfix({'Loss': running_loss / iter,\n",
    "                                    'Acc': accuracy.item() / iter,\n",
    "                                    'Lr': param['lr'] if not history['lrs'] else history['lrs'][-1],\n",
    "                                    'L_mae': running_l_mae / iter,\n",
    "                                    'L_norm': running_l_norm / iter,\n",
    "                                    'L_grad': running_l_grad / iter,\n",
    "                                    'L_ssim': running_l_ssim / iter\n",
    "                                    })\n",
    "                iter += 1\n",
    "\n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    # Evaluation metrics\n",
    "                    test_accuracy += compute_accuracy(outputs, depths)\n",
    "                    # Loss\n",
    "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                    test_loss_list.append(test_loss_support)\n",
    "\n",
    "                    # RMSE\n",
    "                    test_rmse += compute_rmse(outputs, depths)\n",
    "                    tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
    "                                        'RMSE': test_rmse.item() / iter})\n",
    "                    iter += 1\n",
    "\n",
    "        # Update history infos\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        history['train_loss'].append(running_loss / len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss / len(test_DataLoader))\n",
    "        history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
    "        history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
    "        # Update history losses infos\n",
    "        history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
    "        history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
    "        history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
    "        history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
    "        # Update scheduler LR\n",
    "        scheduler.step(history['test_rmse'][-1])\n",
    "        # Save model by best RMSE\n",
    "        if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
    "            trigger_times = 0\n",
    "            min_rmse = test_rmse / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best', save_model_root)\n",
    "            print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
    "        # Save model by best ACCURACY\n",
    "        if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
    "            min_acc = test_accuracy / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
    "            print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
    "            if trigger_times > 4:\n",
    "                trigger_times = trigger_times - 2\n",
    "                print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
    "\n",
    "        save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
    "                                 save_path=save_model_root + 'evolution_img/')\n",
    "        save_history(history, save_model_root + model_name + '_history')\n",
    "        # Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if trigger_times == early_stopping_epochs:\n",
    "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
    "            break\n",
    "\n",
    "        # Save loss for graphs\n",
    "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
    "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
    "\n",
    "        print('Finished Training')\n",
    "        save_csv_history(model_name=model_name, path=save_model_root)\n",
    "        plot_history(history, path=save_model_root)\n",
    "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
    "\n",
    "        if os.path.exists(save_model_root + 'example&augment_img/'):\n",
    "            shutil.rmtree(save_model_root + 'example&augment_img/')\n",
    "\n",
    "\n",
    "    # model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
    "    # model, model_name = load_pretrained_model(model=model,\n",
    "    #                                           path_weigths=save_model_root + 'build_model_best',\n",
    "    #                                           device=device,\n",
    "    #                                           do_pretrained=global_var['do_pretrained'],\n",
    "    #                                           imagenet_w_init=global_var['imagenet_w_init'])\n",
    "    # if global_var['do_print_model']:\n",
    "    #     print_model(model=model, device=device, save_model_root=save_model_root,\n",
    "    #                 input_shape=global_var['RGB_img_res'])\n",
    "    # print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "\n",
    "    # Evaluate\n",
    "    print(' --- Begin evaluation --- ')\n",
    "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "    print(' --- End evaluation --- ')\n",
    "\n",
    "    sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "    save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "    save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware\n",
    "    device = hardware_check()\n",
    "\n",
    "    # -- TRAIN 1\n",
    "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
    "    # Directory test\n",
    "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
    "    #print(save_model_root)\n",
    "    # Create folders\n",
    "    if not os.path.exists(save_model_root):\n",
    "        os.makedirs(save_model_root)\n",
    "    # if not os.path.exists(save_model_root + 'info_code/'):\n",
    "    #     os.makedirs(save_model_root + 'info_code/')\n",
    "    # files_directory = '/work/project/'\n",
    "    # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
    "    # for f in files:\n",
    "    #     shutil.copy(f, save_model_root + 'info_code/')\n",
    "    # Run process\n",
    "    start_time = perf_counter()\n",
    "    process(device=device)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = perf_counter()\n",
    "    print(\"Total time elapsed: \",end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./meta_meterbuild_model_best')\n",
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "    dts_root_path=dataset_root,\n",
    "    rgb_h_res=param['img_res'][1],\n",
    "    d_h_res=param['depth_img_res'][1],\n",
    "    bs_train=param['batch_size'],\n",
    "    bs_eval=param['batch_size_eval'],\n",
    "    num_workers=param['n_workers'],\n",
    ")\n",
    "best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results='.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "g2bPw-5W4hJe",
    "Nm2TAq6B5UBI",
    "BoQYEe4V5j5E",
    "bbCnAwv453IN",
    "tAWQQQzf8ctn",
    "eAvmPzvA8Pu-",
    "EjiqGK4q42zP",
    "Ns4BuU6tsskL"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
