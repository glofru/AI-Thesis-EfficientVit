{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3fcf5a-8f76-40d2-b1b7-13086f3db534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Using cached torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.126254637Z",
     "start_time": "2023-10-15T00:35:21.094038559Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from timm.models.vision_transformer import trunc_normal_\n",
    "from timm.models.layers import SqueezeExcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c9bd930f7eb74d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.152808114Z",
     "start_time": "2023-10-15T00:35:21.097903351Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"seed\": 4242,\n",
    "    \"img_res\": (3, 256, 256),\n",
    "    \"depth_img_res\": (1, 64, 64),\n",
    "    \"n_workers\": 2,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"batch_size_eval\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_patience\": 15,\n",
    "    \"e_stop_epochs\": 30,\n",
    "    \"epochs\": 120,\n",
    "}\n",
    "\n",
    "augmentation_parameters = {\n",
    "    'flip': 0.5,\n",
    "    'mirror': 0.5,\n",
    "    'color&bright': 0.5,\n",
    "    'c_swap': 0.5,\n",
    "    'random_crop': 0.5,\n",
    "    'random_d_shift': 0.5  # range(+-10)cm\n",
    "}\n",
    "\n",
    "dataset_root = './data/NYUv2/'\n",
    "save_model_root = './results/v3_2/pyramid6_rmse_v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31c0b70b2a31fe",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b96bc0c825097de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.194945871Z",
     "start_time": "2023-10-15T00:35:21.146859600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def hardware_check():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Actual device: \", device)\n",
    "    if 'cuda' in device:\n",
    "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def plot_depth_map(dm):\n",
    "\n",
    "    MIN_DEPTH = 0.0\n",
    "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
    "\n",
    "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
    "    cmap = plt.cm.plasma_r\n",
    "\n",
    "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
    "\n",
    "\n",
    "def resize_keeping_aspect_ratio(img, base):\n",
    "    \"\"\"\n",
    "    Resize the image to a defined length manteining its proportions\n",
    "    Scaling the shortest side of the image to a fixed 'base' length'\n",
    "    \"\"\"\n",
    "\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        basewidth = int(base)\n",
    "        wpercent = (basewidth / float(img.shape[0]))\n",
    "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
    "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
    "    else:\n",
    "        baseheight = int(base)\n",
    "        wpercent = (baseheight / float(img.shape[1]))\n",
    "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
    "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, depths):\n",
    "    valid_mask = depths > 0.0\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
    "    valid_mask = y_true > 0.0\n",
    "    valid_pred = y_pred[valid_mask]\n",
    "    valid_true = y_true[valid_mask]\n",
    "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
    "    return 100 * torch.mean(correct.float())\n",
    "\n",
    "\n",
    "def print_model(model, input_shape):\n",
    "    info = summary(model, input_shape)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name, path_save_model):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    if not os.path.isdir(path_save_model):\n",
    "        os.makedirs(path_save_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path_save_model + name)\n",
    "\n",
    "\n",
    "def save_history(history, filepath):\n",
    "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "\n",
    "def save_csv_history(model_name, path):\n",
    "    objects = []\n",
    "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    df = pd.DataFrame(objects)\n",
    "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if do_pretrained:\n",
    "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
    "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"checkpoint loaded\\n\")\n",
    "\n",
    "    if imagenet_w_init:\n",
    "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
    "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model_dict = model.state_dict()\n",
    "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
    "\n",
    "        # pretrained_param = len(pretrained_dict.items())\n",
    "        counter_param = 0\n",
    "        for i, j in pretrained_dict.items():\n",
    "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
    "                counter_param += 1\n",
    "\n",
    "        print(f'Pertained parameters: {counter_param}\\n')\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        # alternativa to 2 e 3\n",
    "        # model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Partial initialization computed\\n\")\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title, path):\n",
    "    epochs = range(0, len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_history(history, path):\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
    "\n",
    "\n",
    "def plot_loss_parts(history, path, title):\n",
    "    l_mae_list = history['l_mae']\n",
    "    l_norm_list = history['l_norm']\n",
    "    l_grad_list = history['l_grad']\n",
    "    l_ssim_list = history['l_ssim']\n",
    "    epochs = range(0, len(l_mae_list))\n",
    "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
    "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
    "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
    "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
    "    for i in range(quantity):\n",
    "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
    "\n",
    "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
    "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
    "              f' variance =  {torch.var(img)}\\n')\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input image')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Grayscale DepthMap')\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Colored DepthMap')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        print(\"************************** \",save_model_root)\n",
    "        save_path = save_model_root + 'example&augment_img/'\n",
    "        print(\"************************** \",save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
    "    for i, index in zip(range(len(indices)), indices):\n",
    "        img, depth = dataset.__getitem__(index)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.from_numpy(img).to(device))\n",
    "            # Build plot\n",
    "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "            plt.subplot(1, len(indices), i+1)\n",
    "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
    "            if False:\n",
    "                plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
    "    save_path = save_model_root + type + '_predictions/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(list_type)):\n",
    "        index_image = list_type[i][0]\n",
    "        rmse_value = list_type[i][1]\n",
    "\n",
    "        img, depth = dataset.__getitem__(index=index_image)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f'Original image {index_image}')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title('Predicted DepthMap')\n",
    "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
    "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
    "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
    "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def compute_MeanVar(dataset):\n",
    "    r_mean, g_mean, b_mean = [], [], []\n",
    "    r_var, g_var, b_var = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r = np.array(img[0, :, :])\n",
    "        g = np.array(img[1, :, :])\n",
    "        b = np.array(img[2, :, :])\n",
    "\n",
    "        r_mean.append(np.mean(r))\n",
    "        g_mean.append(np.mean(g))\n",
    "        b_mean.append(np.mean(b))\n",
    "\n",
    "        r_var.append(np.var(r))\n",
    "        g_var.append(np.var(g))\n",
    "        b_var.append(np.var(b))\n",
    "\n",
    "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
    "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
    "\n",
    "\n",
    "def compute_MeanImg(dataset, save_model_root):\n",
    "    r, g, b = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r.append(np.array(img[0, :, :]))\n",
    "        g.append(np.array(img[1, :, :]))\n",
    "        b.append(np.array(img[2, :, :]))\n",
    "\n",
    "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
    "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
    "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
    "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
    "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
    "\n",
    "    print(\"Process Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ae2358f0c16d2",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e14242f44dcf3ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.195170649Z",
     "start_time": "2023-10-15T00:35:21.147039384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pixel_shift(depth_img, shift):\n",
    "    depth_img = depth_img + shift\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "def random_crop(x, y, crop_size=(192, 256)):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    h, w, _ = x.shape\n",
    "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
    "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
    "    if cropped_y.shape[-1] == 0:\n",
    "        return x, y\n",
    "    else:\n",
    "        return cropped_x, cropped_y\n",
    "\n",
    "\n",
    "def augmentation2D(img, depth, print_info_aug):\n",
    "    # Random flipping\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
    "        img = (img[..., ::1, :, :]).copy()\n",
    "        depth = (depth[..., ::1, :, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random flipped')\n",
    "    # Random mirroring\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
    "        img = (img[..., ::-1, :]).copy()\n",
    "        depth = (depth[..., ::-1, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random mirrored')\n",
    "    # Augment image\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
    "        # gamma augmentation\n",
    "        gamma = random.uniform(0.9, 1.1)\n",
    "        img = img ** gamma\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        img = img * brightness\n",
    "        # color augmentation\n",
    "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
    "        white = np.ones((img.shape[0], img.shape[1]))\n",
    "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        img *= color_image\n",
    "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
    "        if print_info_aug:\n",
    "            print('--> Image randomly augmented')\n",
    "    # Channel swap\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
    "        indices = list(product([0, 1, 2], repeat=3))\n",
    "        policy_idx = random.randint(0, len(indices) - 1)\n",
    "        img = img[..., list(indices[policy_idx])]\n",
    "        if print_info_aug:\n",
    "            print('--> Channel swapped')\n",
    "    # Random crop\n",
    "    if random.random() <= augmentation_parameters['random_crop']:\n",
    "        img, depth = random_crop(img, depth)\n",
    "        if print_info_aug:\n",
    "            print('--> Random cropped')\n",
    "    # Depth Shift\n",
    "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
    "        random_shift = random.randint(-10, 10)\n",
    "        depth = pixel_shift(depth, shift=random_shift)\n",
    "        if print_info_aug:\n",
    "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
    "\n",
    "    return img, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993932aac8813d7",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e40cba8dfbdb6020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.196198980Z",
     "start_time": "2023-10-15T00:35:21.147176850Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NYU2_Dataset:\n",
    "    \"\"\"\n",
    "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
    "      * 654 Test and 50688 Train images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
    "        self.dataset = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.info = 0\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.rgb_h_res = rgb_h_res\n",
    "        self.d_h_res = d_h_res\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "        # Handle dataset\n",
    "        if self.dts_type == 'test':\n",
    "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
    "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
    "\n",
    "            rgb = np.load(img_path)\n",
    "            depth = np.load(depth_path)\n",
    "\n",
    "            self.x = rgb\n",
    "            self.y = depth\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = rgb[:dts_size]\n",
    "                self.y = depth[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        elif self.dts_type == 'train':\n",
    "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
    "            for scene in scenarios:\n",
    "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
    "                for el in elem:\n",
    "                    if 'jpg' in el:\n",
    "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    elif 'png' in el:\n",
    "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    else:\n",
    "                        raise SystemError('Type image error (train)')\n",
    "\n",
    "            if len(self.x) != len(self.y):\n",
    "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
    "\n",
    "            self.x.sort()\n",
    "            self.y.sort()\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = self.x[:dts_size]\n",
    "                self.y = self.y[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        else:\n",
    "            raise SystemError('Problem in the path')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def __getitem__(self, index=None, print_info_aug=False):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "\n",
    "        # Load Image\n",
    "        if self.dts_type == 'test':\n",
    "            img = self.x[index]\n",
    "        else:\n",
    "            img_name = self.dataset + self.x[index]\n",
    "            try:\n",
    "                raw_img = Image.open(img_name)\n",
    "                img = np.array(raw_img.convert('RGB'))\n",
    "                raw_img.close()\n",
    "            except:\n",
    "                exit(f\"Failed opening {img_name}\")\n",
    "\n",
    "        # Load Depth Image\n",
    "        if self.dts_type == 'test':\n",
    "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
    "        else:\n",
    "            depth = Image.open(self.dataset + self.y[index])\n",
    "            depth = np.array(depth) / 255\n",
    "            depth = np.clip(depth * 1000, 50, 1000)\n",
    "            depth = np.expand_dims(depth, axis=-1)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
    "\n",
    "        img_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['img_res'][1], param['img_res'][2]), antialias=True),\n",
    "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
    "        ])\n",
    "        depth_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['depth_img_res'][1], param['depth_img_res'][2]), antialias=True),\n",
    "        ])\n",
    "\n",
    "        img = img_post_processing(img/255)\n",
    "        depth = depth_post_processing(depth)\n",
    "\n",
    "        return img.float(), depth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "109124c84a5ace20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.213384206Z",
     "start_time": "2023-10-15T00:35:21.162874260Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def init_train_test_loader(dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
    "    # Load Datasets\n",
    "    test_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
    "    )\n",
    "    training_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
    "    )\n",
    "    # Create Dataloaders\n",
    "    training_DataLoader = DataLoader(\n",
    "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    test_DataLoader = DataLoader(\n",
    "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d87f8c10eb08df3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.213525128Z",
     "start_time": "2023-10-15T00:35:21.210956245Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class balanced_loss_function(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / 2\n",
    "\n",
    "        return loss_depth, loss_ssim, loss_normal, loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eef5ed64d70f1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6a8541506bb85f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.255077358Z",
     "start_time": "2023-10-15T00:35:21.211155385Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def conv_1x1_bn(inp, oup):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "#         nn.BatchNorm2d(oup),\n",
    "#         nn.ReLU()  # nn.SiLU()\n",
    "#     )\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth, kernel_size=kernel_size, groups=depth, padding=1, stride=stride, bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "#     return nn.Sequential(\n",
    "#         # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "#         SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "#                         bias=False, device='cpu'),\n",
    "#         nn.BatchNorm2d(oup),\n",
    "#         nn.ReLU()  # nn.SiLU()\n",
    "#     )\n",
    "\n",
    "\n",
    "# class PreNorm(nn.Module):\n",
    "#     def __init__(self, dim, fn):\n",
    "#         super().__init__()\n",
    "#         self.norm = nn.LayerNorm(dim)\n",
    "#         self.fn = fn\n",
    "\n",
    "#     def forward(self, x, **kwargs):\n",
    "#         return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(dim, hidden_dim),\n",
    "#             nn.ReLU(),  # nn.SiLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, dim),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "#         self.heads = heads\n",
    "#         self.dim_head = dim_head\n",
    "#         # head_dim = dim // heads\n",
    "#         self.scale = dim_head ** -0.5\n",
    "#         # print(\"------------------------------------DIM--------------------------\", dim)\n",
    "#         self.q = nn.Linear(dim, dim, bias=False)\n",
    "#         self.kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "#         self.attn_drop = nn.Dropout(0)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(0)\n",
    "\n",
    "#         self.sr_ratio = 4\n",
    "#         if self.sr_ratio > 1:\n",
    "#             self.sr = nn.Conv2d(dim, dim, kernel_size=self.sr_ratio, stride=self.sr_ratio)\n",
    "#             self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C, W = x.shape # torch.Size([1, 4, 192, 144])\n",
    "\n",
    "#         q = self.q(x)\n",
    "\n",
    "#         if self.sr_ratio > 1:\n",
    "#             # x_ = x.permute(0, 2, 1).reshape(B, C, N, W)\n",
    "#             x_ = x.reshape(B, W, N, C)\n",
    "#             # print(\"-------------------------------------------------------------------\", x_.size)\n",
    "#             x_ = self.sr(x_).reshape(B, -1, self.dim).permute(0, 2, 1)\n",
    "#             # print(\"-------------------------------------------------------------------\", x_.size)\n",
    "#             x_ = self.norm(x_.permute(0, 2, 1))\n",
    "#             kv = self.kv(x_).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "#         else:\n",
    "#             kv = self.kv(x).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "#         k, v = kv[0], kv[1]\n",
    "\n",
    "#         # q = q.reshape([q.shape[0], q.shape[1], q.shape[2]*(q.shape[3]//k.shape[3]), k.shape[3]])\n",
    "#         q = q.reshape([q.shape[0], q.shape[1], (q.shape[0]*q.shape[1]*q.shape[2]*q.shape[3])//(q.shape[0]*q.shape[1]*k.shape[3]), k.shape[3]])    # use this for xxs architecture\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C, W)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([])\n",
    "#         for _ in range(depth):\n",
    "#             self.layers.append(nn.ModuleList([\n",
    "#                 PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "#                 PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "#             ]))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for attn, ff in self.layers:\n",
    "#             x = attn(x) + x\n",
    "#             x = ff(x) + x\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class MV2Block(nn.Module):\n",
    "#     def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "#         super().__init__()\n",
    "#         self.stride = stride\n",
    "#         assert stride in [1, 2]\n",
    "\n",
    "#         hidden_dim = int(inp * expansion)\n",
    "#         self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "#         if expansion == 1:\n",
    "#             self.conv = nn.Sequential(\n",
    "#                 # dw\n",
    "#                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # pw-linear\n",
    "#                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#             )\n",
    "#         else:\n",
    "#             self.conv = nn.Sequential(\n",
    "#                 # pw\n",
    "#                 nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # dw\n",
    "#                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # pw-linear\n",
    "#                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.use_res_connect:\n",
    "#             return x + self.conv(x)\n",
    "#         else:\n",
    "#             return self.conv(x)\n",
    "\n",
    "\n",
    "# class MobileViTBlock(nn.Module):\n",
    "#     def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.ph, self.pw = patch_size\n",
    "\n",
    "#         self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "#         self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "#         self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "#         self.conv3 = conv_1x1_bn(dim, channel)\n",
    "#         self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = x.clone()\n",
    "\n",
    "#         # Local representations\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "\n",
    "#         #print(\"*********************************** Start logging ***********************************\")\n",
    "#         #print(\"Transformer input shape: \",x.shape)\n",
    "\n",
    "#         # Global representations\n",
    "#         _, _, h, w = x.shape\n",
    "#         x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
    "#         #print(\"Rearranged input shape: \",x.shape)\n",
    "\n",
    "#         start_time = perf_counter() ############################## Time measurament\n",
    "#         x = self.transformer(x)\n",
    "#         end_time = perf_counter() ############################## Time measurament\n",
    "\n",
    "#         #print(\"Transformer output shape: \",x.shape)\n",
    "#         x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
    "#                       pw=self.pw)\n",
    "#         #print(\"Rearranged output shape: \",x.shape)\n",
    "#         #print(\"**************************************************************************************\")\n",
    "#         # Fusion\n",
    "#         x = self.conv3(x)\n",
    "#         x = torch.cat((x, y), 1)\n",
    "#         x = self.conv4(x)\n",
    "#         return x, end_time-start_time ############################## Time measurament\n",
    "\n",
    "\n",
    "# class MobileViT(nn.Module):\n",
    "#     def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "#         super().__init__()\n",
    "#         ih, iw = image_size\n",
    "#         ph, pw = patch_size\n",
    "#         assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "#         self.transformer_times = transformer_times ############################## Time measurament\n",
    "#         self.sample_cnt = sample_cnt ############################## Time measurament\n",
    "\n",
    "#         L = [1, 1, 1]  # L = [2, 4, 3] # --> +5 FPS\n",
    "\n",
    "#         self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "#         self.mv2 = nn.ModuleList([])\n",
    "#         self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
    "#         self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "#         self.mvit = nn.ModuleList([])\n",
    "#         self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
    "#         self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
    "#         self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
    "\n",
    "#         self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "#         # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "#         # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y0 = self.conv1(x)\n",
    "#         x = self.mv2[0](y0)\n",
    "\n",
    "#         y1 = self.mv2[1](x)\n",
    "#         x = self.mv2[2](y1)\n",
    "#         x = self.mv2[3](x)  # Repeat\n",
    "\n",
    "#         y2 = self.mv2[4](x)\n",
    "#         x,mvit_time_1 = self.mvit[0](y2)\n",
    "#         self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
    "\n",
    "#         y3 = self.mv2[5](x)\n",
    "#         x,mvit_time_2 = self.mvit[1](y3)\n",
    "#         self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
    "\n",
    "#         x = self.mv2[6](x)\n",
    "#         x,mvit_time_3 = self.mvit[2](x)\n",
    "#         self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
    "#         x = self.conv2(x)\n",
    "\n",
    "#         self.sample_cnt += 1 ############################## Time measurament\n",
    "#         if(self.sample_cnt == 655):\n",
    "#             self.sample_cnt = 0\n",
    "\n",
    "#         return x, [y0, y1, y2, y3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8def71b7-d3dc-424b-83a3-65762110c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mobilevit_xxs(transformer_times, sample_cnt): ############################## Time measurament\n",
    "#     enc_type = 'xxs'\n",
    "#     dims = [64, 80, 96]\n",
    "#     channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]  # 320\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000, expansion=2,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "# def mobilevit_xs(transformer_times, sample_cnt):\n",
    "#     enc_type = 'xs'\n",
    "#     dims = [96, 120, 144]\n",
    "#     channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 192] # 384\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "# def mobilevit_s(transformer_times, sample_cnt):\n",
    "#     enc_type = 's'\n",
    "#     dims = [144, 192, 240]\n",
    "#     channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e9ba174d-d2b7-41ca-b466-291211dc5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim, input_resolution):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0, resolution=input_resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim, resolution=input_resolution)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0, resolution=input_resolution // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h, resolution):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h, resolution=resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0, resolution=resolution)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CascadedGroupAttention(torch.nn.Module):\n",
    "    r\"\"\" Cascaded Group Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution, correspond to the window size.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.attn_ratio = attn_ratio\n",
    "\n",
    "        qkvs = []\n",
    "        dws = []\n",
    "        for i in range(num_heads):\n",
    "            qkvs.append(Conv2d_BN(dim // (num_heads), self.key_dim * 2 + self.d, resolution=resolution))\n",
    "            dws.append(Conv2d_BN(self.key_dim, self.key_dim, kernels[i], 1, kernels[i]//2, groups=self.key_dim, resolution=resolution))\n",
    "        self.qkvs = torch.nn.ModuleList(qkvs)\n",
    "        self.dws = torch.nn.ModuleList(dws)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            self.d * num_heads, dim, bn_weight_init=0, resolution=resolution))\n",
    "\n",
    "        points = list(itertools.product(range(resolution), range(resolution)))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "        trainingab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "        feats_in = x.chunk(len(self.qkvs), dim=1)\n",
    "        feats_out = []\n",
    "        feat = feats_in[0]\n",
    "        for i, qkv in enumerate(self.qkvs):\n",
    "            if i > 0: # add the previous output to the input\n",
    "                feat = feat + feats_in[i]\n",
    "            feat = qkv(feat)\n",
    "            q, k, v = feat.view(B, -1, H, W).split([self.key_dim, self.key_dim, self.d], dim=1) # B, C/h, H, W\n",
    "            q = self.dws[i](q)\n",
    "            q, k, v = q.flatten(2), k.flatten(2), v.flatten(2) # B, C/h, N\n",
    "            attn = (\n",
    "                (q.transpose(-2, -1) @ k) * self.scale\n",
    "                +\n",
    "                (trainingab[i] if self.training else self.ab[i])\n",
    "            )\n",
    "            attn = attn.softmax(dim=-1) # BNN\n",
    "            feat = (v @ attn.transpose(-2, -1)).view(B, self.d, H, W) # BCHW\n",
    "            feats_out.append(feat)\n",
    "        x = self.proj(torch.cat(feats_out, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocalWindowAttention(torch.nn.Module):\n",
    "    r\"\"\" Local Window Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.resolution = resolution\n",
    "        assert window_resolution > 0, 'window_size must be greater than 0'\n",
    "        self.window_resolution = window_resolution\n",
    "        \n",
    "        window_resolution = min(window_resolution, resolution)\n",
    "        self.attn = CascadedGroupAttention(dim, key_dim, num_heads,\n",
    "                                attn_ratio=attn_ratio, \n",
    "                                resolution=window_resolution,\n",
    "                                kernels=kernels,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = W = self.resolution\n",
    "        B, C, H_, W_ = x.shape\n",
    "        # Only check this for classifcation models\n",
    "        assert H == H_ and W == W_, 'input feature has wrong size, expect {}, got {}'.format((H, W), (H_, W_))\n",
    "               \n",
    "        if H <= self.window_resolution and W <= self.window_resolution:\n",
    "            x = self.attn(x)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            pad_b = (self.window_resolution - H %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            pad_r = (self.window_resolution - W %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            padding = pad_b > 0 or pad_r > 0\n",
    "\n",
    "            if padding:\n",
    "                x = torch.nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "\n",
    "            pH, pW = H + pad_b, W + pad_r\n",
    "            nH = pH // self.window_resolution\n",
    "            nW = pW // self.window_resolution\n",
    "            # window partition, BHWC -> B(nHh)(nWw)C -> BnHnWhwC -> (BnHnW)hwC -> (BnHnW)Chw\n",
    "            x = x.view(B, nH, self.window_resolution, nW, self.window_resolution, C).transpose(2, 3).reshape(\n",
    "                B * nH * nW, self.window_resolution, self.window_resolution, C\n",
    "            ).permute(0, 3, 1, 2)\n",
    "            x = self.attn(x)\n",
    "            # window reverse, (BnHnW)Chw -> (BnHnW)hwC -> BnHnWhwC -> B(nHh)(nWw)C -> BHWC\n",
    "            x = x.permute(0, 2, 3, 1).view(B, nH, nW, self.window_resolution, self.window_resolution,\n",
    "                       C).transpose(2, 3).reshape(B, pH, pW, C)\n",
    "            if padding:\n",
    "                x = x[:, :H, :W].contiguous()\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientViTBlock(torch.nn.Module):    \n",
    "    \"\"\" A basic EfficientViT building block.\n",
    "\n",
    "    Args:\n",
    "        type (str): Type for token mixer. Default: 's' for self-attention.\n",
    "        ed (int): Number of input channels.\n",
    "        kd (int): Dimension for query and key in the token mixer.\n",
    "        nh (int): Number of attention heads.\n",
    "        ar (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, type,\n",
    "                 ed, kd, nh=8,\n",
    "                 ar=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.dw0 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn0 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "        if type == 's':\n",
    "            self.mixer = Residual(LocalWindowAttention(ed, kd, nh, attn_ratio=ar, \\\n",
    "                    resolution=resolution, window_resolution=window_resolution, kernels=kernels))\n",
    "                \n",
    "        self.dw1 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn1 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn1(self.dw1(self.mixer(self.ffn0(self.dw0(x)))))\n",
    "\n",
    "\n",
    "class EfficientViT(torch.nn.Module):\n",
    "    def __init__(self, img_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 stages=['s', 's', 's'],\n",
    "                 embed_dim=[64, 128, 192],\n",
    "                 key_dim=[16, 16, 16],\n",
    "                 depth=[1, 2, 3],\n",
    "                 num_heads=[4, 4, 4],\n",
    "                 window_size=[7, 7, 7],\n",
    "                 kernels=[5, 5, 5, 5],\n",
    "                 down_ops=[['subsample', 2], ['subsample', 2], ['']],\n",
    "                 distillation=False,):\n",
    "        super().__init__()\n",
    "\n",
    "        resolution = img_size\n",
    "        # Patch embedding\n",
    "        self.patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1, resolution=resolution), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1, resolution=resolution // 2), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1, resolution=resolution // 4), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1, resolution=resolution // 8))\n",
    "\n",
    "        resolution = img_size // patch_size\n",
    "        attn_ratio = [embed_dim[i] / (key_dim[i] * num_heads[i]) for i in range(len(embed_dim))]\n",
    "        self.blocks1 = []\n",
    "        self.blocks2 = []\n",
    "        self.blocks3 = []\n",
    "\n",
    "        # Build EfficientViT blocks\n",
    "        for i, (stg, ed, kd, dpth, nh, ar, wd, do) in enumerate(\n",
    "                zip(stages, embed_dim, key_dim, depth, num_heads, attn_ratio, window_size, down_ops)):\n",
    "            for d in range(dpth):\n",
    "                eval('self.blocks' + str(i+1)).append(EfficientViTBlock(stg, ed, kd, nh, ar, resolution, wd, kernels))\n",
    "            if do[0] == 'subsample':\n",
    "                # Build EfficientViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('self.blocks' + str(i+2))\n",
    "                resolution_ = (resolution - 1) // do[1] + 1\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i], resolution=resolution)),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2), resolution)),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2], resolution))\n",
    "                resolution = resolution_\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1], resolution=resolution)),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2), resolution)),))\n",
    "        self.blocks1 = torch.nn.Sequential(*self.blocks1)\n",
    "        self.blocks2 = torch.nn.Sequential(*self.blocks2)\n",
    "        self.blocks3 = torch.nn.Sequential(*self.blocks3)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {x for x in self.state_dict().keys() if 'attention_biases' in x}\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.patch_embed(x)\n",
    "        y1 = self.blocks1(y0)\n",
    "        y2 = self.blocks2(y1)\n",
    "        y3 = self.blocks3(y2)\n",
    "\n",
    "        return y3, [y0, y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a15571ab-97f2-488a-bab2-354168b94280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        # print(f\"Enc layer pre: {enc_layer.shape}\")\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            pad = (x.shape[-1] - enc_layer.shape[-1]) // 2\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(pad, pad, pad, pad), mode='constant', value=0.0)\n",
    "        # print(f\"X: {x.shape}\")\n",
    "        # print(f\"Enc layer post: {enc_layer.shape}\")\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        # print(f\"Final X: {x.shape}\")\n",
    "        x = self.end_up_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, device, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(192, 128, kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128, 64, flag=True, sep_conv_filters=192, name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(64, 32, flag=False, sep_conv_filters=96 if typ == 's' else 96 if typ == 'xs' else 64, name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(32, 64, flag=False, sep_conv_filters=128 if typ == 's' else 64 if typ == 'xs' else 32, name='up3', device=device)\n",
    "        self.ups_block_4 = UpSample_layer(64, 64, flag=False, sep_conv_filters=128 if typ == 's' else 64 if typ == 'xs' else 32, name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(64, 1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[2])\n",
    "        x = self.ups_block_2(x, enc_layer_list[1])\n",
    "        x = self.ups_block_3(x, enc_layer_list[0])\n",
    "        x = self.ups_block_4(x, x)\n",
    "        x = self.conv2d_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c4bb83e7-c987-42eb-9651-fc2676b2e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_model(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(build_model, self).__init__()\n",
    "        # self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
    "        # self.sample_cnt = 0 ############################## Time measurament\n",
    "        # self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
    "        # self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
    "\n",
    "        self.encoder = EfficientViT(img_size=param['img_res'][1]) ############################## Time measurament\n",
    "        self.decoder = SPEED_decoder(device=device, typ='s')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b3d1d-896e-4872-b055-facadf8ce8ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a5f7fe73-d04a-4bfe-82f8-b09e51abe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    return torch.log(x) / math.log(10)\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.irmse, self.imae = 0, 0\n",
    "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
    "        self.absrel, self.lg10 = 0, 0\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def set_to_worst(self):\n",
    "        self.irmse, self.imae = np.inf, np.inf\n",
    "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
    "        self.absrel, self.lg10 = np.inf, np.inf\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
    "        self.irmse, self.imae = irmse, imae\n",
    "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
    "        self.absrel, self.lg10 = absrel, lg10\n",
    "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
    "\n",
    "    def evaluate(self, output, target):\n",
    "        valid_mask = target > 0\n",
    "\n",
    "        output = output[valid_mask]\n",
    "        target = target[valid_mask]\n",
    "        \n",
    "\n",
    "        abs_diff = (output - target).abs()\n",
    "\n",
    "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
    "        self.rmse = math.sqrt(self.mse)\n",
    "        self.mae = float(abs_diff.mean())\n",
    "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
    "        self.absrel = float((abs_diff / target).mean())\n",
    "\n",
    "        maxRatio = torch.max(output / target, target / output)\n",
    "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
    "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
    "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
    "\n",
    "        inv_output = 1 / output\n",
    "        inv_target = 1 / target\n",
    "        abs_inv_diff = (inv_output - inv_target).abs()\n",
    "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
    "        self.imae = float(abs_inv_diff.mean())\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0.0\n",
    "        self.sum_irmse, self.sum_imae = 0, 0\n",
    "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
    "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
    "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, result, n=1):\n",
    "        self.count += n\n",
    "\n",
    "        self.sum_irmse += n * result.irmse\n",
    "        self.sum_imae += n * result.imae\n",
    "        self.sum_mse += n * result.mse\n",
    "        self.sum_rmse += n * result.rmse\n",
    "        self.sum_mae += n * result.mae\n",
    "        self.sum_absrel += n * result.absrel\n",
    "        self.sum_lg10 += n * result.lg10\n",
    "        self.sum_delta1 += n * result.delta1\n",
    "        self.sum_delta2 += n * result.delta2\n",
    "        self.sum_delta3 += n * result.delta3\n",
    "\n",
    "    def average(self):\n",
    "        avg = Result()\n",
    "        avg.update(\n",
    "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
    "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
    "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
    "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
    "        return avg\n",
    "\n",
    "\n",
    "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
    "    best_worst_dict = {}\n",
    "    result = Result()\n",
    "    result.set_to_worst()\n",
    "    average_meter = AverageMeter()\n",
    "    model.eval()  # switch to evaluate mode\n",
    "\n",
    "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
    "        inputs, depths = inputs.cuda(), depths.cuda()\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "        result.evaluate(predictions, depths)\n",
    "        average_meter.update(result)  # (result, inputs.size(0))\n",
    "        best_worst_dict[i] = result.rmse\n",
    "\n",
    "    avg = average_meter.average()\n",
    "\n",
    "    print('MAE={average.mae:.3f}\\n'\n",
    "          'RMSE={average.rmse:.3f}\\n'\n",
    "          'Delta1={average.delta1:.3f}\\n'\n",
    "          'REL={average.absrel:.3f}\\n'\n",
    "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
    "\n",
    "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
    "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
    "\n",
    "    return best_worst_dict, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de100e85-e556-48bd-a6fc-01ae631351ad",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "91da3393-1ccb-48d3-b76f-5b7c4f1a1e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EfficientViT: 1-1                           [-1, 192, 4, 4]           --\n",
      "|    Sequential: 2-1                        [-1, 64, 16, 16]          --\n",
      "|    |    Conv2d_BN: 3-1                    [-1, 8, 128, 128]         232\n",
      "|    |    ReLU: 3-2                         [-1, 8, 128, 128]         --\n",
      "|    |    Conv2d_BN: 3-3                    [-1, 16, 64, 64]          1,184\n",
      "|    |    ReLU: 3-4                         [-1, 16, 64, 64]          --\n",
      "|    |    Conv2d_BN: 3-5                    [-1, 32, 32, 32]          4,672\n",
      "|    |    ReLU: 3-6                         [-1, 32, 32, 32]          --\n",
      "|    |    Conv2d_BN: 3-7                    [-1, 64, 16, 16]          18,560\n",
      "|    Sequential: 2-2                        [-1, 64, 16, 16]          --\n",
      "|    |    EfficientViTBlock: 3-8            [-1, 64, 16, 16]          44,548\n",
      "|    Sequential: 2-3                        [-1, 128, 8, 8]           --\n",
      "|    |    Sequential: 3-9                   [-1, 64, 16, 16]          17,472\n",
      "|    |    PatchMerging: 3-10                [-1, 128, 8, 8]           85,824\n",
      "|    |    Sequential: 3-11                  [-1, 128, 8, 8]           67,712\n",
      "|    |    EfficientViTBlock: 3-12           [-1, 128, 8, 8]           162,692\n",
      "|    |    EfficientViTBlock: 3-13           [-1, 128, 8, 8]           162,692\n",
      "|    Sequential: 2-4                        [-1, 192, 4, 4]           --\n",
      "|    |    Sequential: 3-14                  [-1, 128, 8, 8]           67,712\n",
      "|    |    PatchMerging: 3-15                [-1, 192, 4, 4]           302,592\n",
      "|    |    Sequential: 3-16                  [-1, 192, 4, 4]           150,720\n",
      "|    |    EfficientViTBlock: 3-17           [-1, 192, 4, 4]           356,480\n",
      "|    |    EfficientViTBlock: 3-18           [-1, 192, 4, 4]           356,480\n",
      "|    |    EfficientViTBlock: 3-19           [-1, 192, 4, 4]           356,480\n",
      "SPEED_decoder: 1-2                          [-1, 1, 64, 64]           --\n",
      "|    Conv2d: 2-5                            [-1, 128, 4, 4]           24,576\n",
      "|    UpSample_layer: 2-6                    [-1, 64, 8, 8]            --\n",
      "|    |    ConvTranspose2d: 3-20             [-1, 64, 8, 8]            73,728\n",
      "|    |    Sequential: 3-21                  [-1, 64, 8, 8]            114,688\n",
      "|    UpSample_layer: 2-7                    [-1, 32, 16, 16]          --\n",
      "|    |    ConvTranspose2d: 3-22             [-1, 32, 16, 16]          18,432\n",
      "|    |    Sequential: 3-23                  [-1, 32, 16, 16]          28,672\n",
      "|    UpSample_layer: 2-8                    [-1, 64, 32, 32]          --\n",
      "|    |    ConvTranspose2d: 3-24             [-1, 64, 32, 32]          18,432\n",
      "|    |    Sequential: 3-25                  [-1, 64, 32, 32]          77,824\n",
      "|    UpSample_layer: 2-9                    [-1, 64, 64, 64]          --\n",
      "|    |    ConvTranspose2d: 3-26             [-1, 64, 64, 64]          36,864\n",
      "|    |    Sequential: 3-27                  [-1, 64, 64, 64]          77,824\n",
      "|    Conv2d: 2-10                           [-1, 1, 64, 64]           576\n",
      "===============================================================================================\n",
      "Total params: 2,627,668\n",
      "Trainable params: 2,627,668\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 6.39\n",
      "Params size (MB): 10.02\n",
      "Estimated Total Size (MB): 17.16\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "print_model(model=model, input_shape=param['img_res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a005fab024ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:27.628147620Z",
     "start_time": "2023-10-15T00:35:21.255178486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: There are 50688 training and 654 testing samples\n",
      " --- Test samples --- \n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 509.86090087890625, min = 107.78614044189453\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.29162493348121643, variance =  1.499471664428711\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 992.6409301757812, min = 187.010009765625\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.114142417907715, mean = -0.197127565741539, variance =  1.3339691162109375\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Training augmented samples --- \n",
      "--> Random flipped\n",
      "--> Random mirrored\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 389.2088928222656, min = 82.37699127197266\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.8259815573692322, variance =  1.5658296346664429\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "--> Random flipped\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 470.4520568847656, min = 272.95751953125\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 0.43080082535743713, variance =  1.7236429452896118\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random mirrored\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 990.6998901367188, min = 302.8186340332031\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 1.0471901893615723, variance =  0.8517109155654907\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "--> Depth Shifted of -1 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 588.3338623046875, min = 137.83006286621094\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1136350631713867, mean = 0.27235546708106995, variance =  2.357508420944214\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Image randomly augmented\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 994.4202880859375, min = 149.62440490722656\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.149726629257202, min = -2.0929322242736816, mean = 0.040091175585985184, variance =  1.2828030586242676\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EfficientViT: 1-1                           [-1, 192, 4, 4]           --\n",
      "|    Sequential: 2-1                        [-1, 64, 16, 16]          --\n",
      "|    |    Conv2d_BN: 3-1                    [-1, 8, 128, 128]         232\n",
      "|    |    ReLU: 3-2                         [-1, 8, 128, 128]         --\n",
      "|    |    Conv2d_BN: 3-3                    [-1, 16, 64, 64]          1,184\n",
      "|    |    ReLU: 3-4                         [-1, 16, 64, 64]          --\n",
      "|    |    Conv2d_BN: 3-5                    [-1, 32, 32, 32]          4,672\n",
      "|    |    ReLU: 3-6                         [-1, 32, 32, 32]          --\n",
      "|    |    Conv2d_BN: 3-7                    [-1, 64, 16, 16]          18,560\n",
      "|    Sequential: 2-2                        [-1, 64, 16, 16]          --\n",
      "|    |    EfficientViTBlock: 3-8            [-1, 64, 16, 16]          44,548\n",
      "|    Sequential: 2-3                        [-1, 128, 8, 8]           --\n",
      "|    |    Sequential: 3-9                   [-1, 64, 16, 16]          17,472\n",
      "|    |    PatchMerging: 3-10                [-1, 128, 8, 8]           85,824\n",
      "|    |    Sequential: 3-11                  [-1, 128, 8, 8]           67,712\n",
      "|    |    EfficientViTBlock: 3-12           [-1, 128, 8, 8]           162,692\n",
      "|    |    EfficientViTBlock: 3-13           [-1, 128, 8, 8]           162,692\n",
      "|    Sequential: 2-4                        [-1, 192, 4, 4]           --\n",
      "|    |    Sequential: 3-14                  [-1, 128, 8, 8]           67,712\n",
      "|    |    PatchMerging: 3-15                [-1, 192, 4, 4]           302,592\n",
      "|    |    Sequential: 3-16                  [-1, 192, 4, 4]           150,720\n",
      "|    |    EfficientViTBlock: 3-17           [-1, 192, 4, 4]           356,480\n",
      "|    |    EfficientViTBlock: 3-18           [-1, 192, 4, 4]           356,480\n",
      "|    |    EfficientViTBlock: 3-19           [-1, 192, 4, 4]           356,480\n",
      "SPEED_decoder: 1-2                          [-1, 1, 64, 64]           --\n",
      "|    Conv2d: 2-5                            [-1, 128, 4, 4]           24,576\n",
      "|    UpSample_layer: 2-6                    [-1, 64, 8, 8]            --\n",
      "|    |    ConvTranspose2d: 3-20             [-1, 64, 8, 8]            73,728\n",
      "|    |    Sequential: 3-21                  [-1, 64, 8, 8]            114,688\n",
      "|    UpSample_layer: 2-7                    [-1, 32, 16, 16]          --\n",
      "|    |    ConvTranspose2d: 3-22             [-1, 32, 16, 16]          18,432\n",
      "|    |    Sequential: 3-23                  [-1, 32, 16, 16]          28,672\n",
      "|    UpSample_layer: 2-8                    [-1, 64, 32, 32]          --\n",
      "|    |    ConvTranspose2d: 3-24             [-1, 64, 32, 32]          18,432\n",
      "|    |    Sequential: 3-25                  [-1, 64, 32, 32]          77,824\n",
      "|    UpSample_layer: 2-9                    [-1, 64, 64, 64]          --\n",
      "|    |    ConvTranspose2d: 3-26             [-1, 64, 64, 64]          36,864\n",
      "|    |    Sequential: 3-27                  [-1, 64, 64, 64]          77,824\n",
      "|    Conv2d: 2-10                           [-1, 1, 64, 64]           576\n",
      "===============================================================================================\n",
      "Total params: 2,627,668\n",
      "Trainable params: 2,627,668\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 6.39\n",
      "Params size (MB): 10.02\n",
      "Estimated Total Size (MB): 17.16\n",
      "===============================================================================================\n",
      "The build_model model has: 2627668 trainable parameters\n",
      "Start training: build_model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 - Training: 100%|| 792/792 [03:46<00:00,  3.50step/s, Loss=260, Acc=9.48, Lr=0.001, L_mae=109, L_norm=66.8, L_grad=60.4, L_ssim=23.9]\n",
      "Epoch 1/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.30step/s, Loss=201, Acc=11.6, RMSE=90.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 90.579 at epoch 1\n",
      "New best ACCURACY: 11.623 at epoch 1\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 - Training: 100%|| 792/792 [03:46<00:00,  3.49step/s, Loss=222, Acc=11.7, Lr=0.001, L_mae=90, L_norm=60.2, L_grad=51.6, L_ssim=20]\n",
      "Epoch 2/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.53step/s, Loss=189, Acc=13.6, RMSE=83.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 83.814 at epoch 2\n",
      "New best ACCURACY: 13.606 at epoch 2\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=208, Acc=13, Lr=0.001, L_mae=82.5, L_norm=57, L_grad=50, L_ssim=18.8]\n",
      "Epoch 3/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.01step/s, Loss=184, Acc=14.8, RMSE=81.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 81.669 at epoch 3\n",
      "New best ACCURACY: 14.772 at epoch 3\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=198, Acc=14.4, Lr=0.001, L_mae=76.9, L_norm=54.5, L_grad=48.8, L_ssim=17.9]\n",
      "Epoch 4/120 - Validation: 100%|| 654/654 [00:07<00:00, 83.57step/s, Loss=180, Acc=13.9, RMSE=81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 80.978 at epoch 4\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=190, Acc=15.4, Lr=0.001, L_mae=72.6, L_norm=52.7, L_grad=47.7, L_ssim=17.2]\n",
      "Epoch 5/120 - Validation: 100%|| 654/654 [00:07<00:00, 83.77step/s, Loss=176, Acc=14.8, RMSE=78.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 78.707 at epoch 5\n",
      "New best ACCURACY: 14.844 at epoch 5\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=184, Acc=16.5, Lr=0.001, L_mae=68.9, L_norm=51.2, L_grad=46.9, L_ssim=16.5]\n",
      "Epoch 6/120 - Validation: 100%|| 654/654 [00:07<00:00, 83.77step/s, Loss=177, Acc=15, RMSE=78.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 78.532 at epoch 6\n",
      "New best ACCURACY: 15.025 at epoch 6\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=178, Acc=17.2, Lr=0.001, L_mae=66.3, L_norm=50, L_grad=46, L_ssim=16]\n",
      "Epoch 7/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.67step/s, Loss=171, Acc=16.1, RMSE=75.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 75.075 at epoch 7\n",
      "New best ACCURACY: 16.078 at epoch 7\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=174, Acc=18, Lr=0.001, L_mae=63.9, L_norm=49, L_grad=45.5, L_ssim=15.6]\n",
      "Epoch 8/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.20step/s, Loss=166, Acc=16.8, RMSE=73.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 73.184 at epoch 8\n",
      "New best ACCURACY: 16.769 at epoch 8\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 - Training: 100%|| 792/792 [03:44<00:00,  3.52step/s, Loss=170, Acc=18.9, Lr=0.001, L_mae=61.7, L_norm=48.1, L_grad=44.8, L_ssim=15.1]\n",
      "Epoch 9/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.98step/s, Loss=168, Acc=16.7, RMSE=73.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 - Training: 100%|| 792/792 [03:45<00:00,  3.52step/s, Loss=166, Acc=19.5, Lr=0.001, L_mae=59.7, L_norm=47.2, L_grad=44.2, L_ssim=14.7]\n",
      "Epoch 10/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.72step/s, Loss=165, Acc=16.6, RMSE=73.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 73.141 at epoch 10\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 - Training: 100%|| 792/792 [03:46<00:00,  3.50step/s, Loss=163, Acc=20.1, Lr=0.001, L_mae=58.2, L_norm=46.5, L_grad=43.7, L_ssim=14.4]\n",
      "Epoch 11/120 - Validation: 100%|| 654/654 [00:07<00:00, 83.01step/s, Loss=165, Acc=16.2, RMSE=73.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 - Training: 100%|| 792/792 [03:43<00:00,  3.55step/s, Loss=159, Acc=21, Lr=0.001, L_mae=55.8, L_norm=45.7, L_grad=43.1, L_ssim=13.9]\n",
      "Epoch 12/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.42step/s, Loss=164, Acc=16.7, RMSE=71.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 71.833 at epoch 12\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 - Training: 100%|| 792/792 [03:43<00:00,  3.54step/s, Loss=156, Acc=21.8, Lr=0.001, L_mae=54.5, L_norm=45.1, L_grad=42.6, L_ssim=13.7]\n",
      "Epoch 13/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.64step/s, Loss=162, Acc=16.1, RMSE=72.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=153, Acc=22.4, Lr=0.001, L_mae=52.8, L_norm=44.4, L_grad=42.1, L_ssim=13.3]\n",
      "Epoch 14/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.87step/s, Loss=160, Acc=17.5, RMSE=70.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.542 at epoch 14\n",
      "New best ACCURACY: 17.515 at epoch 14\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 - Training: 100%|| 792/792 [03:44<00:00,  3.53step/s, Loss=151, Acc=22.9, Lr=0.001, L_mae=51.7, L_norm=44, L_grad=41.8, L_ssim=13.1]\n",
      "Epoch 15/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.50step/s, Loss=159, Acc=17.4, RMSE=70.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.521 at epoch 15\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=147, Acc=23.8, Lr=0.001, L_mae=50, L_norm=43.2, L_grad=41.3, L_ssim=12.8]\n",
      "Epoch 16/120 - Validation: 100%|| 654/654 [00:07<00:00, 83.37step/s, Loss=159, Acc=16.4, RMSE=71.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 - Training: 100%|| 792/792 [03:45<00:00,  3.51step/s, Loss=146, Acc=23.9, Lr=0.001, L_mae=49.6, L_norm=43, L_grad=41, L_ssim=12.6]\n",
      "Epoch 17/120 - Validation: 100%|| 654/654 [00:11<00:00, 56.47step/s, Loss=160, Acc=16.9, RMSE=70.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 - Training: 100%|| 792/792 [04:13<00:00,  3.13step/s, Loss=143, Acc=24.7, Lr=0.001, L_mae=48.1, L_norm=42.4, L_grad=40.6, L_ssim=12.3]\n",
      "Epoch 18/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.02step/s, Loss=156, Acc=18.7, RMSE=68.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 68.445 at epoch 18\n",
      "New best ACCURACY: 18.653 at epoch 18\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 - Training: 100%|| 792/792 [04:10<00:00,  3.16step/s, Loss=141, Acc=25.4, Lr=0.001, L_mae=47, L_norm=42.1, L_grad=40.1, L_ssim=12]\n",
      "Epoch 19/120 - Validation: 100%|| 654/654 [00:08<00:00, 78.70step/s, Loss=157, Acc=17.7, RMSE=70.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 - Training: 100%|| 792/792 [04:11<00:00,  3.15step/s, Loss=139, Acc=26, Lr=0.001, L_mae=45.7, L_norm=41.5, L_grad=39.8, L_ssim=11.9]\n",
      "Epoch 20/120 - Validation: 100%|| 654/654 [00:08<00:00, 78.89step/s, Loss=155, Acc=18.1, RMSE=69.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 - Training: 100%|| 792/792 [04:06<00:00,  3.22step/s, Loss=137, Acc=26.4, Lr=0.001, L_mae=45, L_norm=41.2, L_grad=39.5, L_ssim=11.6]\n",
      "Epoch 21/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.14step/s, Loss=157, Acc=18.1, RMSE=69.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 - Training: 100%|| 792/792 [04:08<00:00,  3.19step/s, Loss=134, Acc=27.5, Lr=0.001, L_mae=43.4, L_norm=40.6, L_grad=39, L_ssim=11.4]\n",
      "Epoch 22/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.53step/s, Loss=154, Acc=18.7, RMSE=67.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.705 at epoch 22\n",
      "New best ACCURACY: 18.703 at epoch 22\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=134, Acc=27.5, Lr=0.001, L_mae=43.1, L_norm=40.4, L_grad=38.9, L_ssim=11.3]\n",
      "Epoch 23/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.95step/s, Loss=154, Acc=18, RMSE=68.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 - Training: 100%|| 792/792 [04:08<00:00,  3.19step/s, Loss=132, Acc=28.4, Lr=0.001, L_mae=42.1, L_norm=39.8, L_grad=38.6, L_ssim=11.1]\n",
      "Epoch 24/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.37step/s, Loss=152, Acc=19.3, RMSE=67.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.462 at epoch 24\n",
      "New best ACCURACY: 19.279 at epoch 24\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 - Training: 100%|| 792/792 [04:08<00:00,  3.19step/s, Loss=130, Acc=28.7, Lr=0.001, L_mae=41.5, L_norm=39.5, L_grad=38.3, L_ssim=10.9]\n",
      "Epoch 25/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.03step/s, Loss=153, Acc=19, RMSE=67.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.256 at epoch 25\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=129, Acc=29.5, Lr=0.001, L_mae=40.5, L_norm=39.3, L_grad=38, L_ssim=10.7]\n",
      "Epoch 26/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.37step/s, Loss=151, Acc=19.3, RMSE=66.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 66.669 at epoch 26\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=127, Acc=30.2, Lr=0.001, L_mae=39.8, L_norm=39, L_grad=37.7, L_ssim=10.6]\n",
      "Epoch 27/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.06step/s, Loss=150, Acc=20.1, RMSE=65.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 65.845 at epoch 27\n",
      "New best ACCURACY: 20.054 at epoch 27\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 - Training: 100%|| 792/792 [04:08<00:00,  3.19step/s, Loss=126, Acc=30.1, Lr=0.001, L_mae=39.5, L_norm=38.8, L_grad=37.6, L_ssim=10.5]\n",
      "Epoch 28/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.19step/s, Loss=155, Acc=16.9, RMSE=70.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 - Training: 100%|| 792/792 [04:09<00:00,  3.18step/s, Loss=125, Acc=30.7, Lr=0.001, L_mae=38.9, L_norm=38.5, L_grad=37.3, L_ssim=10.3]\n",
      "Epoch 29/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.23step/s, Loss=152, Acc=18.4, RMSE=66.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 - Training: 100%|| 792/792 [04:07<00:00,  3.21step/s, Loss=123, Acc=31.5, Lr=0.001, L_mae=38, L_norm=38.1, L_grad=37, L_ssim=10.2]\n",
      "Epoch 30/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.42step/s, Loss=148, Acc=19.7, RMSE=65.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 65.200 at epoch 30\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=122, Acc=31.9, Lr=0.001, L_mae=37.3, L_norm=37.8, L_grad=36.8, L_ssim=10]\n",
      "Epoch 31/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.43step/s, Loss=149, Acc=19.8, RMSE=66.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=121, Acc=32, Lr=0.001, L_mae=37.2, L_norm=37.7, L_grad=36.6, L_ssim=9.93]\n",
      "Epoch 32/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.88step/s, Loss=148, Acc=19.1, RMSE=65.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 - Training: 100%|| 792/792 [04:12<00:00,  3.13step/s, Loss=121, Acc=32.5, Lr=0.001, L_mae=36.8, L_norm=37.4, L_grad=36.5, L_ssim=9.85]\n",
      "Epoch 33/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.98step/s, Loss=149, Acc=19, RMSE=65.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=119, Acc=33.1, Lr=0.001, L_mae=36.1, L_norm=37.1, L_grad=36.3, L_ssim=9.72]\n",
      "Epoch 34/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.48step/s, Loss=148, Acc=19.9, RMSE=65.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=118, Acc=33.4, Lr=0.001, L_mae=35.5, L_norm=36.9, L_grad=36.1, L_ssim=9.57]\n",
      "Epoch 35/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.81step/s, Loss=148, Acc=19.9, RMSE=65.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 - Training: 100%|| 792/792 [04:06<00:00,  3.22step/s, Loss=117, Acc=34.1, Lr=0.001, L_mae=34.8, L_norm=36.6, L_grad=35.8, L_ssim=9.43]\n",
      "Epoch 36/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.52step/s, Loss=146, Acc=19.8, RMSE=64.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.689 at epoch 36\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 - Training: 100%|| 792/792 [04:05<00:00,  3.22step/s, Loss=117, Acc=34.1, Lr=0.001, L_mae=34.8, L_norm=36.5, L_grad=35.8, L_ssim=9.41]\n",
      "Epoch 37/120 - Validation: 100%|| 654/654 [00:08<00:00, 78.74step/s, Loss=147, Acc=19.5, RMSE=65.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 - Training: 100%|| 792/792 [04:06<00:00,  3.21step/s, Loss=115, Acc=35.2, Lr=0.001, L_mae=33.9, L_norm=36.3, L_grad=35.4, L_ssim=9.23]\n",
      "Epoch 38/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.94step/s, Loss=147, Acc=20.4, RMSE=65.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.414 at epoch 38\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=115, Acc=35, Lr=0.001, L_mae=34.1, L_norm=36.2, L_grad=35.4, L_ssim=9.22]\n",
      "Epoch 39/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.30step/s, Loss=146, Acc=20.6, RMSE=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.266 at epoch 39\n",
      "New best ACCURACY: 20.553 at epoch 39\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 - Training: 100%|| 792/792 [04:06<00:00,  3.21step/s, Loss=114, Acc=35.4, Lr=0.001, L_mae=33.6, L_norm=35.9, L_grad=35.3, L_ssim=9.14]\n",
      "Epoch 40/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.35step/s, Loss=147, Acc=19.7, RMSE=65.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 - Training: 100%|| 792/792 [04:07<00:00,  3.21step/s, Loss=113, Acc=35.3, Lr=0.001, L_mae=33.4, L_norm=35.9, L_grad=35.1, L_ssim=9.04]\n",
      "Epoch 41/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.90step/s, Loss=145, Acc=20.4, RMSE=63.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 63.608 at epoch 41\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 - Training: 100%|| 792/792 [04:06<00:00,  3.21step/s, Loss=113, Acc=35.8, Lr=0.001, L_mae=33.1, L_norm=35.6, L_grad=35.1, L_ssim=8.99]\n",
      "Epoch 42/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.34step/s, Loss=146, Acc=20.6, RMSE=64.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 20.645 at epoch 42\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=111, Acc=36.8, Lr=0.001, L_mae=32.3, L_norm=35.3, L_grad=34.7, L_ssim=8.81]\n",
      "Epoch 43/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.03step/s, Loss=147, Acc=20.3, RMSE=65.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=111, Acc=36.6, Lr=0.001, L_mae=32.4, L_norm=35.3, L_grad=34.7, L_ssim=8.83]\n",
      "Epoch 44/120 - Validation: 100%|| 654/654 [00:08<00:00, 79.45step/s, Loss=146, Acc=20, RMSE=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=110, Acc=37.1, Lr=0.001, L_mae=31.8, L_norm=35.2, L_grad=34.5, L_ssim=8.7]\n",
      "Epoch 45/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.07step/s, Loss=144, Acc=20.6, RMSE=62.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.947 at epoch 45\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 - Training: 100%|| 792/792 [04:07<00:00,  3.20step/s, Loss=110, Acc=37.6, Lr=0.001, L_mae=31.5, L_norm=35, L_grad=34.4, L_ssim=8.66]\n",
      "Epoch 46/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.58step/s, Loss=146, Acc=19.7, RMSE=64.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 - Training: 100%|| 792/792 [04:04<00:00,  3.23step/s, Loss=109, Acc=37.8, Lr=0.001, L_mae=31.2, L_norm=34.8, L_grad=34.2, L_ssim=8.54]\n",
      "Epoch 47/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.28step/s, Loss=146, Acc=20.7, RMSE=64.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.657 at epoch 47\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 - Training: 100%|| 792/792 [04:05<00:00,  3.23step/s, Loss=108, Acc=37.9, Lr=0.001, L_mae=31, L_norm=34.6, L_grad=34.1, L_ssim=8.51]\n",
      "Epoch 48/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.07step/s, Loss=145, Acc=20.7, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "New best ACCURACY: 20.703 at epoch 48\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 - Training: 100%|| 792/792 [04:04<00:00,  3.24step/s, Loss=108, Acc=37.9, Lr=0.001, L_mae=30.9, L_norm=34.6, L_grad=34, L_ssim=8.43]\n",
      "Epoch 49/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.51step/s, Loss=149, Acc=19.9, RMSE=65.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 - Training: 100%|| 792/792 [04:05<00:00,  3.23step/s, Loss=107, Acc=38.9, Lr=0.001, L_mae=30.3, L_norm=34.4, L_grad=33.9, L_ssim=8.4]\n",
      "Epoch 50/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.39step/s, Loss=144, Acc=20.8, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 20.804 at epoch 50\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 - Training: 100%|| 792/792 [03:58<00:00,  3.31step/s, Loss=107, Acc=38.7, Lr=0.001, L_mae=30.3, L_norm=34.4, L_grad=33.8, L_ssim=8.31]\n",
      "Epoch 51/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.89step/s, Loss=144, Acc=20.6, RMSE=62.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.743 at epoch 51\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=106, Acc=39.2, Lr=0.001, L_mae=30.1, L_norm=34.2, L_grad=33.7, L_ssim=8.29]\n",
      "Epoch 52/120 - Validation: 100%|| 654/654 [00:08<00:00, 80.77step/s, Loss=145, Acc=20.4, RMSE=63.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=106, Acc=39.5, Lr=0.001, L_mae=29.8, L_norm=34.1, L_grad=33.6, L_ssim=8.23]\n",
      "Epoch 53/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.16step/s, Loss=145, Acc=20.8, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.822 at epoch 53\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=105, Acc=39.5, Lr=0.001, L_mae=29.6, L_norm=33.9, L_grad=33.5, L_ssim=8.15]\n",
      "Epoch 54/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.32step/s, Loss=145, Acc=19.9, RMSE=63.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 - Training: 100%|| 792/792 [04:04<00:00,  3.25step/s, Loss=104, Acc=39.9, Lr=0.001, L_mae=29.3, L_norm=33.8, L_grad=33.4, L_ssim=8.07]\n",
      "Epoch 55/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.16step/s, Loss=145, Acc=21.1, RMSE=63.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "New best ACCURACY: 21.073 at epoch 55\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 - Training: 100%|| 792/792 [04:02<00:00,  3.26step/s, Loss=104, Acc=40.2, Lr=0.001, L_mae=29.1, L_norm=33.6, L_grad=33.3, L_ssim=8.04]\n",
      "Epoch 56/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.93step/s, Loss=144, Acc=21.2, RMSE=64.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 21.188 at epoch 56\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 - Training: 100%|| 792/792 [04:02<00:00,  3.27step/s, Loss=104, Acc=40.5, Lr=0.001, L_mae=28.9, L_norm=33.6, L_grad=33.2, L_ssim=7.99]\n",
      "Epoch 57/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.33step/s, Loss=144, Acc=21.1, RMSE=64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=103, Acc=40.6, Lr=0.001, L_mae=28.6, L_norm=33.5, L_grad=33, L_ssim=7.91]\n",
      "Epoch 58/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.90step/s, Loss=146, Acc=21.3, RMSE=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 21.260 at epoch 58\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 - Training: 100%|| 792/792 [04:02<00:00,  3.27step/s, Loss=103, Acc=40.8, Lr=0.001, L_mae=28.7, L_norm=33.4, L_grad=33, L_ssim=7.9]\n",
      "Epoch 59/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.20step/s, Loss=143, Acc=20.5, RMSE=63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 - Training: 100%|| 792/792 [04:02<00:00,  3.27step/s, Loss=102, Acc=41.3, Lr=0.001, L_mae=28.3, L_norm=33.2, L_grad=32.9, L_ssim=7.85]\n",
      "Epoch 60/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.90step/s, Loss=144, Acc=20.7, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 - Training: 100%|| 792/792 [04:05<00:00,  3.23step/s, Loss=102, Acc=41.7, Lr=0.001, L_mae=27.9, L_norm=33.1, L_grad=32.9, L_ssim=7.8]\n",
      "Epoch 61/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.52step/s, Loss=144, Acc=21, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 - Training: 100%|| 792/792 [04:04<00:00,  3.23step/s, Loss=102, Acc=40.8, Lr=0.001, L_mae=28.3, L_norm=33.1, L_grad=32.8, L_ssim=7.77]\n",
      "Epoch 62/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.19step/s, Loss=142, Acc=20.8, RMSE=63.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=101, Acc=41.5, Lr=0.001, L_mae=28, L_norm=32.9, L_grad=32.7, L_ssim=7.73]\n",
      "Epoch 63/120 - Validation: 100%|| 654/654 [00:07<00:00, 81.76step/s, Loss=145, Acc=20.6, RMSE=64.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 - Training: 100%|| 792/792 [04:04<00:00,  3.24step/s, Loss=101, Acc=42.1, Lr=0.001, L_mae=27.6, L_norm=32.9, L_grad=32.6, L_ssim=7.66]\n",
      "Epoch 64/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.13step/s, Loss=143, Acc=21.3, RMSE=63.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 21 epochs\n",
      "New best ACCURACY: 21.334 at epoch 64\n",
      "EarlyStopping increased due to Accuracy, stop in 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 - Training: 100%|| 792/792 [04:05<00:00,  3.22step/s, Loss=101, Acc=42.2, Lr=0.001, L_mae=27.5, L_norm=32.9, L_grad=32.6, L_ssim=7.63]\n",
      "Epoch 65/120 - Validation: 100%|| 654/654 [00:07<00:00, 82.53step/s, Loss=144, Acc=20.9, RMSE=64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 - Training: 100%|| 792/792 [04:03<00:00,  3.25step/s, Loss=99.9, Acc=42.6, Lr=0.001, L_mae=27.2, L_norm=32.7, L_grad=32.4, L_ssim=7.6]\n",
      "Epoch 66/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.32step/s, Loss=142, Acc=20.7, RMSE=62.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.350 at epoch 66\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 - Training: 100%|| 792/792 [04:04<00:00,  3.24step/s, Loss=99.8, Acc=42.4, Lr=0.001, L_mae=27.3, L_norm=32.5, L_grad=32.4, L_ssim=7.56]\n",
      "Epoch 67/120 - Validation: 100%|| 654/654 [00:08<00:00, 81.03step/s, Loss=143, Acc=20.7, RMSE=63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 - Training:  88%|                           | 700/792 [03:34<00:28,  3.26step/s, Loss=100, Acc=42.4, Lr=0.001, L_mae=27.4, L_norm=32.7, L_grad=32.3, L_ssim=7.56]"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "def process(device):\n",
    "    # Set-seed\n",
    "    seed = param['seed']\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Datasets loading\n",
    "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "        dts_root_path=dataset_root,\n",
    "        rgb_h_res=param['img_res'][1],\n",
    "        d_h_res=param['depth_img_res'][1],\n",
    "        bs_train=param['batch_size'],\n",
    "        bs_eval=param['batch_size_eval'],\n",
    "        num_workers=param['n_workers'],\n",
    "    )\n",
    "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
    "    # Prints samples\n",
    "    print(' --- Test samples --- ')\n",
    "    print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
    "              save_model_root=save_model_root)\n",
    "    print(' --- Training augmented samples --- ')\n",
    "    print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
    "                  save_model_root=save_model_root)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # Globals\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
    "               'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
    "    min_rmse = float('inf')\n",
    "    min_acc = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    # Loss\n",
    "    criterion = balanced_loss_function(device=device)\n",
    "    # Model\n",
    "    model = build_model(device=device).to(device=device)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    print_model(model=model, input_shape=param['img_res'])\n",
    "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=param['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
    "    )\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=param['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
    "        cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
    "    )\n",
    "    # Early stopping\n",
    "    trigger_times, early_stopping_epochs = 0, param['e_stop_epochs']\n",
    "    print(\"Start training: {}\\n\".format(model_name))\n",
    "    \n",
    "    epochs = param['epochs']\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        iter = 1\n",
    "        model.train()\n",
    "        running_loss, accuracy = 0, 0\n",
    "        running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Forward\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # Compute loss\n",
    "                loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Evaluation and Stats\n",
    "                running_loss += loss.item()\n",
    "                running_l_mae += loss_depth.item()\n",
    "                running_l_norm += loss_normal.item()\n",
    "                running_l_grad += loss_grad.item()\n",
    "                running_l_ssim += loss_ssim.item()\n",
    "\n",
    "                train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                train_loss_list.append(train_loss_support)\n",
    "\n",
    "                accuracy += compute_accuracy(outputs, depths)\n",
    "                tepoch.set_postfix({'Loss': running_loss / iter,\n",
    "                                    'Acc': accuracy.item() / iter,\n",
    "                                    'Lr': param['lr'] if not history['lrs'] else history['lrs'][-1],\n",
    "                                    'L_mae': running_l_mae / iter,\n",
    "                                    'L_norm': running_l_norm / iter,\n",
    "                                    'L_grad': running_l_grad / iter,\n",
    "                                    'L_ssim': running_l_ssim / iter\n",
    "                                    })\n",
    "                iter += 1\n",
    "\n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    # Evaluation metrics\n",
    "                    test_accuracy += compute_accuracy(outputs, depths)\n",
    "                    # Loss\n",
    "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                    test_loss_list.append(test_loss_support)\n",
    "\n",
    "                    # RMSE\n",
    "                    test_rmse += compute_rmse(outputs, depths)\n",
    "                    tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
    "                                        'RMSE': test_rmse.item() / iter})\n",
    "                    iter += 1\n",
    "\n",
    "        # Update history infos\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        history['train_loss'].append(running_loss / len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss / len(test_DataLoader))\n",
    "        history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
    "        history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
    "        # Update history losses infos\n",
    "        history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
    "        history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
    "        history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
    "        history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
    "        # Update scheduler LR\n",
    "        scheduler.step(history['test_rmse'][-1])\n",
    "        # Save model by best RMSE\n",
    "        if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
    "            trigger_times = 0\n",
    "            min_rmse = test_rmse / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best', save_model_root)\n",
    "            print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
    "        # Save model by best ACCURACY\n",
    "        if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
    "            min_acc = test_accuracy / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
    "            print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
    "            if trigger_times > 4:\n",
    "                trigger_times = trigger_times - 2\n",
    "                print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
    "\n",
    "        save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
    "                                 save_path=save_model_root + 'evolution_img/')\n",
    "        save_history(history, save_model_root + model_name + '_history')\n",
    "        # Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if trigger_times == early_stopping_epochs:\n",
    "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
    "            break\n",
    "\n",
    "        # Save loss for graphs\n",
    "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
    "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
    "\n",
    "        print('Finished Training')\n",
    "        save_csv_history(model_name=model_name, path=save_model_root)\n",
    "        plot_history(history, path=save_model_root)\n",
    "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
    "\n",
    "        if os.path.exists(save_model_root + 'example&augment_img/'):\n",
    "            shutil.rmtree(save_model_root + 'example&augment_img/')\n",
    "\n",
    "\n",
    "    # model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
    "    # model, model_name = load_pretrained_model(model=model,\n",
    "    #                                           path_weigths=save_model_root + 'build_model_best',\n",
    "    #                                           device=device,\n",
    "    #                                           do_pretrained=global_var['do_pretrained'],\n",
    "    #                                           imagenet_w_init=global_var['imagenet_w_init'])\n",
    "    # if global_var['do_print_model']:\n",
    "    print_model(model=model, input_shape=param['img_res'])\n",
    "    # print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "\n",
    "    # Evaluate\n",
    "    print(' --- Begin evaluation --- ')\n",
    "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "    print(' --- End evaluation --- ')\n",
    "\n",
    "    sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "    save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "    save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware\n",
    "    device = hardware_check()\n",
    "\n",
    "    # -- TRAIN 1\n",
    "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
    "    # Directory test\n",
    "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
    "    #print(save_model_root)\n",
    "    # Create folders\n",
    "    if not os.path.exists(save_model_root):\n",
    "        os.makedirs(save_model_root)\n",
    "    # if not os.path.exists(save_model_root + 'info_code/'):\n",
    "    #     os.makedirs(save_model_root + 'info_code/')\n",
    "    # files_directory = '/work/project/'\n",
    "    # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
    "    # for f in files:\n",
    "    #     shutil.copy(f, save_model_root + 'info_code/')\n",
    "    # Run process\n",
    "    start_time = perf_counter()\n",
    "    process(device=device)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = perf_counter()\n",
    "    print(\"Total time elapsed: \",end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dca0a-3084-46ff-8ed8-a81bb630e425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
