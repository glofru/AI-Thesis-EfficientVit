{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3fcf5a-8f76-40d2-b1b7-13086f3db534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Using cached torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.126254637Z",
     "start_time": "2023-10-15T00:35:21.094038559Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from timm.models.vision_transformer import trunc_normal_\n",
    "from timm.models.layers import SqueezeExcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8c9bd930f7eb74d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.152808114Z",
     "start_time": "2023-10-15T00:35:21.097903351Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"seed\": 4242,\n",
    "    \"img_res\": (3, 256, 256),\n",
    "    \"depth_img_res\": (1, 64, 64),\n",
    "    \"n_workers\": 2,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"batch_size_eval\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_patience\": 15,\n",
    "    \"e_stop_epochs\": 30,\n",
    "    \"epochs\": 120,\n",
    "}\n",
    "\n",
    "augmentation_parameters = {\n",
    "    'flip': 0.5,\n",
    "    'mirror': 0.5,\n",
    "    'color&bright': 0.5,\n",
    "    'c_swap': 0.5,\n",
    "    'random_crop': 0.5,\n",
    "    'random_d_shift': 0.5  # range(+-10)cm\n",
    "}\n",
    "\n",
    "dataset_root = './data/NYUv2/'\n",
    "save_model_root = './results/v3_2/pyramid6_rmse_v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31c0b70b2a31fe",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b96bc0c825097de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.194945871Z",
     "start_time": "2023-10-15T00:35:21.146859600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def hardware_check():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Actual device: \", device)\n",
    "    if 'cuda' in device:\n",
    "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def plot_depth_map(dm):\n",
    "\n",
    "    MIN_DEPTH = 0.0\n",
    "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
    "\n",
    "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
    "    cmap = plt.cm.plasma_r\n",
    "\n",
    "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
    "\n",
    "\n",
    "def resize_keeping_aspect_ratio(img, base):\n",
    "    \"\"\"\n",
    "    Resize the image to a defined length manteining its proportions\n",
    "    Scaling the shortest side of the image to a fixed 'base' length'\n",
    "    \"\"\"\n",
    "\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        basewidth = int(base)\n",
    "        wpercent = (basewidth / float(img.shape[0]))\n",
    "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
    "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
    "    else:\n",
    "        baseheight = int(base)\n",
    "        wpercent = (baseheight / float(img.shape[1]))\n",
    "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
    "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, depths):\n",
    "    valid_mask = depths > 0.0\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
    "    valid_mask = y_true > 0.0\n",
    "    valid_pred = y_pred[valid_mask]\n",
    "    valid_true = y_true[valid_mask]\n",
    "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
    "    return 100 * torch.mean(correct.float())\n",
    "\n",
    "\n",
    "def print_model(model, input_shape):\n",
    "    info = summary(model, input_shape)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name, path_save_model):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    if not os.path.isdir(path_save_model):\n",
    "        os.makedirs(path_save_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path_save_model + name)\n",
    "\n",
    "\n",
    "def save_history(history, filepath):\n",
    "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "\n",
    "def save_csv_history(model_name, path):\n",
    "    objects = []\n",
    "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    df = pd.DataFrame(objects)\n",
    "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if do_pretrained:\n",
    "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
    "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"checkpoint loaded\\n\")\n",
    "\n",
    "    if imagenet_w_init:\n",
    "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
    "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model_dict = model.state_dict()\n",
    "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
    "\n",
    "        # pretrained_param = len(pretrained_dict.items())\n",
    "        counter_param = 0\n",
    "        for i, j in pretrained_dict.items():\n",
    "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
    "                counter_param += 1\n",
    "\n",
    "        print(f'Pertained parameters: {counter_param}\\n')\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        # alternativa to 2 e 3\n",
    "        # model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Partial initialization computed\\n\")\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title, path):\n",
    "    epochs = range(0, len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_history(history, path):\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
    "\n",
    "\n",
    "def plot_loss_parts(history, path, title):\n",
    "    l_mae_list = history['l_mae']\n",
    "    l_norm_list = history['l_norm']\n",
    "    l_grad_list = history['l_grad']\n",
    "    l_ssim_list = history['l_ssim']\n",
    "    epochs = range(0, len(l_mae_list))\n",
    "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
    "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
    "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
    "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
    "    for i in range(quantity):\n",
    "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
    "\n",
    "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
    "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
    "              f' variance =  {torch.var(img)}\\n')\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input image')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Grayscale DepthMap')\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Colored DepthMap')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        print(\"************************** \",save_model_root)\n",
    "        save_path = save_model_root + 'example&augment_img/'\n",
    "        print(\"************************** \",save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
    "    for i, index in zip(range(len(indices)), indices):\n",
    "        img, depth = dataset.__getitem__(index)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.from_numpy(img).to(device))\n",
    "            # Build plot\n",
    "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "            plt.subplot(1, len(indices), i+1)\n",
    "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
    "            if False:\n",
    "                plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
    "    save_path = save_model_root + type + '_predictions/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(list_type)):\n",
    "        index_image = list_type[i][0]\n",
    "        rmse_value = list_type[i][1]\n",
    "\n",
    "        img, depth = dataset.__getitem__(index=index_image)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f'Original image {index_image}')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title('Predicted DepthMap')\n",
    "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
    "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
    "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
    "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def compute_MeanVar(dataset):\n",
    "    r_mean, g_mean, b_mean = [], [], []\n",
    "    r_var, g_var, b_var = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r = np.array(img[0, :, :])\n",
    "        g = np.array(img[1, :, :])\n",
    "        b = np.array(img[2, :, :])\n",
    "\n",
    "        r_mean.append(np.mean(r))\n",
    "        g_mean.append(np.mean(g))\n",
    "        b_mean.append(np.mean(b))\n",
    "\n",
    "        r_var.append(np.var(r))\n",
    "        g_var.append(np.var(g))\n",
    "        b_var.append(np.var(b))\n",
    "\n",
    "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
    "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
    "\n",
    "\n",
    "def compute_MeanImg(dataset, save_model_root):\n",
    "    r, g, b = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r.append(np.array(img[0, :, :]))\n",
    "        g.append(np.array(img[1, :, :]))\n",
    "        b.append(np.array(img[2, :, :]))\n",
    "\n",
    "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
    "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
    "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
    "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
    "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
    "\n",
    "    print(\"Process Completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ae2358f0c16d2",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e14242f44dcf3ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.195170649Z",
     "start_time": "2023-10-15T00:35:21.147039384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def pixel_shift(depth_img, shift):\n",
    "    depth_img = depth_img + shift\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "def random_crop(x, y, crop_size=(192, 256)):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    h, w, _ = x.shape\n",
    "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
    "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
    "    if cropped_y.shape[-1] == 0:\n",
    "        return x, y\n",
    "    else:\n",
    "        return cropped_x, cropped_y\n",
    "\n",
    "\n",
    "def augmentation2D(img, depth, print_info_aug):\n",
    "    # Random flipping\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
    "        img = (img[..., ::1, :, :]).copy()\n",
    "        depth = (depth[..., ::1, :, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random flipped')\n",
    "    # Random mirroring\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
    "        img = (img[..., ::-1, :]).copy()\n",
    "        depth = (depth[..., ::-1, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random mirrored')\n",
    "    # Augment image\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
    "        # gamma augmentation\n",
    "        gamma = random.uniform(0.9, 1.1)\n",
    "        img = img ** gamma\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        img = img * brightness\n",
    "        # color augmentation\n",
    "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
    "        white = np.ones((img.shape[0], img.shape[1]))\n",
    "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        img *= color_image\n",
    "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
    "        if print_info_aug:\n",
    "            print('--> Image randomly augmented')\n",
    "    # Channel swap\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
    "        indices = list(product([0, 1, 2], repeat=3))\n",
    "        policy_idx = random.randint(0, len(indices) - 1)\n",
    "        img = img[..., list(indices[policy_idx])]\n",
    "        if print_info_aug:\n",
    "            print('--> Channel swapped')\n",
    "    # Random crop\n",
    "    if random.random() <= augmentation_parameters['random_crop']:\n",
    "        img, depth = random_crop(img, depth)\n",
    "        if print_info_aug:\n",
    "            print('--> Random cropped')\n",
    "    # Depth Shift\n",
    "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
    "        random_shift = random.randint(-10, 10)\n",
    "        depth = pixel_shift(depth, shift=random_shift)\n",
    "        if print_info_aug:\n",
    "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
    "\n",
    "    return img, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993932aac8813d7",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e40cba8dfbdb6020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.196198980Z",
     "start_time": "2023-10-15T00:35:21.147176850Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class NYU2_Dataset:\n",
    "    \"\"\"\n",
    "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
    "      * 654 Test and 50688 Train images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
    "        self.dataset = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.info = 0\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.rgb_h_res = rgb_h_res\n",
    "        self.d_h_res = d_h_res\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "        # Handle dataset\n",
    "        if self.dts_type == 'test':\n",
    "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
    "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
    "\n",
    "            rgb = np.load(img_path)\n",
    "            depth = np.load(depth_path)\n",
    "\n",
    "            self.x = rgb\n",
    "            self.y = depth\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = rgb[:dts_size]\n",
    "                self.y = depth[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        elif self.dts_type == 'train':\n",
    "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
    "            for scene in scenarios:\n",
    "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
    "                for el in elem:\n",
    "                    if 'jpg' in el:\n",
    "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    elif 'png' in el:\n",
    "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    else:\n",
    "                        raise SystemError('Type image error (train)')\n",
    "\n",
    "            if len(self.x) != len(self.y):\n",
    "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
    "\n",
    "            self.x.sort()\n",
    "            self.y.sort()\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = self.x[:dts_size]\n",
    "                self.y = self.y[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        else:\n",
    "            raise SystemError('Problem in the path')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def __getitem__(self, index=None, print_info_aug=False):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "\n",
    "        # Load Image\n",
    "        if self.dts_type == 'test':\n",
    "            img = self.x[index]\n",
    "        else:\n",
    "            img_name = self.dataset + self.x[index]\n",
    "            try:\n",
    "                raw_img = Image.open(img_name)\n",
    "                img = np.array(raw_img.convert('RGB'))\n",
    "                raw_img.close()\n",
    "            except:\n",
    "                exit(f\"Failed opening {img_name}\")\n",
    "\n",
    "        # Load Depth Image\n",
    "        if self.dts_type == 'test':\n",
    "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
    "        else:\n",
    "            depth = Image.open(self.dataset + self.y[index])\n",
    "            depth = np.array(depth) / 255\n",
    "            depth = np.clip(depth * 1000, 50, 1000)\n",
    "            depth = np.expand_dims(depth, axis=-1)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
    "\n",
    "        img_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['img_res'][1], param['img_res'][2]), antialias=True),\n",
    "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
    "        ])\n",
    "        depth_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['depth_img_res'][1], param['depth_img_res'][2]), antialias=True),\n",
    "        ])\n",
    "\n",
    "        img = img_post_processing(img/255)\n",
    "        depth = depth_post_processing(depth)\n",
    "\n",
    "        return img.float(), depth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "109124c84a5ace20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.213384206Z",
     "start_time": "2023-10-15T00:35:21.162874260Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def init_train_test_loader(dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
    "    # Load Datasets\n",
    "    test_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
    "    )\n",
    "    training_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
    "    )\n",
    "    # Create Dataloaders\n",
    "    training_DataLoader = DataLoader(\n",
    "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    test_DataLoader = DataLoader(\n",
    "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d87f8c10eb08df3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.213525128Z",
     "start_time": "2023-10-15T00:35:21.210956245Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class balanced_loss_function(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / 2\n",
    "\n",
    "        return loss_depth, loss_ssim, loss_normal, loss_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eef5ed64d70f1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6a8541506bb85f7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:21.255077358Z",
     "start_time": "2023-10-15T00:35:21.211155385Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def conv_1x1_bn(inp, oup):\n",
    "#     return nn.Sequential(\n",
    "#         nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "#         nn.BatchNorm2d(oup),\n",
    "#         nn.ReLU()  # nn.SiLU()\n",
    "#     )\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth, kernel_size=kernel_size, groups=depth, padding=1, stride=stride, bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "#     return nn.Sequential(\n",
    "#         # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "#         SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "#                         bias=False, device='cpu'),\n",
    "#         nn.BatchNorm2d(oup),\n",
    "#         nn.ReLU()  # nn.SiLU()\n",
    "#     )\n",
    "\n",
    "\n",
    "# class PreNorm(nn.Module):\n",
    "#     def __init__(self, dim, fn):\n",
    "#         super().__init__()\n",
    "#         self.norm = nn.LayerNorm(dim)\n",
    "#         self.fn = fn\n",
    "\n",
    "#     def forward(self, x, **kwargs):\n",
    "#         return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(dim, hidden_dim),\n",
    "#             nn.ReLU(),  # nn.SiLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, dim),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.dim = dim\n",
    "#         self.heads = heads\n",
    "#         self.dim_head = dim_head\n",
    "#         # head_dim = dim // heads\n",
    "#         self.scale = dim_head ** -0.5\n",
    "#         # print(\"------------------------------------DIM--------------------------\", dim)\n",
    "#         self.q = nn.Linear(dim, dim, bias=False)\n",
    "#         self.kv = nn.Linear(dim, dim * 2, bias=False)\n",
    "#         self.attn_drop = nn.Dropout(0)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(0)\n",
    "\n",
    "#         self.sr_ratio = 4\n",
    "#         if self.sr_ratio > 1:\n",
    "#             self.sr = nn.Conv2d(dim, dim, kernel_size=self.sr_ratio, stride=self.sr_ratio)\n",
    "#             self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C, W = x.shape # torch.Size([1, 4, 192, 144])\n",
    "\n",
    "#         q = self.q(x)\n",
    "\n",
    "#         if self.sr_ratio > 1:\n",
    "#             # x_ = x.permute(0, 2, 1).reshape(B, C, N, W)\n",
    "#             x_ = x.reshape(B, W, N, C)\n",
    "#             # print(\"-------------------------------------------------------------------\", x_.size)\n",
    "#             x_ = self.sr(x_).reshape(B, -1, self.dim).permute(0, 2, 1)\n",
    "#             # print(\"-------------------------------------------------------------------\", x_.size)\n",
    "#             x_ = self.norm(x_.permute(0, 2, 1))\n",
    "#             kv = self.kv(x_).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "#         else:\n",
    "#             kv = self.kv(x).reshape(B, -1, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
    "#         k, v = kv[0], kv[1]\n",
    "\n",
    "#         # q = q.reshape([q.shape[0], q.shape[1], q.shape[2]*(q.shape[3]//k.shape[3]), k.shape[3]])\n",
    "#         q = q.reshape([q.shape[0], q.shape[1], (q.shape[0]*q.shape[1]*q.shape[2]*q.shape[3])//(q.shape[0]*q.shape[1]*k.shape[3]), k.shape[3]])    # use this for xxs architecture\n",
    "#         attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "\n",
    "#         x = (attn @ v).transpose(1, 2).reshape(B, N, C, W)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([])\n",
    "#         for _ in range(depth):\n",
    "#             self.layers.append(nn.ModuleList([\n",
    "#                 PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "#                 PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "#             ]))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for attn, ff in self.layers:\n",
    "#             x = attn(x) + x\n",
    "#             x = ff(x) + x\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class MV2Block(nn.Module):\n",
    "#     def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "#         super().__init__()\n",
    "#         self.stride = stride\n",
    "#         assert stride in [1, 2]\n",
    "\n",
    "#         hidden_dim = int(inp * expansion)\n",
    "#         self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "#         if expansion == 1:\n",
    "#             self.conv = nn.Sequential(\n",
    "#                 # dw\n",
    "#                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # pw-linear\n",
    "#                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#             )\n",
    "#         else:\n",
    "#             self.conv = nn.Sequential(\n",
    "#                 # pw\n",
    "#                 nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # dw\n",
    "#                 nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "#                 nn.BatchNorm2d(hidden_dim),\n",
    "#                 nn.ReLU(),  # nn.SiLU(),\n",
    "#                 # pw-linear\n",
    "#                 nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "#                 nn.BatchNorm2d(oup),\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.use_res_connect:\n",
    "#             return x + self.conv(x)\n",
    "#         else:\n",
    "#             return self.conv(x)\n",
    "\n",
    "\n",
    "# class MobileViTBlock(nn.Module):\n",
    "#     def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "#         super().__init__()\n",
    "#         self.ph, self.pw = patch_size\n",
    "\n",
    "#         self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "#         self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "#         self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "#         self.conv3 = conv_1x1_bn(dim, channel)\n",
    "#         self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = x.clone()\n",
    "\n",
    "#         # Local representations\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "\n",
    "#         #print(\"*********************************** Start logging ***********************************\")\n",
    "#         #print(\"Transformer input shape: \",x.shape)\n",
    "\n",
    "#         # Global representations\n",
    "#         _, _, h, w = x.shape\n",
    "#         x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
    "#         #print(\"Rearranged input shape: \",x.shape)\n",
    "\n",
    "#         start_time = perf_counter() ############################## Time measurament\n",
    "#         x = self.transformer(x)\n",
    "#         end_time = perf_counter() ############################## Time measurament\n",
    "\n",
    "#         #print(\"Transformer output shape: \",x.shape)\n",
    "#         x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,\n",
    "#                       pw=self.pw)\n",
    "#         #print(\"Rearranged output shape: \",x.shape)\n",
    "#         #print(\"**************************************************************************************\")\n",
    "#         # Fusion\n",
    "#         x = self.conv3(x)\n",
    "#         x = torch.cat((x, y), 1)\n",
    "#         x = self.conv4(x)\n",
    "#         return x, end_time-start_time ############################## Time measurament\n",
    "\n",
    "\n",
    "# class MobileViT(nn.Module):\n",
    "#     def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "#         super().__init__()\n",
    "#         ih, iw = image_size\n",
    "#         ph, pw = patch_size\n",
    "#         assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "#         self.transformer_times = transformer_times ############################## Time measurament\n",
    "#         self.sample_cnt = sample_cnt ############################## Time measurament\n",
    "\n",
    "#         L = [1, 1, 1]  # L = [2, 4, 3] # --> +5 FPS\n",
    "\n",
    "#         self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "#         self.mv2 = nn.ModuleList([])\n",
    "#         self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
    "#         self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "#         self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "#         self.mvit = nn.ModuleList([])\n",
    "#         self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
    "#         self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
    "#         self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
    "\n",
    "#         self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "#         # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "#         # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y0 = self.conv1(x)\n",
    "#         x = self.mv2[0](y0)\n",
    "\n",
    "#         y1 = self.mv2[1](x)\n",
    "#         x = self.mv2[2](y1)\n",
    "#         x = self.mv2[3](x)  # Repeat\n",
    "\n",
    "#         y2 = self.mv2[4](x)\n",
    "#         x,mvit_time_1 = self.mvit[0](y2)\n",
    "#         self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
    "\n",
    "#         y3 = self.mv2[5](x)\n",
    "#         x,mvit_time_2 = self.mvit[1](y3)\n",
    "#         self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
    "\n",
    "#         x = self.mv2[6](x)\n",
    "#         x,mvit_time_3 = self.mvit[2](x)\n",
    "#         self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
    "#         x = self.conv2(x)\n",
    "\n",
    "#         self.sample_cnt += 1 ############################## Time measurament\n",
    "#         if(self.sample_cnt == 655):\n",
    "#             self.sample_cnt = 0\n",
    "\n",
    "#         return x, [y0, y1, y2, y3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8def71b7-d3dc-424b-83a3-65762110c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mobilevit_xxs(transformer_times, sample_cnt): ############################## Time measurament\n",
    "#     enc_type = 'xxs'\n",
    "#     dims = [64, 80, 96]\n",
    "#     channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]  # 320\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000, expansion=2,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "# def mobilevit_xs(transformer_times, sample_cnt):\n",
    "#     enc_type = 'xs'\n",
    "#     dims = [96, 120, 144]\n",
    "#     channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 192] # 384\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "# def mobilevit_s(transformer_times, sample_cnt):\n",
    "#     enc_type = 's'\n",
    "#     dims = [144, 192, 240]\n",
    "#     channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
    "#     return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "#                      transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e9ba174d-d2b7-41ca-b466-291211dc5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m\n",
    "\n",
    "\n",
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim, input_resolution):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0, resolution=input_resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim, resolution=input_resolution)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0, resolution=input_resolution // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "\n",
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h, resolution):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h, resolution=resolution)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0, resolution=resolution)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CascadedGroupAttention(torch.nn.Module):\n",
    "    r\"\"\" Cascaded Group Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution, correspond to the window size.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = key_dim ** -0.5\n",
    "        self.key_dim = key_dim\n",
    "        self.d = int(attn_ratio * key_dim)\n",
    "        self.attn_ratio = attn_ratio\n",
    "\n",
    "        qkvs = []\n",
    "        dws = []\n",
    "        for i in range(num_heads):\n",
    "            qkvs.append(Conv2d_BN(dim // (num_heads), self.key_dim * 2 + self.d, resolution=resolution))\n",
    "            dws.append(Conv2d_BN(self.key_dim, self.key_dim, kernels[i], 1, kernels[i]//2, groups=self.key_dim, resolution=resolution))\n",
    "        self.qkvs = torch.nn.ModuleList(qkvs)\n",
    "        self.dws = torch.nn.ModuleList(dws)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            self.d * num_heads, dim, bn_weight_init=0, resolution=resolution))\n",
    "\n",
    "        points = list(itertools.product(range(resolution), range(resolution)))\n",
    "        N = len(points)\n",
    "        attention_offsets = {}\n",
    "        idxs = []\n",
    "        for p1 in points:\n",
    "            for p2 in points:\n",
    "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
    "                if offset not in attention_offsets:\n",
    "                    attention_offsets[offset] = len(attention_offsets)\n",
    "                idxs.append(attention_offsets[offset])\n",
    "        self.attention_biases = torch.nn.Parameter(\n",
    "            torch.zeros(num_heads, len(attention_offsets)))\n",
    "        self.register_buffer('attention_bias_idxs',\n",
    "                             torch.LongTensor(idxs).view(N, N))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if mode and hasattr(self, 'ab'):\n",
    "            del self.ab\n",
    "        else:\n",
    "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "\n",
    "    def forward(self, x):  # x (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "        trainingab = self.attention_biases[:, self.attention_bias_idxs]\n",
    "        feats_in = x.chunk(len(self.qkvs), dim=1)\n",
    "        feats_out = []\n",
    "        feat = feats_in[0]\n",
    "        for i, qkv in enumerate(self.qkvs):\n",
    "            if i > 0: # add the previous output to the input\n",
    "                feat = feat + feats_in[i]\n",
    "            feat = qkv(feat)\n",
    "            q, k, v = feat.view(B, -1, H, W).split([self.key_dim, self.key_dim, self.d], dim=1) # B, C/h, H, W\n",
    "            q = self.dws[i](q)\n",
    "            q, k, v = q.flatten(2), k.flatten(2), v.flatten(2) # B, C/h, N\n",
    "            attn = (\n",
    "                (q.transpose(-2, -1) @ k) * self.scale\n",
    "                +\n",
    "                (trainingab[i] if self.training else self.ab[i])\n",
    "            )\n",
    "            attn = attn.softmax(dim=-1) # BNN\n",
    "            feat = (v @ attn.transpose(-2, -1)).view(B, self.d, H, W) # BCHW\n",
    "            feats_out.append(feat)\n",
    "        x = self.proj(torch.cat(feats_out, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocalWindowAttention(torch.nn.Module):\n",
    "    r\"\"\" Local Window Attention.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        key_dim (int): The dimension for query and key.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        attn_ratio (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, key_dim, num_heads=8,\n",
    "                 attn_ratio=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.resolution = resolution\n",
    "        assert window_resolution > 0, 'window_size must be greater than 0'\n",
    "        self.window_resolution = window_resolution\n",
    "        \n",
    "        window_resolution = min(window_resolution, resolution)\n",
    "        self.attn = CascadedGroupAttention(dim, key_dim, num_heads,\n",
    "                                attn_ratio=attn_ratio, \n",
    "                                resolution=window_resolution,\n",
    "                                kernels=kernels,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = W = self.resolution\n",
    "        B, C, H_, W_ = x.shape\n",
    "        # Only check this for classifcation models\n",
    "        assert H == H_ and W == W_, 'input feature has wrong size, expect {}, got {}'.format((H, W), (H_, W_))\n",
    "               \n",
    "        if H <= self.window_resolution and W <= self.window_resolution:\n",
    "            x = self.attn(x)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            pad_b = (self.window_resolution - H %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            pad_r = (self.window_resolution - W %\n",
    "                     self.window_resolution) % self.window_resolution\n",
    "            padding = pad_b > 0 or pad_r > 0\n",
    "\n",
    "            if padding:\n",
    "                x = torch.nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))\n",
    "\n",
    "            pH, pW = H + pad_b, W + pad_r\n",
    "            nH = pH // self.window_resolution\n",
    "            nW = pW // self.window_resolution\n",
    "            # window partition, BHWC -> B(nHh)(nWw)C -> BnHnWhwC -> (BnHnW)hwC -> (BnHnW)Chw\n",
    "            x = x.view(B, nH, self.window_resolution, nW, self.window_resolution, C).transpose(2, 3).reshape(\n",
    "                B * nH * nW, self.window_resolution, self.window_resolution, C\n",
    "            ).permute(0, 3, 1, 2)\n",
    "            x = self.attn(x)\n",
    "            # window reverse, (BnHnW)Chw -> (BnHnW)hwC -> BnHnWhwC -> B(nHh)(nWw)C -> BHWC\n",
    "            x = x.permute(0, 2, 3, 1).view(B, nH, nW, self.window_resolution, self.window_resolution,\n",
    "                       C).transpose(2, 3).reshape(B, pH, pW, C)\n",
    "            if padding:\n",
    "                x = x[:, :H, :W].contiguous()\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EfficientViTBlock(torch.nn.Module):    \n",
    "    \"\"\" A basic EfficientViT building block.\n",
    "\n",
    "    Args:\n",
    "        type (str): Type for token mixer. Default: 's' for self-attention.\n",
    "        ed (int): Number of input channels.\n",
    "        kd (int): Dimension for query and key in the token mixer.\n",
    "        nh (int): Number of attention heads.\n",
    "        ar (int): Multiplier for the query dim for value dimension.\n",
    "        resolution (int): Input resolution.\n",
    "        window_resolution (int): Local window resolution.\n",
    "        kernels (List[int]): The kernel size of the dw conv on query.\n",
    "    \"\"\"\n",
    "    def __init__(self, type,\n",
    "                 ed, kd, nh=8,\n",
    "                 ar=4,\n",
    "                 resolution=14,\n",
    "                 window_resolution=7,\n",
    "                 kernels=[5, 5, 5, 5],):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.dw0 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn0 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "        if type == 's':\n",
    "            self.mixer = Residual(LocalWindowAttention(ed, kd, nh, attn_ratio=ar, \\\n",
    "                    resolution=resolution, window_resolution=window_resolution, kernels=kernels))\n",
    "                \n",
    "        self.dw1 = Residual(Conv2d_BN(ed, ed, 3, 1, 1, groups=ed, bn_weight_init=0., resolution=resolution))\n",
    "        self.ffn1 = Residual(FFN(ed, int(ed * 2), resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn1(self.dw1(self.mixer(self.ffn0(self.dw0(x)))))\n",
    "\n",
    "\n",
    "class EfficientViT(torch.nn.Module):\n",
    "    def __init__(self, img_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 stages=['s', 's', 's'],\n",
    "                 embed_dim=[64, 128, 192],\n",
    "                 key_dim=[16, 16, 16],\n",
    "                 depth=[1, 2, 3],\n",
    "                 num_heads=[4, 4, 4],\n",
    "                 window_size=[7, 7, 7],\n",
    "                 kernels=[5, 5, 5, 5],\n",
    "                 down_ops=[['subsample', 2], ['subsample', 2], ['']],\n",
    "                 distillation=False,):\n",
    "        super().__init__()\n",
    "\n",
    "        resolution = img_size\n",
    "        # Patch embedding\n",
    "        self.patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1, resolution=resolution), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1, resolution=resolution // 2), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1, resolution=resolution // 4), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1, resolution=resolution // 8))\n",
    "\n",
    "        resolution = img_size // patch_size\n",
    "        attn_ratio = [embed_dim[i] / (key_dim[i] * num_heads[i]) for i in range(len(embed_dim))]\n",
    "        self.blocks1 = []\n",
    "        self.blocks2 = []\n",
    "        self.blocks3 = []\n",
    "\n",
    "        # Build EfficientViT blocks\n",
    "        for i, (stg, ed, kd, dpth, nh, ar, wd, do) in enumerate(\n",
    "                zip(stages, embed_dim, key_dim, depth, num_heads, attn_ratio, window_size, down_ops)):\n",
    "            for d in range(dpth):\n",
    "                eval('self.blocks' + str(i+1)).append(EfficientViTBlock(stg, ed, kd, nh, ar, resolution, wd, kernels))\n",
    "            if do[0] == 'subsample':\n",
    "                # Build EfficientViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('self.blocks' + str(i+2))\n",
    "                resolution_ = (resolution - 1) // do[1] + 1\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i], resolution=resolution)),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2), resolution)),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2], resolution))\n",
    "                resolution = resolution_\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1], resolution=resolution)),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2), resolution)),))\n",
    "        self.blocks1 = torch.nn.Sequential(*self.blocks1)\n",
    "        self.blocks2 = torch.nn.Sequential(*self.blocks2)\n",
    "        self.blocks3 = torch.nn.Sequential(*self.blocks3)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {x for x in self.state_dict().keys() if 'attention_biases' in x}\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.patch_embed(x)\n",
    "        y1 = self.blocks1(y0)\n",
    "        y2 = self.blocks2(y1)\n",
    "        y3 = self.blocks3(y2)\n",
    "\n",
    "        return y3, [y0, y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a15571ab-97f2-488a-bab2-354168b94280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        # print(f\"Enc layer pre: {enc_layer.shape}\")\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            pad = (x.shape[-1] - enc_layer.shape[-1]) // 2\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(pad, pad, pad, pad), mode='constant', value=0.0)\n",
    "        # print(f\"X: {x.shape}\")\n",
    "        # print(f\"Enc layer post: {enc_layer.shape}\")\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        # print(f\"Final X: {x.shape}\")\n",
    "        x = self.end_up_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, device, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(192, 128, kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128, 64, flag=True, sep_conv_filters=192, name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(64, 32, flag=False, sep_conv_filters=96 if typ == 's' else 96 if typ == 'xs' else 64, name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(32, 64, flag=False, sep_conv_filters=128 if typ == 's' else 64 if typ == 'xs' else 32, name='up3', device=device)\n",
    "        self.ups_block_4 = UpSample_layer(64, 64, flag=False, sep_conv_filters=128 if typ == 's' else 64 if typ == 'xs' else 32, name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(64, 1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[2])\n",
    "        x = self.ups_block_2(x, enc_layer_list[1])\n",
    "        x = self.ups_block_3(x, enc_layer_list[0])\n",
    "        x = self.ups_block_4(x, x)\n",
    "        x = self.conv2d_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c4bb83e7-c987-42eb-9651-fc2676b2e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_model(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(build_model, self).__init__()\n",
    "        # self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
    "        # self.sample_cnt = 0 ############################## Time measurament\n",
    "        # self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
    "        # self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
    "\n",
    "        self.encoder = EfficientViT(img_size=param['img_res'][1]) ############################## Time measurament\n",
    "        self.decoder = SPEED_decoder(device=device, typ='s')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b3d1d-896e-4872-b055-facadf8ce8ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a5f7fe73-d04a-4bfe-82f8-b09e51abe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    return torch.log(x) / math.log(10)\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.irmse, self.imae = 0, 0\n",
    "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
    "        self.absrel, self.lg10 = 0, 0\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def set_to_worst(self):\n",
    "        self.irmse, self.imae = np.inf, np.inf\n",
    "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
    "        self.absrel, self.lg10 = np.inf, np.inf\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
    "        self.irmse, self.imae = irmse, imae\n",
    "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
    "        self.absrel, self.lg10 = absrel, lg10\n",
    "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
    "\n",
    "    def evaluate(self, output, target):\n",
    "        valid_mask = target > 0\n",
    "\n",
    "        output = output[valid_mask]\n",
    "        target = target[valid_mask]\n",
    "        \n",
    "\n",
    "        abs_diff = (output - target).abs()\n",
    "\n",
    "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
    "        self.rmse = math.sqrt(self.mse)\n",
    "        self.mae = float(abs_diff.mean())\n",
    "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
    "        self.absrel = float((abs_diff / target).mean())\n",
    "\n",
    "        maxRatio = torch.max(output / target, target / output)\n",
    "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
    "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
    "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
    "\n",
    "        inv_output = 1 / output\n",
    "        inv_target = 1 / target\n",
    "        abs_inv_diff = (inv_output - inv_target).abs()\n",
    "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
    "        self.imae = float(abs_inv_diff.mean())\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0.0\n",
    "        self.sum_irmse, self.sum_imae = 0, 0\n",
    "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
    "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
    "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, result, n=1):\n",
    "        self.count += n\n",
    "\n",
    "        self.sum_irmse += n * result.irmse\n",
    "        self.sum_imae += n * result.imae\n",
    "        self.sum_mse += n * result.mse\n",
    "        self.sum_rmse += n * result.rmse\n",
    "        self.sum_mae += n * result.mae\n",
    "        self.sum_absrel += n * result.absrel\n",
    "        self.sum_lg10 += n * result.lg10\n",
    "        self.sum_delta1 += n * result.delta1\n",
    "        self.sum_delta2 += n * result.delta2\n",
    "        self.sum_delta3 += n * result.delta3\n",
    "\n",
    "    def average(self):\n",
    "        avg = Result()\n",
    "        avg.update(\n",
    "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
    "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
    "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
    "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
    "        return avg\n",
    "\n",
    "\n",
    "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
    "    best_worst_dict = {}\n",
    "    result = Result()\n",
    "    result.set_to_worst()\n",
    "    average_meter = AverageMeter()\n",
    "    model.eval()  # switch to evaluate mode\n",
    "\n",
    "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
    "        inputs, depths = inputs.cuda(), depths.cuda()\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "        result.evaluate(predictions, depths)\n",
    "        average_meter.update(result)  # (result, inputs.size(0))\n",
    "        best_worst_dict[i] = result.rmse\n",
    "\n",
    "    avg = average_meter.average()\n",
    "\n",
    "    print('MAE={average.mae:.3f}\\n'\n",
    "          'RMSE={average.rmse:.3f}\\n'\n",
    "          'Delta1={average.delta1:.3f}\\n'\n",
    "          'REL={average.absrel:.3f}\\n'\n",
    "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
    "\n",
    "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
    "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
    "\n",
    "    return best_worst_dict, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de100e85-e556-48bd-a6fc-01ae631351ad",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "91da3393-1ccb-48d3-b76f-5b7c4f1a1e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─EfficientViT: 1-1                           [-1, 192, 4, 4]           --\n",
      "|    └─Sequential: 2-1                        [-1, 64, 16, 16]          --\n",
      "|    |    └─Conv2d_BN: 3-1                    [-1, 8, 128, 128]         232\n",
      "|    |    └─ReLU: 3-2                         [-1, 8, 128, 128]         --\n",
      "|    |    └─Conv2d_BN: 3-3                    [-1, 16, 64, 64]          1,184\n",
      "|    |    └─ReLU: 3-4                         [-1, 16, 64, 64]          --\n",
      "|    |    └─Conv2d_BN: 3-5                    [-1, 32, 32, 32]          4,672\n",
      "|    |    └─ReLU: 3-6                         [-1, 32, 32, 32]          --\n",
      "|    |    └─Conv2d_BN: 3-7                    [-1, 64, 16, 16]          18,560\n",
      "|    └─Sequential: 2-2                        [-1, 64, 16, 16]          --\n",
      "|    |    └─EfficientViTBlock: 3-8            [-1, 64, 16, 16]          44,548\n",
      "|    └─Sequential: 2-3                        [-1, 128, 8, 8]           --\n",
      "|    |    └─Sequential: 3-9                   [-1, 64, 16, 16]          17,472\n",
      "|    |    └─PatchMerging: 3-10                [-1, 128, 8, 8]           85,824\n",
      "|    |    └─Sequential: 3-11                  [-1, 128, 8, 8]           67,712\n",
      "|    |    └─EfficientViTBlock: 3-12           [-1, 128, 8, 8]           162,692\n",
      "|    |    └─EfficientViTBlock: 3-13           [-1, 128, 8, 8]           162,692\n",
      "|    └─Sequential: 2-4                        [-1, 192, 4, 4]           --\n",
      "|    |    └─Sequential: 3-14                  [-1, 128, 8, 8]           67,712\n",
      "|    |    └─PatchMerging: 3-15                [-1, 192, 4, 4]           302,592\n",
      "|    |    └─Sequential: 3-16                  [-1, 192, 4, 4]           150,720\n",
      "|    |    └─EfficientViTBlock: 3-17           [-1, 192, 4, 4]           356,480\n",
      "|    |    └─EfficientViTBlock: 3-18           [-1, 192, 4, 4]           356,480\n",
      "|    |    └─EfficientViTBlock: 3-19           [-1, 192, 4, 4]           356,480\n",
      "├─SPEED_decoder: 1-2                          [-1, 1, 64, 64]           --\n",
      "|    └─Conv2d: 2-5                            [-1, 128, 4, 4]           24,576\n",
      "|    └─UpSample_layer: 2-6                    [-1, 64, 8, 8]            --\n",
      "|    |    └─ConvTranspose2d: 3-20             [-1, 64, 8, 8]            73,728\n",
      "|    |    └─Sequential: 3-21                  [-1, 64, 8, 8]            114,688\n",
      "|    └─UpSample_layer: 2-7                    [-1, 32, 16, 16]          --\n",
      "|    |    └─ConvTranspose2d: 3-22             [-1, 32, 16, 16]          18,432\n",
      "|    |    └─Sequential: 3-23                  [-1, 32, 16, 16]          28,672\n",
      "|    └─UpSample_layer: 2-8                    [-1, 64, 32, 32]          --\n",
      "|    |    └─ConvTranspose2d: 3-24             [-1, 64, 32, 32]          18,432\n",
      "|    |    └─Sequential: 3-25                  [-1, 64, 32, 32]          77,824\n",
      "|    └─UpSample_layer: 2-9                    [-1, 64, 64, 64]          --\n",
      "|    |    └─ConvTranspose2d: 3-26             [-1, 64, 64, 64]          36,864\n",
      "|    |    └─Sequential: 3-27                  [-1, 64, 64, 64]          77,824\n",
      "|    └─Conv2d: 2-10                           [-1, 1, 64, 64]           576\n",
      "===============================================================================================\n",
      "Total params: 2,627,668\n",
      "Trainable params: 2,627,668\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 6.39\n",
      "Params size (MB): 10.02\n",
      "Estimated Total Size (MB): 17.16\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "print_model(model=model, input_shape=param['img_res'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524a005fab024ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-15T00:35:27.628147620Z",
     "start_time": "2023-10-15T00:35:21.255178486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: There are 50688 training and 654 testing samples\n",
      " --- Test samples --- \n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 509.86090087890625, min = 107.78614044189453\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.29162493348121643, variance =  1.499471664428711\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 992.6409301757812, min = 187.010009765625\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.114142417907715, mean = -0.197127565741539, variance =  1.3339691162109375\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --- Training augmented samples --- \n",
      "--> Random flipped\n",
      "--> Random mirrored\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 389.2088928222656, min = 82.37699127197266\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.8259815573692322, variance =  1.5658296346664429\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "--> Random flipped\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 470.4520568847656, min = 272.95751953125\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 0.43080082535743713, variance =  1.7236429452896118\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random mirrored\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 990.6998901367188, min = 302.8186340332031\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 1.0471901893615723, variance =  0.8517109155654907\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "--> Depth Shifted of -1 cm\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 588.3338623046875, min = 137.83006286621094\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.640000104904175, min = -2.1136350631713867, mean = 0.27235546708106995, variance =  2.357508420944214\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Image randomly augmented\n",
      "Depth -> Shape = torch.Size([1, 64, 64]), max = 994.4202880859375, min = 149.62440490722656\n",
      "IMG -> Shape = torch.Size([3, 256, 256]), max = 2.149726629257202, min = -2.0929322242736816, mean = 0.040091175585985184, variance =  1.2828030586242676\n",
      "\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3\n",
      "**************************  ./results/v3_2/pyramid6_rmse_v3example&augment_img/\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─EfficientViT: 1-1                           [-1, 192, 4, 4]           --\n",
      "|    └─Sequential: 2-1                        [-1, 64, 16, 16]          --\n",
      "|    |    └─Conv2d_BN: 3-1                    [-1, 8, 128, 128]         232\n",
      "|    |    └─ReLU: 3-2                         [-1, 8, 128, 128]         --\n",
      "|    |    └─Conv2d_BN: 3-3                    [-1, 16, 64, 64]          1,184\n",
      "|    |    └─ReLU: 3-4                         [-1, 16, 64, 64]          --\n",
      "|    |    └─Conv2d_BN: 3-5                    [-1, 32, 32, 32]          4,672\n",
      "|    |    └─ReLU: 3-6                         [-1, 32, 32, 32]          --\n",
      "|    |    └─Conv2d_BN: 3-7                    [-1, 64, 16, 16]          18,560\n",
      "|    └─Sequential: 2-2                        [-1, 64, 16, 16]          --\n",
      "|    |    └─EfficientViTBlock: 3-8            [-1, 64, 16, 16]          44,548\n",
      "|    └─Sequential: 2-3                        [-1, 128, 8, 8]           --\n",
      "|    |    └─Sequential: 3-9                   [-1, 64, 16, 16]          17,472\n",
      "|    |    └─PatchMerging: 3-10                [-1, 128, 8, 8]           85,824\n",
      "|    |    └─Sequential: 3-11                  [-1, 128, 8, 8]           67,712\n",
      "|    |    └─EfficientViTBlock: 3-12           [-1, 128, 8, 8]           162,692\n",
      "|    |    └─EfficientViTBlock: 3-13           [-1, 128, 8, 8]           162,692\n",
      "|    └─Sequential: 2-4                        [-1, 192, 4, 4]           --\n",
      "|    |    └─Sequential: 3-14                  [-1, 128, 8, 8]           67,712\n",
      "|    |    └─PatchMerging: 3-15                [-1, 192, 4, 4]           302,592\n",
      "|    |    └─Sequential: 3-16                  [-1, 192, 4, 4]           150,720\n",
      "|    |    └─EfficientViTBlock: 3-17           [-1, 192, 4, 4]           356,480\n",
      "|    |    └─EfficientViTBlock: 3-18           [-1, 192, 4, 4]           356,480\n",
      "|    |    └─EfficientViTBlock: 3-19           [-1, 192, 4, 4]           356,480\n",
      "├─SPEED_decoder: 1-2                          [-1, 1, 64, 64]           --\n",
      "|    └─Conv2d: 2-5                            [-1, 128, 4, 4]           24,576\n",
      "|    └─UpSample_layer: 2-6                    [-1, 64, 8, 8]            --\n",
      "|    |    └─ConvTranspose2d: 3-20             [-1, 64, 8, 8]            73,728\n",
      "|    |    └─Sequential: 3-21                  [-1, 64, 8, 8]            114,688\n",
      "|    └─UpSample_layer: 2-7                    [-1, 32, 16, 16]          --\n",
      "|    |    └─ConvTranspose2d: 3-22             [-1, 32, 16, 16]          18,432\n",
      "|    |    └─Sequential: 3-23                  [-1, 32, 16, 16]          28,672\n",
      "|    └─UpSample_layer: 2-8                    [-1, 64, 32, 32]          --\n",
      "|    |    └─ConvTranspose2d: 3-24             [-1, 64, 32, 32]          18,432\n",
      "|    |    └─Sequential: 3-25                  [-1, 64, 32, 32]          77,824\n",
      "|    └─UpSample_layer: 2-9                    [-1, 64, 64, 64]          --\n",
      "|    |    └─ConvTranspose2d: 3-26             [-1, 64, 64, 64]          36,864\n",
      "|    |    └─Sequential: 3-27                  [-1, 64, 64, 64]          77,824\n",
      "|    └─Conv2d: 2-10                           [-1, 1, 64, 64]           576\n",
      "===============================================================================================\n",
      "Total params: 2,627,668\n",
      "Trainable params: 2,627,668\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 209.81\n",
      "===============================================================================================\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 6.39\n",
      "Params size (MB): 10.02\n",
      "Estimated Total Size (MB): 17.16\n",
      "===============================================================================================\n",
      "The build_model model has: 2627668 trainable parameters\n",
      "Start training: build_model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:46<00:00,  3.50step/s, Loss=260, Acc=9.48, Lr=0.001, L_mae=109, L_norm=66.8, L_grad=60.4, L_ssim=23.9]\n",
      "Epoch 1/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.30step/s, Loss=201, Acc=11.6, RMSE=90.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 90.579 at epoch 1\n",
      "New best ACCURACY: 11.623 at epoch 1\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:46<00:00,  3.49step/s, Loss=222, Acc=11.7, Lr=0.001, L_mae=90, L_norm=60.2, L_grad=51.6, L_ssim=20]\n",
      "Epoch 2/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.53step/s, Loss=189, Acc=13.6, RMSE=83.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 83.814 at epoch 2\n",
      "New best ACCURACY: 13.606 at epoch 2\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 - Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:44<00:00,  3.53step/s, Loss=208, Acc=13, Lr=0.001, L_mae=82.5, L_norm=57, L_grad=50, L_ssim=18.8]\n",
      "Epoch 3/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.01step/s, Loss=184, Acc=14.8, RMSE=81.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 81.669 at epoch 3\n",
      "New best ACCURACY: 14.772 at epoch 3\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.51step/s, Loss=198, Acc=14.4, Lr=0.001, L_mae=76.9, L_norm=54.5, L_grad=48.8, L_ssim=17.9]\n",
      "Epoch 4/120 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 83.57step/s, Loss=180, Acc=13.9, RMSE=81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 80.978 at epoch 4\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:43<00:00,  3.54step/s, Loss=190, Acc=15.4, Lr=0.001, L_mae=72.6, L_norm=52.7, L_grad=47.7, L_ssim=17.2]\n",
      "Epoch 5/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 83.77step/s, Loss=176, Acc=14.8, RMSE=78.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 78.707 at epoch 5\n",
      "New best ACCURACY: 14.844 at epoch 5\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:43<00:00,  3.54step/s, Loss=184, Acc=16.5, Lr=0.001, L_mae=68.9, L_norm=51.2, L_grad=46.9, L_ssim=16.5]\n",
      "Epoch 6/120 - Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 83.77step/s, Loss=177, Acc=15, RMSE=78.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 78.532 at epoch 6\n",
      "New best ACCURACY: 15.025 at epoch 6\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 - Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:43<00:00,  3.54step/s, Loss=178, Acc=17.2, Lr=0.001, L_mae=66.3, L_norm=50, L_grad=46, L_ssim=16]\n",
      "Epoch 7/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.67step/s, Loss=171, Acc=16.1, RMSE=75.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 75.075 at epoch 7\n",
      "New best ACCURACY: 16.078 at epoch 7\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.51step/s, Loss=174, Acc=18, Lr=0.001, L_mae=63.9, L_norm=49, L_grad=45.5, L_ssim=15.6]\n",
      "Epoch 8/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.20step/s, Loss=166, Acc=16.8, RMSE=73.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 73.184 at epoch 8\n",
      "New best ACCURACY: 16.769 at epoch 8\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:44<00:00,  3.52step/s, Loss=170, Acc=18.9, Lr=0.001, L_mae=61.7, L_norm=48.1, L_grad=44.8, L_ssim=15.1]\n",
      "Epoch 9/120 - Validation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.98step/s, Loss=168, Acc=16.7, RMSE=73.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.52step/s, Loss=166, Acc=19.5, Lr=0.001, L_mae=59.7, L_norm=47.2, L_grad=44.2, L_ssim=14.7]\n",
      "Epoch 10/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.72step/s, Loss=165, Acc=16.6, RMSE=73.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 73.141 at epoch 10\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:46<00:00,  3.50step/s, Loss=163, Acc=20.1, Lr=0.001, L_mae=58.2, L_norm=46.5, L_grad=43.7, L_ssim=14.4]\n",
      "Epoch 11/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 83.01step/s, Loss=165, Acc=16.2, RMSE=73.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:43<00:00,  3.55step/s, Loss=159, Acc=21, Lr=0.001, L_mae=55.8, L_norm=45.7, L_grad=43.1, L_ssim=13.9]\n",
      "Epoch 12/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.42step/s, Loss=164, Acc=16.7, RMSE=71.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 71.833 at epoch 12\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:43<00:00,  3.54step/s, Loss=156, Acc=21.8, Lr=0.001, L_mae=54.5, L_norm=45.1, L_grad=42.6, L_ssim=13.7]\n",
      "Epoch 13/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.64step/s, Loss=162, Acc=16.1, RMSE=72.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.51step/s, Loss=153, Acc=22.4, Lr=0.001, L_mae=52.8, L_norm=44.4, L_grad=42.1, L_ssim=13.3]\n",
      "Epoch 14/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.87step/s, Loss=160, Acc=17.5, RMSE=70.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.542 at epoch 14\n",
      "New best ACCURACY: 17.515 at epoch 14\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:44<00:00,  3.53step/s, Loss=151, Acc=22.9, Lr=0.001, L_mae=51.7, L_norm=44, L_grad=41.8, L_ssim=13.1]\n",
      "Epoch 15/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.50step/s, Loss=159, Acc=17.4, RMSE=70.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 70.521 at epoch 15\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.51step/s, Loss=147, Acc=23.8, Lr=0.001, L_mae=50, L_norm=43.2, L_grad=41.3, L_ssim=12.8]\n",
      "Epoch 16/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 83.37step/s, Loss=159, Acc=16.4, RMSE=71.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:45<00:00,  3.51step/s, Loss=146, Acc=23.9, Lr=0.001, L_mae=49.6, L_norm=43, L_grad=41, L_ssim=12.6]\n",
      "Epoch 17/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:11<00:00, 56.47step/s, Loss=160, Acc=16.9, RMSE=70.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:13<00:00,  3.13step/s, Loss=143, Acc=24.7, Lr=0.001, L_mae=48.1, L_norm=42.4, L_grad=40.6, L_ssim=12.3]\n",
      "Epoch 18/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.02step/s, Loss=156, Acc=18.7, RMSE=68.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 68.445 at epoch 18\n",
      "New best ACCURACY: 18.653 at epoch 18\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:10<00:00,  3.16step/s, Loss=141, Acc=25.4, Lr=0.001, L_mae=47, L_norm=42.1, L_grad=40.1, L_ssim=12]\n",
      "Epoch 19/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 78.70step/s, Loss=157, Acc=17.7, RMSE=70.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:11<00:00,  3.15step/s, Loss=139, Acc=26, Lr=0.001, L_mae=45.7, L_norm=41.5, L_grad=39.8, L_ssim=11.9]\n",
      "Epoch 20/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 78.89step/s, Loss=155, Acc=18.1, RMSE=69.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:06<00:00,  3.22step/s, Loss=137, Acc=26.4, Lr=0.001, L_mae=45, L_norm=41.2, L_grad=39.5, L_ssim=11.6]\n",
      "Epoch 21/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.14step/s, Loss=157, Acc=18.1, RMSE=69.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:08<00:00,  3.19step/s, Loss=134, Acc=27.5, Lr=0.001, L_mae=43.4, L_norm=40.6, L_grad=39, L_ssim=11.4]\n",
      "Epoch 22/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.53step/s, Loss=154, Acc=18.7, RMSE=67.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.705 at epoch 22\n",
      "New best ACCURACY: 18.703 at epoch 22\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=134, Acc=27.5, Lr=0.001, L_mae=43.1, L_norm=40.4, L_grad=38.9, L_ssim=11.3]\n",
      "Epoch 23/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.95step/s, Loss=154, Acc=18, RMSE=68.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:08<00:00,  3.19step/s, Loss=132, Acc=28.4, Lr=0.001, L_mae=42.1, L_norm=39.8, L_grad=38.6, L_ssim=11.1]\n",
      "Epoch 24/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.37step/s, Loss=152, Acc=19.3, RMSE=67.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.462 at epoch 24\n",
      "New best ACCURACY: 19.279 at epoch 24\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:08<00:00,  3.19step/s, Loss=130, Acc=28.7, Lr=0.001, L_mae=41.5, L_norm=39.5, L_grad=38.3, L_ssim=10.9]\n",
      "Epoch 25/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.03step/s, Loss=153, Acc=19, RMSE=67.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 67.256 at epoch 25\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=129, Acc=29.5, Lr=0.001, L_mae=40.5, L_norm=39.3, L_grad=38, L_ssim=10.7]\n",
      "Epoch 26/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.37step/s, Loss=151, Acc=19.3, RMSE=66.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 66.669 at epoch 26\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=127, Acc=30.2, Lr=0.001, L_mae=39.8, L_norm=39, L_grad=37.7, L_ssim=10.6]\n",
      "Epoch 27/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.06step/s, Loss=150, Acc=20.1, RMSE=65.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 65.845 at epoch 27\n",
      "New best ACCURACY: 20.054 at epoch 27\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:08<00:00,  3.19step/s, Loss=126, Acc=30.1, Lr=0.001, L_mae=39.5, L_norm=38.8, L_grad=37.6, L_ssim=10.5]\n",
      "Epoch 28/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.19step/s, Loss=155, Acc=16.9, RMSE=70.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:09<00:00,  3.18step/s, Loss=125, Acc=30.7, Lr=0.001, L_mae=38.9, L_norm=38.5, L_grad=37.3, L_ssim=10.3]\n",
      "Epoch 29/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.23step/s, Loss=152, Acc=18.4, RMSE=66.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.21step/s, Loss=123, Acc=31.5, Lr=0.001, L_mae=38, L_norm=38.1, L_grad=37, L_ssim=10.2]\n",
      "Epoch 30/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.42step/s, Loss=148, Acc=19.7, RMSE=65.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 65.200 at epoch 30\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=122, Acc=31.9, Lr=0.001, L_mae=37.3, L_norm=37.8, L_grad=36.8, L_ssim=10]\n",
      "Epoch 31/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.43step/s, Loss=149, Acc=19.8, RMSE=66.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=121, Acc=32, Lr=0.001, L_mae=37.2, L_norm=37.7, L_grad=36.6, L_ssim=9.93]\n",
      "Epoch 32/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.88step/s, Loss=148, Acc=19.1, RMSE=65.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:12<00:00,  3.13step/s, Loss=121, Acc=32.5, Lr=0.001, L_mae=36.8, L_norm=37.4, L_grad=36.5, L_ssim=9.85]\n",
      "Epoch 33/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.98step/s, Loss=149, Acc=19, RMSE=65.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=119, Acc=33.1, Lr=0.001, L_mae=36.1, L_norm=37.1, L_grad=36.3, L_ssim=9.72]\n",
      "Epoch 34/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.48step/s, Loss=148, Acc=19.9, RMSE=65.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=118, Acc=33.4, Lr=0.001, L_mae=35.5, L_norm=36.9, L_grad=36.1, L_ssim=9.57]\n",
      "Epoch 35/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.81step/s, Loss=148, Acc=19.9, RMSE=65.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:06<00:00,  3.22step/s, Loss=117, Acc=34.1, Lr=0.001, L_mae=34.8, L_norm=36.6, L_grad=35.8, L_ssim=9.43]\n",
      "Epoch 36/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.52step/s, Loss=146, Acc=19.8, RMSE=64.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.689 at epoch 36\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:05<00:00,  3.22step/s, Loss=117, Acc=34.1, Lr=0.001, L_mae=34.8, L_norm=36.5, L_grad=35.8, L_ssim=9.41]\n",
      "Epoch 37/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 78.74step/s, Loss=147, Acc=19.5, RMSE=65.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:06<00:00,  3.21step/s, Loss=115, Acc=35.2, Lr=0.001, L_mae=33.9, L_norm=36.3, L_grad=35.4, L_ssim=9.23]\n",
      "Epoch 38/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.94step/s, Loss=147, Acc=20.4, RMSE=65.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.414 at epoch 38\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=115, Acc=35, Lr=0.001, L_mae=34.1, L_norm=36.2, L_grad=35.4, L_ssim=9.22]\n",
      "Epoch 39/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.30step/s, Loss=146, Acc=20.6, RMSE=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 64.266 at epoch 39\n",
      "New best ACCURACY: 20.553 at epoch 39\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:06<00:00,  3.21step/s, Loss=114, Acc=35.4, Lr=0.001, L_mae=33.6, L_norm=35.9, L_grad=35.3, L_ssim=9.14]\n",
      "Epoch 40/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.35step/s, Loss=147, Acc=19.7, RMSE=65.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.21step/s, Loss=113, Acc=35.3, Lr=0.001, L_mae=33.4, L_norm=35.9, L_grad=35.1, L_ssim=9.04]\n",
      "Epoch 41/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.90step/s, Loss=145, Acc=20.4, RMSE=63.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 63.608 at epoch 41\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:06<00:00,  3.21step/s, Loss=113, Acc=35.8, Lr=0.001, L_mae=33.1, L_norm=35.6, L_grad=35.1, L_ssim=8.99]\n",
      "Epoch 42/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.34step/s, Loss=146, Acc=20.6, RMSE=64.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "New best ACCURACY: 20.645 at epoch 42\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=111, Acc=36.8, Lr=0.001, L_mae=32.3, L_norm=35.3, L_grad=34.7, L_ssim=8.81]\n",
      "Epoch 43/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.03step/s, Loss=147, Acc=20.3, RMSE=65.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=111, Acc=36.6, Lr=0.001, L_mae=32.4, L_norm=35.3, L_grad=34.7, L_ssim=8.83]\n",
      "Epoch 44/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 79.45step/s, Loss=146, Acc=20, RMSE=64.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=110, Acc=37.1, Lr=0.001, L_mae=31.8, L_norm=35.2, L_grad=34.5, L_ssim=8.7]\n",
      "Epoch 45/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.07step/s, Loss=144, Acc=20.6, RMSE=62.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.947 at epoch 45\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:07<00:00,  3.20step/s, Loss=110, Acc=37.6, Lr=0.001, L_mae=31.5, L_norm=35, L_grad=34.4, L_ssim=8.66]\n",
      "Epoch 46/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.58step/s, Loss=146, Acc=19.7, RMSE=64.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.23step/s, Loss=109, Acc=37.8, Lr=0.001, L_mae=31.2, L_norm=34.8, L_grad=34.2, L_ssim=8.54]\n",
      "Epoch 47/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.28step/s, Loss=146, Acc=20.7, RMSE=64.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.657 at epoch 47\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:05<00:00,  3.23step/s, Loss=108, Acc=37.9, Lr=0.001, L_mae=31, L_norm=34.6, L_grad=34.1, L_ssim=8.51]\n",
      "Epoch 48/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.07step/s, Loss=145, Acc=20.7, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "New best ACCURACY: 20.703 at epoch 48\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.24step/s, Loss=108, Acc=37.9, Lr=0.001, L_mae=30.9, L_norm=34.6, L_grad=34, L_ssim=8.43]\n",
      "Epoch 49/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.51step/s, Loss=149, Acc=19.9, RMSE=65.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:05<00:00,  3.23step/s, Loss=107, Acc=38.9, Lr=0.001, L_mae=30.3, L_norm=34.4, L_grad=33.9, L_ssim=8.4]\n",
      "Epoch 50/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.39step/s, Loss=144, Acc=20.8, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 20.804 at epoch 50\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [03:58<00:00,  3.31step/s, Loss=107, Acc=38.7, Lr=0.001, L_mae=30.3, L_norm=34.4, L_grad=33.8, L_ssim=8.31]\n",
      "Epoch 51/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.89step/s, Loss=144, Acc=20.6, RMSE=62.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.743 at epoch 51\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=106, Acc=39.2, Lr=0.001, L_mae=30.1, L_norm=34.2, L_grad=33.7, L_ssim=8.29]\n",
      "Epoch 52/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 80.77step/s, Loss=145, Acc=20.4, RMSE=63.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=106, Acc=39.5, Lr=0.001, L_mae=29.8, L_norm=34.1, L_grad=33.6, L_ssim=8.23]\n",
      "Epoch 53/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.16step/s, Loss=145, Acc=20.8, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 28 epochs\n",
      "New best ACCURACY: 20.822 at epoch 53\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=105, Acc=39.5, Lr=0.001, L_mae=29.6, L_norm=33.9, L_grad=33.5, L_ssim=8.15]\n",
      "Epoch 54/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.32step/s, Loss=145, Acc=19.9, RMSE=63.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.25step/s, Loss=104, Acc=39.9, Lr=0.001, L_mae=29.3, L_norm=33.8, L_grad=33.4, L_ssim=8.07]\n",
      "Epoch 55/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.16step/s, Loss=145, Acc=21.1, RMSE=63.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "New best ACCURACY: 21.073 at epoch 55\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:02<00:00,  3.26step/s, Loss=104, Acc=40.2, Lr=0.001, L_mae=29.1, L_norm=33.6, L_grad=33.3, L_ssim=8.04]\n",
      "Epoch 56/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.93step/s, Loss=144, Acc=21.2, RMSE=64.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 21.188 at epoch 56\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:02<00:00,  3.27step/s, Loss=104, Acc=40.5, Lr=0.001, L_mae=28.9, L_norm=33.6, L_grad=33.2, L_ssim=7.99]\n",
      "Epoch 57/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.33step/s, Loss=144, Acc=21.1, RMSE=64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=103, Acc=40.6, Lr=0.001, L_mae=28.6, L_norm=33.5, L_grad=33, L_ssim=7.91]\n",
      "Epoch 58/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.90step/s, Loss=146, Acc=21.3, RMSE=64.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "New best ACCURACY: 21.260 at epoch 58\n",
      "EarlyStopping increased due to Accuracy, stop in 27 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 - Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:02<00:00,  3.27step/s, Loss=103, Acc=40.8, Lr=0.001, L_mae=28.7, L_norm=33.4, L_grad=33, L_ssim=7.9]\n",
      "Epoch 59/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.20step/s, Loss=143, Acc=20.5, RMSE=63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 26 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:02<00:00,  3.27step/s, Loss=102, Acc=41.3, Lr=0.001, L_mae=28.3, L_norm=33.2, L_grad=32.9, L_ssim=7.85]\n",
      "Epoch 60/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.90step/s, Loss=144, Acc=20.7, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 25 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 - Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:05<00:00,  3.23step/s, Loss=102, Acc=41.7, Lr=0.001, L_mae=27.9, L_norm=33.1, L_grad=32.9, L_ssim=7.8]\n",
      "Epoch 61/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.52step/s, Loss=144, Acc=21, RMSE=63.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 24 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.23step/s, Loss=102, Acc=40.8, Lr=0.001, L_mae=28.3, L_norm=33.1, L_grad=32.8, L_ssim=7.77]\n",
      "Epoch 62/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.19step/s, Loss=142, Acc=20.8, RMSE=63.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 - Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=101, Acc=41.5, Lr=0.001, L_mae=28, L_norm=32.9, L_grad=32.7, L_ssim=7.73]\n",
      "Epoch 63/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 81.76step/s, Loss=145, Acc=20.6, RMSE=64.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.24step/s, Loss=101, Acc=42.1, Lr=0.001, L_mae=27.6, L_norm=32.9, L_grad=32.6, L_ssim=7.66]\n",
      "Epoch 64/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.13step/s, Loss=143, Acc=21.3, RMSE=63.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 21 epochs\n",
      "New best ACCURACY: 21.334 at epoch 64\n",
      "EarlyStopping increased due to Accuracy, stop in 23 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:05<00:00,  3.22step/s, Loss=101, Acc=42.2, Lr=0.001, L_mae=27.5, L_norm=32.9, L_grad=32.6, L_ssim=7.63]\n",
      "Epoch 65/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:07<00:00, 82.53step/s, Loss=144, Acc=20.9, RMSE=64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 22 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 - Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:03<00:00,  3.25step/s, Loss=99.9, Acc=42.6, Lr=0.001, L_mae=27.2, L_norm=32.7, L_grad=32.4, L_ssim=7.6]\n",
      "Epoch 66/120 - Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.32step/s, Loss=142, Acc=20.7, RMSE=62.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best RMSE: 62.350 at epoch 66\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 - Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792/792 [04:04<00:00,  3.24step/s, Loss=99.8, Acc=42.4, Lr=0.001, L_mae=27.3, L_norm=32.5, L_grad=32.4, L_ssim=7.56]\n",
      "Epoch 67/120 - Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 654/654 [00:08<00:00, 81.03step/s, Loss=143, Acc=20.7, RMSE=63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE did not improved, EarlyStopping from 29 epochs\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 - Training:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 700/792 [03:34<00:28,  3.26step/s, Loss=100, Acc=42.4, Lr=0.001, L_mae=27.4, L_norm=32.7, L_grad=32.3, L_ssim=7.56]"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "def process(device):\n",
    "    # Set-seed\n",
    "    seed = param['seed']\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Datasets loading\n",
    "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "        dts_root_path=dataset_root,\n",
    "        rgb_h_res=param['img_res'][1],\n",
    "        d_h_res=param['depth_img_res'][1],\n",
    "        bs_train=param['batch_size'],\n",
    "        bs_eval=param['batch_size_eval'],\n",
    "        num_workers=param['n_workers'],\n",
    "    )\n",
    "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
    "    # Prints samples\n",
    "    print(' --- Test samples --- ')\n",
    "    print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
    "              save_model_root=save_model_root)\n",
    "    print(' --- Training augmented samples --- ')\n",
    "    print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
    "                  save_model_root=save_model_root)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # Globals\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
    "               'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
    "    min_rmse = float('inf')\n",
    "    min_acc = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    # Loss\n",
    "    criterion = balanced_loss_function(device=device)\n",
    "    # Model\n",
    "    model = build_model(device=device).to(device=device)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    print_model(model=model, input_shape=param['img_res'])\n",
    "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=param['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
    "    )\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=param['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
    "        cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
    "    )\n",
    "    # Early stopping\n",
    "    trigger_times, early_stopping_epochs = 0, param['e_stop_epochs']\n",
    "    print(\"Start training: {}\\n\".format(model_name))\n",
    "    \n",
    "    epochs = param['epochs']\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        iter = 1\n",
    "        model.train()\n",
    "        running_loss, accuracy = 0, 0\n",
    "        running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Forward\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # Compute loss\n",
    "                loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Evaluation and Stats\n",
    "                running_loss += loss.item()\n",
    "                running_l_mae += loss_depth.item()\n",
    "                running_l_norm += loss_normal.item()\n",
    "                running_l_grad += loss_grad.item()\n",
    "                running_l_ssim += loss_ssim.item()\n",
    "\n",
    "                train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                train_loss_list.append(train_loss_support)\n",
    "\n",
    "                accuracy += compute_accuracy(outputs, depths)\n",
    "                tepoch.set_postfix({'Loss': running_loss / iter,\n",
    "                                    'Acc': accuracy.item() / iter,\n",
    "                                    'Lr': param['lr'] if not history['lrs'] else history['lrs'][-1],\n",
    "                                    'L_mae': running_l_mae / iter,\n",
    "                                    'L_norm': running_l_norm / iter,\n",
    "                                    'L_grad': running_l_grad / iter,\n",
    "                                    'L_ssim': running_l_ssim / iter\n",
    "                                    })\n",
    "                iter += 1\n",
    "\n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    # Evaluation metrics\n",
    "                    test_accuracy += compute_accuracy(outputs, depths)\n",
    "                    # Loss\n",
    "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                    test_loss_list.append(test_loss_support)\n",
    "\n",
    "                    # RMSE\n",
    "                    test_rmse += compute_rmse(outputs, depths)\n",
    "                    tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
    "                                        'RMSE': test_rmse.item() / iter})\n",
    "                    iter += 1\n",
    "\n",
    "        # Update history infos\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        history['train_loss'].append(running_loss / len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss / len(test_DataLoader))\n",
    "        history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
    "        history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
    "        # Update history losses infos\n",
    "        history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
    "        history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
    "        history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
    "        history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
    "        # Update scheduler LR\n",
    "        scheduler.step(history['test_rmse'][-1])\n",
    "        # Save model by best RMSE\n",
    "        if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
    "            trigger_times = 0\n",
    "            min_rmse = test_rmse / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best', save_model_root)\n",
    "            print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
    "        # Save model by best ACCURACY\n",
    "        if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
    "            min_acc = test_accuracy / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
    "            print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
    "            if trigger_times > 4:\n",
    "                trigger_times = trigger_times - 2\n",
    "                print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
    "\n",
    "        save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
    "                                 save_path=save_model_root + 'evolution_img/')\n",
    "        save_history(history, save_model_root + model_name + '_history')\n",
    "        # Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if trigger_times == early_stopping_epochs:\n",
    "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
    "            break\n",
    "\n",
    "        # Save loss for graphs\n",
    "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
    "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
    "\n",
    "        print('Finished Training')\n",
    "        save_csv_history(model_name=model_name, path=save_model_root)\n",
    "        plot_history(history, path=save_model_root)\n",
    "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
    "\n",
    "        if os.path.exists(save_model_root + 'example&augment_img/'):\n",
    "            shutil.rmtree(save_model_root + 'example&augment_img/')\n",
    "\n",
    "\n",
    "    # model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
    "    # model, model_name = load_pretrained_model(model=model,\n",
    "    #                                           path_weigths=save_model_root + 'build_model_best',\n",
    "    #                                           device=device,\n",
    "    #                                           do_pretrained=global_var['do_pretrained'],\n",
    "    #                                           imagenet_w_init=global_var['imagenet_w_init'])\n",
    "    # if global_var['do_print_model']:\n",
    "    print_model(model=model, input_shape=param['img_res'])\n",
    "    # print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "\n",
    "    # Evaluate\n",
    "    print(' --- Begin evaluation --- ')\n",
    "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "    print(' --- End evaluation --- ')\n",
    "\n",
    "    sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "    save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "    save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware\n",
    "    device = hardware_check()\n",
    "\n",
    "    # -- TRAIN 1\n",
    "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
    "    # Directory test\n",
    "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
    "    #print(save_model_root)\n",
    "    # Create folders\n",
    "    if not os.path.exists(save_model_root):\n",
    "        os.makedirs(save_model_root)\n",
    "    # if not os.path.exists(save_model_root + 'info_code/'):\n",
    "    #     os.makedirs(save_model_root + 'info_code/')\n",
    "    # files_directory = '/work/project/'\n",
    "    # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
    "    # for f in files:\n",
    "    #     shutil.copy(f, save_model_root + 'info_code/')\n",
    "    # Run process\n",
    "    start_time = perf_counter()\n",
    "    process(device=device)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = perf_counter()\n",
    "    print(\"Total time elapsed: \",end_time - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dca0a-3084-46ff-8ed8-a81bb630e425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
