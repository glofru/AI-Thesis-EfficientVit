{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "g2bPw-5W4hJe"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (1.8.0)\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb3YQA5HFeNz",
    "outputId": "133cab2d-cd5d-4a53-df6d-6cce2fc083c4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uyr3kLf61IIP"
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "    \"seed\": 4242,\n",
    "    \"img_res\": (3, 192, 256),\n",
    "    \"depth_img_res\": (1, 48, 64),\n",
    "    \"n_workers\": 2,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"batch_size_eval\": 1,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lr_patience\": 15,\n",
    "    \"e_stop_epochs\": 30,\n",
    "    \"epochs\": 120,\n",
    "}\n",
    "\n",
    "augmentation_parameters = {\n",
    "    'flip': 0.5,\n",
    "    'mirror': 0.5,\n",
    "    'color&bright': 0.5,\n",
    "    'c_swap': 0.5,\n",
    "    'random_crop': 0.5,\n",
    "    'random_d_shift': 0.5  # range(+-10)cm\n",
    "}\n",
    "\n",
    "dataset_root = './data/NYUv2/'\n",
    "save_model_root = './results/meta_meter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LOqnce-65LvH"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.transform as st\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummaryX import summary\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import torchvision.transforms as TT\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import exp\n",
    "from einops import rearrange\n",
    "import csv\n",
    "import math\n",
    "from time import perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zNWdsJdwlrh"
   },
   "outputs": [],
   "source": [
    "network_type = \"MetaMETER\"\n",
    "old_stout = sys.stdout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm2TAq6B5UBI"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s77jAM-X5VsY"
   },
   "outputs": [],
   "source": [
    "def hardware_check():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Actual device: \", device)\n",
    "    if 'cuda' in device:\n",
    "        print(\"Device info: {}\".format(str(torch.cuda.get_device_properties(device)).split(\"(\")[1])[:-1])\n",
    "\n",
    "    return device\n",
    "\n",
    "\n",
    "def plot_depth_map(dm):\n",
    "\n",
    "    MIN_DEPTH = 0.0\n",
    "    MAX_DEPTH = min(np.max(dm.numpy()), np.percentile(dm, 99))\n",
    "\n",
    "    dm = np.clip(dm, MIN_DEPTH, MAX_DEPTH)\n",
    "    cmap = plt.cm.plasma_r\n",
    "\n",
    "    return dm, cmap, MIN_DEPTH, MAX_DEPTH\n",
    "\n",
    "\n",
    "def resize_keeping_aspect_ratio(img, base):\n",
    "    \"\"\"\n",
    "    Resize the image to a defined length manteining its proportions\n",
    "    Scaling the shortest side of the image to a fixed 'base' length'\n",
    "    \"\"\"\n",
    "\n",
    "    if img.shape[0] <= img.shape[1]:\n",
    "        basewidth = int(base)\n",
    "        wpercent = (basewidth / float(img.shape[0]))\n",
    "        hsize = int((float(img.shape[1]) * float(wpercent)))\n",
    "        img = st.resize(img, (basewidth, hsize), anti_aliasing=False, preserve_range=True)\n",
    "    else:\n",
    "        baseheight = int(base)\n",
    "        wpercent = (baseheight / float(img.shape[1]))\n",
    "        wsize = int((float(img.shape[0]) * float(wpercent)))\n",
    "        img = st.resize(img, (wsize, baseheight), anti_aliasing=False, preserve_range=True)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def compute_rmse(predictions, depths):\n",
    "    valid_mask = depths > 0.0\n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_depths = depths[valid_mask]\n",
    "    mse = (torch.pow((valid_predictions - valid_depths).abs(), 2)).mean()\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, thr=0.05):\n",
    "    valid_mask = y_true > 0.0\n",
    "    valid_pred = y_pred[valid_mask]\n",
    "    valid_true = y_true[valid_mask]\n",
    "    correct = torch.max((valid_true / valid_pred), (valid_pred / valid_true)) < (1 + thr)\n",
    "    return 100 * torch.mean(correct.float())\n",
    "\n",
    "\n",
    "def print_model(model, input_shape):\n",
    "    info = summary(model, input_size=input_shape)\n",
    "    print(info)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def save_checkpoint(model, name, path_save_model):\n",
    "    \"\"\"\n",
    "    Saves a model\n",
    "    \"\"\"\n",
    "    if '_best' in name:\n",
    "        folder = name.split(\"_best\")[0]\n",
    "    elif '_checkpoint' in name:\n",
    "        folder = name.split(\"_checkpoint\")[0]\n",
    "    if not os.path.isdir(path_save_model):\n",
    "        os.makedirs(path_save_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), path_save_model + name)\n",
    "\n",
    "\n",
    "def save_history(history, filepath):\n",
    "    tmp_file = open(filepath + '.pkl', \"wb\")\n",
    "    pickle.dump(history, tmp_file)\n",
    "    tmp_file.close()\n",
    "\n",
    "\n",
    "def save_csv_history(model_name, path):\n",
    "    objects = []\n",
    "    with (open(path + model_name + '_history.pkl', \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "    df = pd.DataFrame(objects)\n",
    "    df.to_csv(path + model_name + '_history.csv', header=False, index=False, sep=\" \")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, path_weigths, device, do_pretrained, imagenet_w_init):\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    if do_pretrained:\n",
    "        print(\"\\nloading checkpoint for entire {}..\\n\".format(model_name))\n",
    "        model_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model.load_state_dict(model_dict)\n",
    "        print(\"checkpoint loaded\\n\")\n",
    "\n",
    "    if imagenet_w_init:\n",
    "        print(\"\\nloading checkpoint from ImageNet {}..\\n\".format(model_name))\n",
    "        pretrained_dict = torch.load(path_weigths, map_location=torch.device(device))\n",
    "        model_dict = model.state_dict()\n",
    "        print('Pretained on ImageNet has: {} trainable parameters'.format(len(pretrained_dict.items())))\n",
    "\n",
    "        # pretrained_param = len(pretrained_dict.items())\n",
    "        counter_param = 0\n",
    "        for i, j in pretrained_dict.items():\n",
    "            if (i in model_dict) and model_dict[i].shape == pretrained_dict[i].shape:\n",
    "                counter_param += 1\n",
    "\n",
    "        print(f'Pertained parameters: {counter_param}\\n')\n",
    "\n",
    "        # 1. filter out unnecessary keys\n",
    "        # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        model_dict.update(pretrained_dict)\n",
    "        # 3. load the new state dict\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "        # alternativa to 2 e 3\n",
    "        # model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Partial initialization computed\\n\")\n",
    "\n",
    "    return model, model_name\n",
    "\n",
    "\n",
    "def plot_graph(f, g, f_label, g_label, title, path):\n",
    "    epochs = range(0, len(f))\n",
    "    plt.plot(epochs, f, 'b', label=f_label)\n",
    "    plt.plot(epochs, g, 'orange', label=g_label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_history(history, path):\n",
    "    plot_graph(history['train_loss'], history['val_loss'], 'Train Loss', 'Val. Loss', 'TrainVal_loss', path)\n",
    "    plot_graph(history['train_acc'], history['val_acc'], 'Train Acc.', 'Val. Acc.', 'TrainVal_acc', path)\n",
    "\n",
    "\n",
    "def plot_loss_parts(history, path, title):\n",
    "    l_mae_list = history['l_mae']\n",
    "    l_norm_list = history['l_norm']\n",
    "    l_grad_list = history['l_grad']\n",
    "    l_ssim_list = history['l_ssim']\n",
    "    epochs = range(0, len(l_mae_list))\n",
    "    plt.plot(epochs, l_mae_list, 'r', label='l_mae')\n",
    "    plt.plot(epochs, l_norm_list, 'g', label='l_norm')\n",
    "    plt.plot(epochs, l_grad_list, 'b', label='l_grad')\n",
    "    plt.plot(epochs, l_ssim_list, 'orange', label='l_ssim')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid('on', color='#cfcfcf')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path + title + '.pdf')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_img(dataset, label, save_model_root, index=None, quantity=1, print_info_aug=False):\n",
    "    for i in range(quantity):\n",
    "        img, depth = dataset.__getitem__(index, print_info_aug)\n",
    "\n",
    "        print(f'Depth -> Shape = {depth.shape}, max = {torch.max(depth)}, min = {torch.min(depth)}')\n",
    "        print(f'IMG -> Shape = {img.shape}, max = {torch.max(img)}, min = {torch.min(img)}, mean = {torch.mean(img)},'\n",
    "              f' variance =  {torch.var(img)}\\n')\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 3)) # 15 NYU # 30 KITTI\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input image')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Grayscale DepthMap')\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Colored DepthMap')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        print(\"************************** \",save_model_root)\n",
    "        save_path = save_model_root + 'example&augment_img/'\n",
    "        print(\"************************** \",save_path)\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + 'img_' + str(i) + '_' + label + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_prediction_examples(model, dataset, device, indices, save_path, ep):\n",
    "    \"\"\"\n",
    "    Shows prediction example\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 3)) # 20 NYU # 40 KITTI\n",
    "    for i, index in zip(range(len(indices)), indices):\n",
    "        img, depth = dataset.__getitem__(index)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.from_numpy(img).to(device))\n",
    "            # Build plot\n",
    "            _, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "            plt.subplot(1, len(indices), i+1)\n",
    "            plt.imshow(np.squeeze(pred.cpu()), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "            cbar = plt.colorbar()\n",
    "            cbar.ax.set_xlabel('cm', size=13, rotation=0)\n",
    "            if False:\n",
    "                plt.axis('off')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.mkdir(save_path)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path + 'img_ep_' + str(ep) + '.pdf')\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def save_best_worst(list_type, type, model, dataset, device, save_model_root):\n",
    "    save_path = save_model_root + type + '_predictions/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for i in range(len(list_type)):\n",
    "        index_image = list_type[i][0]\n",
    "        rmse_value = list_type[i][1]\n",
    "\n",
    "        img, depth = dataset.__getitem__(index=index_image)\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 3)) # 18 NYU # 40 KITTI\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.title(f'Original image {index_image}')\n",
    "        plt.imshow(torch.moveaxis(img, 0, -1), cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.title('Ground Truth')\n",
    "        depth, cmap_dm, vmin, vmax = plot_depth_map(depth)\n",
    "        plt.imshow(torch.moveaxis(depth, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predict\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(torch.unsqueeze(img, dim=0).to(device))\n",
    "\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.title('Predicted DepthMap')\n",
    "        pred, cmap_dm, _, _ = plot_depth_map(torch.squeeze(pred.cpu(), dim=0))\n",
    "        plt.imshow(torch.moveaxis(pred, 0, -1), cmap=cmap_dm, vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.title('Disparity Map, RMSE = {:.2f}'.format(rmse_value))\n",
    "        intensity_img = torch.moveaxis(torch.abs(depth - pred), 0, -1)\n",
    "        plt.imshow(intensity_img, cmap=plt.cm.magma, vmin=0)\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path + '/seq_' + str(i) + '.pdf')\n",
    "        plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def compute_MeanVar(dataset):\n",
    "    r_mean, g_mean, b_mean = [], [], []\n",
    "    r_var, g_var, b_var = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r = np.array(img[0, :, :])\n",
    "        g = np.array(img[1, :, :])\n",
    "        b = np.array(img[2, :, :])\n",
    "\n",
    "        r_mean.append(np.mean(r))\n",
    "        g_mean.append(np.mean(g))\n",
    "        b_mean.append(np.mean(b))\n",
    "\n",
    "        r_var.append(np.var(r))\n",
    "        g_var.append(np.var(g))\n",
    "        b_var.append(np.var(b))\n",
    "\n",
    "    print(f\"The MEAN are: R - {np.mean(r_mean)}, G - {np.mean(g_mean)}, B - {np.mean(b_mean)}\\n\"\n",
    "          f\"The VAR are: R - {np.mean(r_var)}, G - {np.mean(g_var)}, B - {np.mean(b_var)}\")\n",
    "\n",
    "\n",
    "def compute_MeanImg(dataset, save_model_root):\n",
    "    r, g, b = [], [], []\n",
    "    for i in range(dataset.__len__()):\n",
    "        img, _ = dataset.__getitem__(index=i)\n",
    "        r.append(np.array(img[0, :, :]))\n",
    "        g.append(np.array(img[1, :, :]))\n",
    "        b.append(np.array(img[2, :, :]))\n",
    "\n",
    "    r_sum = np.mean(np.stack(r, axis=-1), axis=-1)\n",
    "    g_sum = np.mean(np.stack(g, axis=-1), axis=-1)\n",
    "    b_sum = np.mean(np.stack(b, axis=-1), axis=-1)\n",
    "    mean_img = torch.moveaxis(torch.from_numpy(np.stack([r_sum, g_sum, b_sum], axis=-1)), -1, 0)\n",
    "    np.save(save_model_root + 'nyu_Mimg.npy', mean_img)\n",
    "\n",
    "    print(\"Process Completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8zUH-3na7eAH"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0CFEYqVd7i1H"
   },
   "outputs": [],
   "source": [
    "def pixel_shift(depth_img, shift):\n",
    "    depth_img = depth_img + shift\n",
    "    return depth_img\n",
    "\n",
    "\n",
    "def random_crop(x, y, crop_size=(192, 256)):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    assert x.shape[1] == y.shape[1]\n",
    "    h, w, _ = x.shape\n",
    "    rangew = (w - crop_size[0]) // 2 if w > crop_size[0] else 0\n",
    "    rangeh = (h - crop_size[1]) // 2 if h > crop_size[1] else 0\n",
    "    offsetw = 0 if rangew == 0 else np.random.randint(rangew)\n",
    "    offseth = 0 if rangeh == 0 else np.random.randint(rangeh)\n",
    "    cropped_x = x[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = y[offseth:offseth + crop_size[0], offsetw:offsetw + crop_size[1], :]\n",
    "    cropped_y = cropped_y[:, :, ~np.all(cropped_y == 0, axis=(0, 1))]\n",
    "    if cropped_y.shape[-1] == 0:\n",
    "        return x, y\n",
    "    else:\n",
    "        return cropped_x, cropped_y\n",
    "\n",
    "\n",
    "def augmentation2D(img, depth, print_info_aug):\n",
    "    # Random flipping\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['flip']:\n",
    "        img = (img[..., ::1, :, :]).copy()\n",
    "        depth = (depth[..., ::1, :, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random flipped')\n",
    "    # Random mirroring\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['mirror']:\n",
    "        img = (img[..., ::-1, :]).copy()\n",
    "        depth = (depth[..., ::-1, :]).copy()\n",
    "        if print_info_aug:\n",
    "            print('--> Random mirrored')\n",
    "    # Augment image\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['color&bright']:\n",
    "        # gamma augmentation\n",
    "        gamma = random.uniform(0.9, 1.1)\n",
    "        img = img ** gamma\n",
    "        brightness = random.uniform(0.9, 1.1)\n",
    "        img = img * brightness\n",
    "        # color augmentation\n",
    "        colors = np.random.uniform(0.9, 1.1, size=3)\n",
    "        white = np.ones((img.shape[0], img.shape[1]))\n",
    "        color_image = np.stack([white * colors[i] for i in range(3)], axis=2)\n",
    "        img *= color_image\n",
    "        img = np.clip(img, 0, 255)  # Originally with 0 and 1\n",
    "        if print_info_aug:\n",
    "            print('--> Image randomly augmented')\n",
    "    # Channel swap\n",
    "    if random.uniform(0, 1) <= augmentation_parameters['c_swap']:\n",
    "        indices = list(product([0, 1, 2], repeat=3))\n",
    "        policy_idx = random.randint(0, len(indices) - 1)\n",
    "        img = img[..., list(indices[policy_idx])]\n",
    "        if print_info_aug:\n",
    "            print('--> Channel swapped')\n",
    "    # Random crop\n",
    "    if random.random() <= augmentation_parameters['random_crop']:\n",
    "        img, depth = random_crop(img, depth)\n",
    "        if print_info_aug:\n",
    "            print('--> Random cropped')\n",
    "    # Depth Shift\n",
    "    if random.random() <= augmentation_parameters['random_d_shift']:\n",
    "        random_shift = random.randint(-10, 10)\n",
    "        depth = pixel_shift(depth, shift=random_shift)\n",
    "        if print_info_aug:\n",
    "            print('--> Depth Shifted of {} cm'.format(random_shift))\n",
    "\n",
    "    return img, depth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bbCnAwv453IN"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zhuQQhwf597g"
   },
   "outputs": [],
   "source": [
    "class NYU2_Dataset:\n",
    "    \"\"\"\n",
    "      * Indoor img (480, 640, 3) depth (480, 640, 1) both in png -> range between 0.5 to 10 meters\n",
    "      * 654 Test and 50688 Train images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, dts_type, aug, rgb_h_res, d_h_res, dts_size=0, scenarios='indoor'):\n",
    "        self.dataset = path\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.info = 0\n",
    "        self.dts_type = dts_type\n",
    "        self.aug = aug\n",
    "        self.rgb_h_res = rgb_h_res\n",
    "        self.d_h_res = d_h_res\n",
    "        self.scenarios = scenarios\n",
    "\n",
    "        # Handle dataset\n",
    "        if self.dts_type == 'test':\n",
    "            img_path = self.dataset + self.dts_type + '/eigen_test_rgb.npy' # '/content/drive/MyDerive/....FOLDER X .../test/carica_file_test.npy\n",
    "            depth_path = self.dataset + self.dts_type + '/eigen_test_depth.npy'\n",
    "\n",
    "            rgb = np.load(img_path)\n",
    "            depth = np.load(depth_path)\n",
    "\n",
    "            self.x = rgb\n",
    "            self.y = depth\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = rgb[:dts_size]\n",
    "                self.y = depth[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        elif self.dts_type == 'train':\n",
    "            scenarios = os.listdir(self.dataset + self.dts_type + '/')\n",
    "            for scene in scenarios:\n",
    "                elem = os.listdir(self.dataset + self.dts_type + '/' + scene)\n",
    "                for el in elem:\n",
    "                    if 'jpg' in el:\n",
    "                        self.x.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    elif 'png' in el:\n",
    "                        self.y.append(self.dts_type + '/' + scene + '/' + el)\n",
    "                    else:\n",
    "                        raise SystemError('Type image error (train)')\n",
    "\n",
    "            if len(self.x) != len(self.y):\n",
    "                raise SystemError('Problem with Img and Gt, no same train_size')\n",
    "\n",
    "            self.x.sort()\n",
    "            self.y.sort()\n",
    "\n",
    "            if dts_size != 0:\n",
    "                self.x = self.x[:dts_size]\n",
    "                self.y = self.y[:dts_size]\n",
    "\n",
    "            self.info = len(self.x)\n",
    "\n",
    "        else:\n",
    "            raise SystemError('Problem in the path')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.info\n",
    "\n",
    "    def __getitem__(self, index=None, print_info_aug=False):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, self.info)\n",
    "\n",
    "        # Load Image\n",
    "        if self.dts_type == 'test':\n",
    "            img = self.x[index]\n",
    "        else:\n",
    "            img_name = self.dataset + self.x[index]\n",
    "            try:\n",
    "                raw_img = Image.open(img_name)\n",
    "                img = np.array(raw_img.convert('RGB'))\n",
    "                raw_img.close()\n",
    "            except:\n",
    "                exit(f\"Failed opening {img_name}\")\n",
    "\n",
    "        # Load Depth Image\n",
    "        if self.dts_type == 'test':\n",
    "            depth = np.expand_dims(self.y[index] * 100, axis=-1)\n",
    "        else:\n",
    "            depth = Image.open(self.dataset + self.y[index])\n",
    "            depth = np.array(depth) / 255\n",
    "            depth = np.clip(depth * 1000, 50, 1000)\n",
    "            depth = np.expand_dims(depth, axis=-1)\n",
    "\n",
    "        # Augmentation\n",
    "        if self.aug:\n",
    "            img, depth = augmentation2D(img, depth, print_info_aug)\n",
    "\n",
    "        img_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['img_res'][1], param['img_res'][2]), antialias=True),\n",
    "            TT.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet\n",
    "        ])\n",
    "        depth_post_processing = TT.Compose([\n",
    "            TT.ToTensor(),\n",
    "            TT.Resize((param['depth_img_res'][1], param['depth_img_res'][2]), antialias=True),\n",
    "        ])\n",
    "\n",
    "        img = img_post_processing(img/255)\n",
    "        depth = depth_post_processing(depth)\n",
    "\n",
    "        return img.float(), depth.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KKUmyNBL5m1o"
   },
   "outputs": [],
   "source": [
    "def init_train_test_loader(dts_root_path, rgb_h_res, d_h_res, bs_train, bs_eval, num_workers, size_train=0, size_test=0):\n",
    "    # Load Datasets\n",
    "    test_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='test', aug=False, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_test\n",
    "    )\n",
    "    training_Dataset = NYU2_Dataset(\n",
    "        path=dts_root_path, dts_type='train', aug=True, rgb_h_res=rgb_h_res, d_h_res=d_h_res, dts_size=size_train\n",
    "    )\n",
    "    # Create Dataloaders\n",
    "    training_DataLoader = DataLoader(\n",
    "        training_Dataset, batch_size=bs_train, shuffle=True, pin_memory=True, num_workers=num_workers\n",
    "    )\n",
    "    test_DataLoader = DataLoader(\n",
    "        test_Dataset, batch_size=bs_eval, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return training_DataLoader, test_DataLoader, training_Dataset, test_Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tAWQQQzf8ctn"
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DTaUymYS8mDJ"
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Sobel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sobel, self).__init__()\n",
    "        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "        edge_k = np.stack((edge_kx, edge_ky))\n",
    "\n",
    "        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n",
    "        self.edge_conv.weight = nn.Parameter(edge_k)\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.edge_conv(x)\n",
    "        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class balanced_loss_function(nn.Module):\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super(balanced_loss_function, self).__init__()\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "        self.get_gradient = Sobel().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, output, depth):\n",
    "        with torch.no_grad():\n",
    "            ones = torch.ones(depth.size(0), 1, depth.size(2), depth.size(3)).float().to(self.device)\n",
    "\n",
    "        depth_grad = self.get_gradient(depth)\n",
    "        output_grad = self.get_gradient(output)\n",
    "\n",
    "        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n",
    "        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n",
    "\n",
    "        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n",
    "        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n",
    "\n",
    "        loss_depth = torch.abs(output - depth).mean()\n",
    "        loss_dx = torch.abs(output_grad_dx - depth_grad_dx).mean()\n",
    "        loss_dy = torch.abs(output_grad_dy - depth_grad_dy).mean()\n",
    "        loss_normal = 100 * torch.abs(1 - self.cos(output_normal, depth_normal)).mean()\n",
    "\n",
    "        loss_ssim = (1 - ssim(output, depth, val_range=1000.0)) * 100\n",
    "\n",
    "        loss_grad = (loss_dx + loss_dy) / 2\n",
    "\n",
    "        return loss_depth, loss_ssim, loss_normal, loss_grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vhLCflR28fxo"
   },
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M9_rdzyp87dB"
   },
   "outputs": [],
   "source": [
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, device, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, out_channels * depth,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   groups=depth,\n",
    "                                   padding=1,\n",
    "                                   stride=stride,\n",
    "                                   bias=bias).to(device)\n",
    "        self.pointwise = nn.Conv2d(out_channels * depth, out_channels, kernel_size=(1, 1), bias=bias).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        # nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride,\n",
    "                        #bias=False, device='cuda:0'),\n",
    "                        bias=False, device='cpu'),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()  # nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class ModLayerNorm(nn.GroupNorm):\n",
    "  def __init__(self, dim):\n",
    "      super().__init__(1, dim)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = ModLayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(dim,hidden_dim,1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.c2 = nn.Conv2d(hidden_dim,dim,1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, pool_size):\n",
    "      super().__init__()\n",
    "      self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "      #print(\"Pre pool: \", x.shape)\n",
    "      #print(\"Post pool: \", self.pool(x).shape)\n",
    "      return self.pool(x) - x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                #PreNorm(dim, Attention(3)),\n",
    "                PreNorm(dim, Attention(1)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            #print(\"***************************** Start logging *****************************\")\n",
    "            #print(\"Input shape: \", x.shape)\n",
    "            x = attn(x) + x\n",
    "            #print(\"Post attention shape: \", x.shape)\n",
    "            #print(\"FF values-> dim:%d, mlp_dim: %2d\" % (240, 120))\n",
    "            x = ff(x) + x\n",
    "            #print(\"Post MLP shape: \", x.shape)\n",
    "            #print(\"***************************** End logging *****************************\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),  # nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)  # Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "\n",
    "        # Local representations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        #B, C, H, W = Batch, Channels, Height, Width\n",
    "        # Global representations\n",
    "        _, _, h, w = x.shape\n",
    "        # b = batch\n",
    "        # d = depth = channels ?\n",
    "        # h = height\n",
    "        # ph = ?\n",
    "        # pw = ?\n",
    "\n",
    "        start_time = perf_counter() ############################## Time measurament\n",
    "        x = self.transformer(x)\n",
    "        end_time = perf_counter() ############################## Time measurament\n",
    "\n",
    "        # Fusion\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x, end_time-start_time ############################## Time measurament\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, num_classes,transformer_times, sample_cnt,expansion=4, kernel_size=3, patch_size=(2, 2)): ############################## Time measurament\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        self.transformer_times = transformer_times ############################## Time measurament\n",
    "        self.sample_cnt = sample_cnt ############################## Time measurament\n",
    "\n",
    "        L = [1, 1, 1]  # L = [2, 4, 3] # --> +5 FPS\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))  # Repeat\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
    "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
    "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "        # self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        # self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.conv1(x)\n",
    "        x = self.mv2[0](y0)\n",
    "\n",
    "        y1 = self.mv2[1](x)\n",
    "        x = self.mv2[2](y1)\n",
    "        x = self.mv2[3](x)  # Repeat\n",
    "\n",
    "        y2 = self.mv2[4](x)\n",
    "        x,mvit_time_1 = self.mvit[0](y2)\n",
    "        self.transformer_times[0][self.sample_cnt] = mvit_time_1 ############################## Time measurament\n",
    "\n",
    "        y3 = self.mv2[5](x)\n",
    "        x,mvit_time_2 = self.mvit[1](y3)\n",
    "        self.transformer_times[1][self.sample_cnt] = mvit_time_2 ############################## Time measurament\n",
    "\n",
    "        x = self.mv2[6](x)\n",
    "        x,mvit_time_3 = self.mvit[2](x)\n",
    "        self.transformer_times[2][self.sample_cnt] = mvit_time_3 ############################## Time measurament\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        self.sample_cnt += 1 ############################## Time measurament\n",
    "        if(self.sample_cnt == 655):\n",
    "          self.sample_cnt = 0\n",
    "\n",
    "        return x, [y0, y1, y2, y3]\n",
    "\n",
    "\n",
    "def mobilevit_s(transformer_times, sample_cnt):\n",
    "    enc_type = 's'\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
    "    return MobileViT((param['img_res'][1], param['img_res'][2]), dims, channels, num_classes=1000,\n",
    "                     transformer_times=transformer_times, sample_cnt=sample_cnt), enc_type ############################## Time measurament\n",
    "\n",
    "\n",
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, flag, sep_conv_filters, name, device):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.flag = flag\n",
    "        self.name = name\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3), device=device),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(1, 0), mode='constant', value=0.0)\n",
    "        if x.shape[-1] != enc_layer.shape[-1]:\n",
    "            enc_layer = torch.nn.functional.pad(enc_layer, pad=(0, 1), mode='constant', value=0.0)\n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        x = self.end_up_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, device, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
    "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          flag=True,\n",
    "                                          sep_conv_filters=192 if typ == 's' else 144 if typ == 'xs' else 96,\n",
    "                                          name='up1', device=device)\n",
    "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=128 if typ == 's' else 96 if typ == 'xs' else 64,\n",
    "                                          name='up2', device=device)\n",
    "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                          flag=False,\n",
    "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          name='up3', device=device)\n",
    "        self.conv2d_out = nn.Conv2d(16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                    1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[3])\n",
    "        x = self.ups_block_2(x, enc_layer_list[2])\n",
    "        x = self.ups_block_3(x, enc_layer_list[1])\n",
    "        x = self.conv2d_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_model(nn.Module):\n",
    "    \"\"\"\n",
    "        MobileVit -> https://arxiv.org/pdf/2110.02178.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(build_model, self).__init__()\n",
    "        self.transformer_times = np.zeros((3,655),dtype='float') ############################## Time measurament\n",
    "        self.sample_cnt = 0 ############################## Time measurament\n",
    "\n",
    "        self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) ############################## Time measurament\n",
    "        self.decoder = SPEED_decoder(device=device, typ=enc_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, enc_layer = self.encoder(x)\n",
    "        x = self.decoder(x, enc_layer)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eAvmPzvA8Pu-"
   },
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "es_aqA008Sof"
   },
   "outputs": [],
   "source": [
    "def log10(x):\n",
    "    return torch.log(x) / math.log(10)\n",
    "\n",
    "\n",
    "class Result(object):\n",
    "    def __init__(self):\n",
    "        self.irmse, self.imae = 0, 0\n",
    "        self.mse, self.rmse, self.mae = 0, 0, 0\n",
    "        self.absrel, self.lg10 = 0, 0\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def set_to_worst(self):\n",
    "        self.irmse, self.imae = np.inf, np.inf\n",
    "        self.mse, self.rmse, self.mae = np.inf, np.inf, np.inf\n",
    "        self.absrel, self.lg10 = np.inf, np.inf\n",
    "        self.delta1, self.delta2, self.delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, irmse, imae, mse, rmse, mae, absrel, lg10, delta1, delta2, delta3):\n",
    "        self.irmse, self.imae = irmse, imae\n",
    "        self.mse, self.rmse, self.mae = mse, rmse, mae\n",
    "        self.absrel, self.lg10 = absrel, lg10\n",
    "        self.delta1, self.delta2, self.delta3 = delta1, delta2, delta3\n",
    "\n",
    "    def evaluate(self, output, target):\n",
    "        valid_mask = target > 0\n",
    "\n",
    "        output = output[valid_mask]\n",
    "        target = target[valid_mask]\n",
    "        \n",
    "\n",
    "        abs_diff = (output - target).abs()\n",
    "\n",
    "        self.mse = float((torch.pow(abs_diff, 2)).mean())\n",
    "        self.rmse = math.sqrt(self.mse)\n",
    "        self.mae = float(abs_diff.mean())\n",
    "        self.lg10 = float((log10(output) - log10(target)).abs().mean())\n",
    "        self.absrel = float((abs_diff / target).mean())\n",
    "\n",
    "        maxRatio = torch.max(output / target, target / output)\n",
    "        self.delta1 = float((maxRatio < 1.25).float().mean())\n",
    "        self.delta2 = float((maxRatio < 1.25 ** 2).float().mean())\n",
    "        self.delta3 = float((maxRatio < 1.25 ** 3).float().mean())\n",
    "\n",
    "        inv_output = 1 / output\n",
    "        inv_target = 1 / target\n",
    "        abs_inv_diff = (inv_output - inv_target).abs()\n",
    "        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n",
    "        self.imae = float(abs_inv_diff.mean())\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count = 0.0\n",
    "        self.sum_irmse, self.sum_imae = 0, 0\n",
    "        self.sum_mse, self.sum_rmse, self.sum_mae = 0, 0, 0\n",
    "        self.sum_absrel, self.sum_lg10 = 0, 0\n",
    "        self.sum_delta1, self.sum_delta2, self.sum_delta3 = 0, 0, 0\n",
    "\n",
    "    def update(self, result, n=1):\n",
    "        self.count += n\n",
    "\n",
    "        self.sum_irmse += n * result.irmse\n",
    "        self.sum_imae += n * result.imae\n",
    "        self.sum_mse += n * result.mse\n",
    "        self.sum_rmse += n * result.rmse\n",
    "        self.sum_mae += n * result.mae\n",
    "        self.sum_absrel += n * result.absrel\n",
    "        self.sum_lg10 += n * result.lg10\n",
    "        self.sum_delta1 += n * result.delta1\n",
    "        self.sum_delta2 += n * result.delta2\n",
    "        self.sum_delta3 += n * result.delta3\n",
    "\n",
    "    def average(self):\n",
    "        avg = Result()\n",
    "        avg.update(\n",
    "            self.sum_irmse / self.count, self.sum_imae / self.count,\n",
    "            self.sum_mse / self.count, self.sum_rmse / self.count, self.sum_mae / self.count,\n",
    "            self.sum_absrel / self.count, self.sum_lg10 / self.count,\n",
    "            self.sum_delta1 / self.count, self.sum_delta2 / self.count, self.sum_delta3 / self.count)\n",
    "        return avg\n",
    "\n",
    "\n",
    "def compute_evaluation(test_dataloader, model, model_type, path_save_csv_results):\n",
    "    best_worst_dict = {}\n",
    "    result = Result()\n",
    "    result.set_to_worst()\n",
    "    average_meter = AverageMeter()\n",
    "    model.eval()  # switch to evaluate mode\n",
    "\n",
    "    for i, (inputs, depths) in enumerate(test_dataloader):\n",
    "        inputs, depths = inputs.cuda(), depths.cuda()\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs)\n",
    "        result.evaluate(predictions, depths)\n",
    "        average_meter.update(result)  # (result, inputs.size(0))\n",
    "        best_worst_dict[i] = result.rmse\n",
    "\n",
    "    avg = average_meter.average()\n",
    "\n",
    "    print('MAE={average.mae:.3f}\\n'\n",
    "          'RMSE={average.rmse:.3f}\\n'\n",
    "          'Delta1={average.delta1:.3f}\\n'\n",
    "          'REL={average.absrel:.3f}\\n'\n",
    "          'Lg10={average.lg10:.3f}'.format(average=avg))\n",
    "\n",
    "    with open(path_save_csv_results + 'test' + model_type + 'results.csv', 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['mse', 'rmse', 'absrel', 'lg10', 'mae', 'delta1', 'delta2', 'delta3'])\n",
    "        writer.writeheader()\n",
    "        writer.writerow({'mse': avg.mse, 'rmse': avg.rmse, 'absrel': avg.absrel, 'lg10': avg.lg10,\n",
    "                         'mae': avg.mae, 'delta1': avg.delta1, 'delta2': avg.delta2, 'delta3': avg.delta3})\n",
    "\n",
    "    return best_worst_dict, avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EjiqGK4q42zP"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                                            Output Shape              Param #\n",
      "===================================================================================================================\n",
      "build_model                                                       [1, 1, 48, 64]            --\n",
      "MobileViT: 1-1                                                  [1, 320, 6, 8]            --\n",
      "    Sequential: 2-1                                            [1, 16, 96, 128]          --\n",
      "        SeparableConv2d: 3-1                                  [1, 16, 96, 128]          688\n",
      "        BatchNorm2d: 3-2                                      [1, 16, 96, 128]          32\n",
      "        ReLU: 3-3                                             [1, 16, 96, 128]          --\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-4                                         [1, 32, 96, 128]          3,968\n",
      "        MV2Block: 3-5                                         [1, 64, 48, 64]           14,080\n",
      "        MV2Block: 3-6                                         [1, 64, 48, 64]           36,224\n",
      "        MV2Block: 3-7                                         [1, 64, 48, 64]           36,224\n",
      "        MV2Block: 3-8                                         [1, 96, 24, 32]           44,480\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        MobileViTBlock: 3-9                                   [1, 96, 24, 32]           379,728\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-10                                        [1, 128, 12, 16]          91,264\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        MobileViTBlock: 3-11                                  [1, 128, 12, 16]          822,080\n",
      "    ModuleList: 2-6                                            --                        (recursive)\n",
      "        MV2Block: 3-12                                        [1, 160, 6, 8]            154,432\n",
      "    ModuleList: 2-7                                            --                        (recursive)\n",
      "        MobileViTBlock: 3-13                                  [1, 160, 6, 8]            1,283,600\n",
      "    Sequential: 2-8                                            [1, 320, 6, 8]            --\n",
      "        Conv2d: 3-14                                          [1, 320, 6, 8]            51,200\n",
      "        BatchNorm2d: 3-15                                     [1, 320, 6, 8]            640\n",
      "        ReLU: 3-16                                            [1, 320, 6, 8]            --\n",
      "SPEED_decoder: 1-2                                              [1, 1, 48, 64]            --\n",
      "    Conv2d: 2-9                                                [1, 128, 6, 8]            40,960\n",
      "    UpSample_layer: 2-10                                       [1, 64, 12, 16]           --\n",
      "        ConvTranspose2d: 3-17                                 [1, 64, 12, 16]           73,728\n",
      "        Sequential: 3-18                                      [1, 64, 12, 16]           114,688\n",
      "    UpSample_layer: 2-11                                       [1, 32, 24, 32]           --\n",
      "        ConvTranspose2d: 3-19                                 [1, 32, 24, 32]           18,432\n",
      "        Sequential: 3-20                                      [1, 32, 24, 32]           37,888\n",
      "    UpSample_layer: 2-12                                       [1, 16, 48, 64]           --\n",
      "        ConvTranspose2d: 3-21                                 [1, 16, 48, 64]           4,608\n",
      "        Sequential: 3-22                                      [1, 16, 48, 64]           11,776\n",
      "    Conv2d: 2-13                                               [1, 1, 48, 64]            144\n",
      "===================================================================================================================\n",
      "Total params: 3,220,864\n",
      "Trainable params: 3,220,864\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 1.12\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 171.53\n",
      "Params size (MB): 12.88\n",
      "Estimated Total Size (MB): 185.01\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = hardware_check()\n",
    "model = build_model(device=device).to(device=device)\n",
    "print_model(model=model, input_shape=(1, *param['img_res']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "590sGQdh45FS",
    "outputId": "14b1c02e-ada3-4ba5-8e8c-4d5be04cc13c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual device:  cuda:0\n",
      "Device info: name='NVIDIA GeForce RTX 4090', major=8, minor=9, total_memory=24195MB, multi_processor_count=128\n",
      "INFO: There are 50688 training and 654 testing samples\n",
      " --- Test samples --- \n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 509.6751708984375, min = 107.8142318725586\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.29186224937438965, variance =  1.485769271850586\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 992.474365234375, min = 187.30963134765625\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1109719276428223, mean = -0.19730940461158752, variance =  1.3164700269699097\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      " --- Training augmented samples --- \n",
      "--> Random mirrored\n",
      "--> Channel swapped\n",
      "--> Depth Shifted of -6 cm\n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 382.3302001953125, min = 76.38421630859375\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.852229118347168, variance =  1.5632150173187256\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random mirrored\n",
      "--> Image randomly augmented\n",
      "--> Channel swapped\n",
      "--> Random cropped\n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 389.5745849609375, min = 172.5490264892578\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.3290196657180786, variance =  1.840871810913086\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      "--> Image randomly augmented\n",
      "--> Random cropped\n",
      "--> Depth Shifted of -3 cm\n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 986.6982421875, min = 264.9446716308594\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = 1.4569069147109985, variance =  1.1588993072509766\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Random flipped\n",
      "--> Random cropped\n",
      "--> Depth Shifted of -3 cm\n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 396.9956359863281, min = 193.07843017578125\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.1179039478302, mean = -0.11887330561876297, variance =  0.48569098114967346\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n",
      "--> Random flipped\n",
      "--> Random mirrored\n",
      "--> Depth Shifted of 2 cm\n",
      "Depth -> Shape = torch.Size([1, 48, 64]), max = 880.5257568359375, min = 128.9499969482422\n",
      "IMG -> Shape = torch.Size([3, 192, 256]), max = 2.640000104904175, min = -2.0357143878936768, mean = 0.27516233921051025, variance =  1.0001124143600464\n",
      "\n",
      "**************************  ./results/meta_meter\n",
      "**************************  ./results/meta_meterexample&augment_img/\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 3, Conv2d: 4, Conv2d: 4]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:295\u001B[0m, in \u001B[0;36mforward_pass\u001B[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 295\u001B[0m     _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m-> 1212\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "Cell \u001B[0;32mIn[9], line 324\u001B[0m, in \u001B[0;36mbuild_model.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 324\u001B[0m     x, enc_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(x, enc_layer)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m-> 1212\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "Cell \u001B[0;32mIn[9], line 218\u001B[0m, in \u001B[0;36mMobileViT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 218\u001B[0m     y0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    219\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmv2[\u001B[38;5;241m0\u001B[39m](y0)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m-> 1212\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1212\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1210\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m bw_hook\u001B[38;5;241m.\u001B[39msetup_input_hook(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m-> 1212\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks:\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py:138\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 138\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_input_dim\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;66;03m# exponential_average_factor is set to self.momentum\u001B[39;00m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;66;03m# (when it is available) only so that it gets updated\u001B[39;00m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py:410\u001B[0m, in \u001B[0;36mBatchNorm2d._check_input_dim\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n\u001B[0;32m--> 410\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected 4D input (got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124mD input)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n",
      "\u001B[0;31mValueError\u001B[0m: expected 4D input (got 3D input)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 220\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;66;03m# if not os.path.exists(save_model_root + 'info_code/'):\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;66;03m#     os.makedirs(save_model_root + 'info_code/')\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;66;03m# files_directory = '/work/project/'\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m#     shutil.copy(f, save_model_root + 'info_code/')\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;66;03m# Run process\u001B[39;00m\n\u001B[1;32m    219\u001B[0m start_time \u001B[38;5;241m=\u001B[39m perf_counter()\n\u001B[0;32m--> 220\u001B[0m \u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n\u001B[1;32m    222\u001B[0m end_time \u001B[38;5;241m=\u001B[39m perf_counter()\n",
      "Cell \u001B[0;32mIn[12], line 40\u001B[0m, in \u001B[0;36mprocess\u001B[0;34m(device)\u001B[0m\n\u001B[1;32m     37\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(device\u001B[38;5;241m=\u001B[39mdevice)\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     38\u001B[0m model_name \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mprint_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparam\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimg_res\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m model has: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m trainable parameters\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(model_name, count_parameters(model)))\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Optimizer\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[4], line 58\u001B[0m, in \u001B[0;36mprint_model\u001B[0;34m(model, input_shape)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprint_model\u001B[39m(model, input_shape):\n\u001B[0;32m---> 58\u001B[0m     info \u001B[38;5;241m=\u001B[39m \u001B[43msummary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_shape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mprint\u001B[39m(info)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:223\u001B[0m, in \u001B[0;36msummary\u001B[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001B[0m\n\u001B[1;32m    216\u001B[0m validate_user_params(\n\u001B[1;32m    217\u001B[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001B[1;32m    218\u001B[0m )\n\u001B[1;32m    220\u001B[0m x, correct_input_size \u001B[38;5;241m=\u001B[39m process_input(\n\u001B[1;32m    221\u001B[0m     input_data, input_size, batch_dim, device, dtypes\n\u001B[1;32m    222\u001B[0m )\n\u001B[0;32m--> 223\u001B[0m summary_list \u001B[38;5;241m=\u001B[39m \u001B[43mforward_pass\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_forward_pass\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    226\u001B[0m formatting \u001B[38;5;241m=\u001B[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001B[1;32m    227\u001B[0m results \u001B[38;5;241m=\u001B[39m ModelStatistics(\n\u001B[1;32m    228\u001B[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001B[1;32m    229\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/dist-packages/torchinfo/torchinfo.py:304\u001B[0m, in \u001B[0;36mforward_pass\u001B[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001B[0m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    303\u001B[0m     executed_layers \u001B[38;5;241m=\u001B[39m [layer \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m summary_list \u001B[38;5;28;01mif\u001B[39;00m layer\u001B[38;5;241m.\u001B[39mexecuted]\n\u001B[0;32m--> 304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuted layers up to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexecuted_layers\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [SeparableConv2d: 3, Conv2d: 4, Conv2d: 4]"
     ]
    }
   ],
   "source": [
    "def process(device):\n",
    "    # Set-seed\n",
    "    seed = param['seed']\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Datasets loading\n",
    "    training_DataLoader, test_DataLoader, training_Dataset, test_Dataset = init_train_test_loader(\n",
    "        dts_root_path=dataset_root,\n",
    "        rgb_h_res=param['img_res'][1],\n",
    "        d_h_res=param['depth_img_res'][1],\n",
    "        bs_train=param['batch_size'],\n",
    "        bs_eval=param['batch_size_eval'],\n",
    "        num_workers=param['n_workers'],\n",
    "    )\n",
    "    print('INFO: There are {} training and {} testing samples'.format(training_Dataset.__len__(), test_Dataset.__len__()))\n",
    "    # Prints samples\n",
    "    print(' --- Test samples --- ')\n",
    "    print_img(test_Dataset, label='rgb_sample', quantity=2,\n",
    "              save_model_root=save_model_root)\n",
    "    print(' --- Training augmented samples --- ')\n",
    "    print_img(training_Dataset, label='aug_sample', quantity=5, print_info_aug=True,\n",
    "                  save_model_root=save_model_root)\n",
    "    \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # Globals\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lrs': [], 'test_rmse': [],\n",
    "               'l_mae': [], 'l_norm': [], 'l_grad': [], 'l_ssim': []}\n",
    "    min_rmse = float('inf')\n",
    "    min_acc = 0\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    # Loss\n",
    "    criterion = balanced_loss_function(device=device)\n",
    "    # Model\n",
    "    model = build_model(device=device).to(device=device)\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    print_model(model=model, input_shape=param['img_res'])\n",
    "    print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=param['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False\n",
    "    )\n",
    "    # Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=param['lr_patience'], threshold=1e-4, threshold_mode='rel',\n",
    "        cooldown=0, min_lr=1e-8, eps=1e-08, verbose=False\n",
    "    )\n",
    "    # Early stopping\n",
    "    trigger_times, early_stopping_epochs = 0, param['e_stop_epochs']\n",
    "    print(\"Start training: {}\\n\".format(model_name))\n",
    "    \n",
    "    epochs = param['epochs']\n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        iter = 1\n",
    "        model.train()\n",
    "        running_loss, accuracy = 0, 0\n",
    "        running_l_mae, running_l_grad, running_l_norm, running_l_ssim = 0, 0, 0, 0\n",
    "        with tqdm(training_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
    "                # Load data\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Forward\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                # Compute loss\n",
    "                loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Evaluation and Stats\n",
    "                running_loss += loss.item()\n",
    "                running_l_mae += loss_depth.item()\n",
    "                running_l_norm += loss_normal.item()\n",
    "                running_l_grad += loss_grad.item()\n",
    "                running_l_ssim += loss_ssim.item()\n",
    "\n",
    "                train_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                train_loss_list.append(train_loss_support)\n",
    "\n",
    "                accuracy += compute_accuracy(outputs, depths)\n",
    "                tepoch.set_postfix({'Loss': running_loss / iter,\n",
    "                                    'Acc': accuracy.item() / iter,\n",
    "                                    'Lr': param['lr'] if not history['lrs'] else history['lrs'][-1],\n",
    "                                    'L_mae': running_l_mae / iter,\n",
    "                                    'L_norm': running_l_norm / iter,\n",
    "                                    'L_grad': running_l_grad / iter,\n",
    "                                    'L_ssim': running_l_ssim / iter\n",
    "                                    })\n",
    "                iter += 1\n",
    "\n",
    "        # Validation\n",
    "        iter = 1\n",
    "        model.eval()\n",
    "        test_loss, test_accuracy, test_rmse = 0, 0, 0\n",
    "        with tqdm(test_DataLoader, unit=\"step\", position=0, leave=True) as tepoch:\n",
    "            for batch in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
    "                inputs, depths = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                # Validation loop\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    # Evaluation metrics\n",
    "                    test_accuracy += compute_accuracy(outputs, depths)\n",
    "                    # Loss\n",
    "                    loss_depth, loss_ssim, loss_normal, loss_grad = criterion(outputs, depths)\n",
    "                    loss = loss_depth + loss_normal + loss_grad + loss_ssim\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    test_loss_support = [loss_depth.item(), loss_normal.item(), loss_grad.item(), loss.item()]\n",
    "                    test_loss_list.append(test_loss_support)\n",
    "\n",
    "                    # RMSE\n",
    "                    test_rmse += compute_rmse(outputs, depths)\n",
    "                    tepoch.set_postfix({'Loss': test_loss / iter, 'Acc': test_accuracy.item() / iter,\n",
    "                                        'RMSE': test_rmse.item() / iter})\n",
    "                    iter += 1\n",
    "\n",
    "        # Update history infos\n",
    "        history['lrs'].append(get_lr(optimizer))\n",
    "        history['train_loss'].append(running_loss / len(training_DataLoader))\n",
    "        history['val_loss'].append(test_loss / len(test_DataLoader))\n",
    "        history['train_acc'].append(accuracy.item() / len(training_DataLoader))\n",
    "        history['val_acc'].append(test_accuracy.item() / len(test_DataLoader))\n",
    "        history['test_rmse'].append(test_rmse.item() / len(test_DataLoader))\n",
    "        # Update history losses infos\n",
    "        history['l_mae'].append(running_l_mae / len(training_DataLoader))\n",
    "        history['l_norm'].append(running_l_norm / len(training_DataLoader))\n",
    "        history['l_grad'].append(running_l_grad / len(training_DataLoader))\n",
    "        history['l_ssim'].append(running_l_ssim / len(training_DataLoader))\n",
    "        # Update scheduler LR\n",
    "        scheduler.step(history['test_rmse'][-1])\n",
    "        # Save model by best RMSE\n",
    "        if min_rmse >= (test_rmse / len(test_DataLoader)):\n",
    "            trigger_times = 0\n",
    "            min_rmse = test_rmse / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best', save_model_root)\n",
    "            print('New best RMSE: {:.3f} at epoch {}'.format(min_rmse, epoch + 1))\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            print('RMSE did not improved, EarlyStopping from {} epochs'.format(early_stopping_epochs - trigger_times))\n",
    "        # Save model by best ACCURACY\n",
    "        if min_acc <= (test_accuracy / len(test_DataLoader)):\n",
    "            min_acc = test_accuracy / len(test_DataLoader)\n",
    "            save_checkpoint(model, model_name + '_best_acc', save_model_root)\n",
    "            print('New best ACCURACY: {:.3f} at epoch {}'.format(min_acc, epoch + 1))\n",
    "            if trigger_times > 4:\n",
    "                trigger_times = trigger_times - 2\n",
    "                print(f\"EarlyStopping increased due to Accuracy, stop in {early_stopping_epochs - trigger_times} epochs\")\n",
    "\n",
    "        save_prediction_examples(model, dataset=test_Dataset, device=device, indices=[0, 216, 432, 639], ep=epoch,\n",
    "                                 save_path=save_model_root + 'evolution_img/')\n",
    "        save_history(history, save_model_root + model_name + '_history')\n",
    "        # Empty CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if trigger_times == early_stopping_epochs:\n",
    "            print('Val Loss did not imporved for {} epochs, training stopped'.format(early_stopping_epochs + 1))\n",
    "            break\n",
    "\n",
    "        # Save loss for graphs\n",
    "        np.save(save_model_root + 'train.npy', np.array(train_loss_list))\n",
    "        np.save(save_model_root + 'test.npy', np.array(test_loss_list))\n",
    "\n",
    "        print('Finished Training')\n",
    "        save_csv_history(model_name=model_name, path=save_model_root)\n",
    "        plot_history(history, path=save_model_root)\n",
    "        plot_loss_parts(history, path=save_model_root, title='Loss Components')\n",
    "\n",
    "        if os.path.exists(save_model_root + 'example&augment_img/'):\n",
    "            shutil.rmtree(save_model_root + 'example&augment_img/')\n",
    "\n",
    "\n",
    "    # model = build_model(device=device, arch_type=global_var['architecture_type']).to(device=device)\n",
    "    # model, model_name = load_pretrained_model(model=model,\n",
    "    #                                           path_weigths=save_model_root + 'build_model_best',\n",
    "    #                                           device=device,\n",
    "    #                                           do_pretrained=global_var['do_pretrained'],\n",
    "    #                                           imagenet_w_init=global_var['imagenet_w_init'])\n",
    "    # if global_var['do_print_model']:\n",
    "    #     print_model(model=model, device=device, save_model_root=save_model_root,\n",
    "    #                 input_shape=global_var['RGB_img_res'])\n",
    "    # print('The {} model has: {} trainable parameters'.format(model_name, count_parameters(model)))\n",
    "\n",
    "    # Evaluate\n",
    "    print(' --- Begin evaluation --- ')\n",
    "    best_worst, avg = compute_evaluation(test_dataloader=test_DataLoader, model=model, model_type='_', path_save_csv_results=save_model_root)\n",
    "    print(' --- End evaluation --- ')\n",
    "\n",
    "    sorted_best_worst = sorted(best_worst.items(), key=lambda item: item[1])\n",
    "    save_best_worst(sorted_best_worst[0:10], type='best', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "    save_best_worst(sorted_best_worst[-10:], type='worst', model=model, dataset=test_Dataset, device=device, save_model_root=save_model_root)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Hardware\n",
    "    device = hardware_check()\n",
    "\n",
    "    # -- TRAIN 1\n",
    "    #TEST_NAME = 'METER_ImgNetNorm_ImgNetInit_Long_bst64_bsv8'\n",
    "    # Directory test\n",
    "    #save_model_root = save_model_root + TEST_NAME + '/'\n",
    "    #print(save_model_root)\n",
    "    # Create folders\n",
    "    if not os.path.exists(save_model_root):\n",
    "        os.makedirs(save_model_root)\n",
    "    # if not os.path.exists(save_model_root + 'info_code/'):\n",
    "    #     os.makedirs(save_model_root + 'info_code/')\n",
    "    # files_directory = '/work/project/'\n",
    "    # files = [files_directory + 'architectures/mobile_vit_fast_sep_SC.py', files_directory + 'globals.py', files_directory + 'loss.py']\n",
    "    # for f in files:\n",
    "    #     shutil.copy(f, save_model_root + 'info_code/')\n",
    "    # Run process\n",
    "    start_time = perf_counter()\n",
    "    process(device=device)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = perf_counter()\n",
    "    print(\"Total time elapsed: \",end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "g2bPw-5W4hJe",
    "Nm2TAq6B5UBI",
    "BoQYEe4V5j5E",
    "bbCnAwv453IN",
    "tAWQQQzf8ctn",
    "eAvmPzvA8Pu-",
    "EjiqGK4q42zP",
    "Ns4BuU6tsskL"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
